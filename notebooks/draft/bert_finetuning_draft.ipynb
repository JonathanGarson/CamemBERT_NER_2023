{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning of BERT for NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the steps for the finetuning of French version of BERT, the objective is to extract specific information from french public business agreements. For this we have a dual approach, first we classify information and then only we use NER property of our model to extract wage variations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: This notebook is inspired from a masterclass and notebook provided by [Thomas Boehler](https://fr.linkedin.com/in/thomas-boehler-ba34a744) and also the work of my colleagues [Mouad Bernoussi](https://ma.linkedin.com/in/mouad-bernoussi-00aa91242) (see ./notebooks/external) and [Conrad Thiounn](https://github.com/cthiounn)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Play with PyTorch\n",
    "2. Play with the model \n",
    "3. Adapt data to BERT format (see Thomas Boehler)\n",
    "4. Create training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaner draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model_checkpoint = \"Jean-Baptiste/camembert-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\") #this last argument might be a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForTokenClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# This will print the device (either 'cuda' or 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# You can then move your model and data to this device like this:\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_conll(path:str):\n",
    "    # Initialize a list to store the data\n",
    "    conll_data = []\n",
    "\n",
    "    # Open the CoNLL file in read mode\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "        # Read each line in the file\n",
    "\n",
    "    # Split the data into sentences\n",
    "    sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "    # Initialize empty lists to store text and labels\n",
    "    text_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split('\\n')\n",
    "        text = \" \".join(token.split()[0] for token in tokens)\n",
    "        labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "    return df\n",
    "\n",
    "#we divide the data into train, test, and validation sets\n",
    "\n",
    "def split_data(data_dict, train_size=0.8, test_size=0.1, val_size=0.1, random_seed=None):\n",
    "    # Set a random seed for reproducibility\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Combine input_ids, attention_mask, and aligned_labels into a single list\n",
    "    combined_data = list(zip(data_dict['input_ids'], data_dict['attention_mask'], data_dict['labels']))\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    total_size = len(combined_data)\n",
    "    train_size = int(train_size * total_size)\n",
    "    test_size = int(test_size * total_size)\n",
    "    val_size = int(val_size * total_size)\n",
    "\n",
    "    # Split the data into train, test, and val sets\n",
    "    train_data = combined_data[:train_size]\n",
    "    test_data = combined_data[train_size:train_size + test_size]\n",
    "    val_data = combined_data[train_size + test_size:train_size + test_size + val_size]\n",
    "\n",
    "    # Unzip the data to restore the original structure\n",
    "    train_input_ids, train_attention_mask, train_aligned_labels = zip(*train_data)\n",
    "    test_input_ids, test_attention_mask, test_aligned_labels = zip(*test_data)\n",
    "    val_input_ids, val_attention_mask, val_aligned_labels = zip(*val_data)\n",
    "\n",
    "    # Create dictionaries for the train, test, and val sets\n",
    "    train_set = {\n",
    "        'input_ids': list(train_input_ids),\n",
    "        'attention_mask': list(train_attention_mask),\n",
    "        'labels': list(train_aligned_labels)\n",
    "    }\n",
    "    test_set = {\n",
    "        'input_ids': list(test_input_ids),\n",
    "        'attention_mask': list(test_attention_mask),\n",
    "        'labels': list(test_aligned_labels)\n",
    "    }\n",
    "    val_set = {\n",
    "        'input_ids': list(val_input_ids),\n",
    "        'attention_mask': list(val_attention_mask),\n",
    "        'labels': list(val_aligned_labels)\n",
    "    }\n",
    "\n",
    "    return train_set, test_set, val_set\n",
    "\n",
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of words\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of words\n",
    "    \"\"\"\n",
    "    return sentence.split()\n",
    "\n",
    "# def format_text(df):\n",
    "#     formated_text = []\n",
    "\n",
    "#     text = df[\"text\"]\n",
    "#     for i in text:\n",
    "#         i = turn_sentence_to_list(i)\n",
    "#         formated_text.append(i)\n",
    "\n",
    "#     # we add it to the dataframe\n",
    "#     df[\"formated_text\"] = formated_text\n",
    "#     return df\n",
    "\n",
    "# def format_labels(df, column):\n",
    "#     labels = df[column].tolist()\n",
    "\n",
    "#     formated_labels = []\n",
    "\n",
    "#     for label in labels:\n",
    "#         # label = list_to_string(label)\n",
    "#         label = turn_sentence_to_list(label)\n",
    "#         formated_labels.append(label)\n",
    "#         # print(label)\n",
    "\n",
    "#     # we add it to the dataframe\n",
    "\n",
    "#     df[\"formated_labels\"] = formated_labels\n",
    "#     return df\n",
    "\n",
    "def change_ids(list_ids:list, new_id:dict):\n",
    "    \"\"\"\n",
    "    change the ids of a list of ids to a new id\n",
    "\n",
    "    Args:\n",
    "        list_ids (list): list of ids\n",
    "        new_id (dict): dictionary with the new id\n",
    "    \"\"\"\n",
    "    return [new_id.get(id) for id in list_ids]\n",
    "\n",
    "def align_labels(word_ids:list, tag_list:list):\n",
    "    \"\"\"\n",
    "    Align the labels with the words\n",
    "\n",
    "    Args:\n",
    "        word_ids (list): list of ids of the words\n",
    "        tag_list (list): list of tags\n",
    "    \"\"\"\n",
    "    aligned_labels = []\n",
    "    for i in word_ids:\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "        else:\n",
    "            aligned_labels.append(tag_list[i])\n",
    "    return aligned_labels\n",
    "\n",
    "def check_labels(list_labels:list):\n",
    "    for label in list_labels:\n",
    "        if type(label)!=int:\n",
    "            print(label)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def count_error(df, column:str):\n",
    "    \"\"\"\n",
    "    Count the number of errors - a cell containing a non numerical value - in a column of a dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): dataframe to check\n",
    "        column (str): column to check\n",
    "\n",
    "    Returns:\n",
    "        int: number of errors\n",
    "    \"\"\"\n",
    "    return len(df[column])- sum(df[column].apply(lambda x: check_labels(x)))\n",
    "\n",
    "def create_iids_am(df):\n",
    "    \"\"\"\n",
    "    Create the input ids and attention mask columns\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe containing the data with the input ids and attention mask columns\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for _,i in df.iterrows():\n",
    "        input = i[\"ids\"][\"input_ids\"]\n",
    "        attention = i[\"ids\"][\"attention_mask\"]\n",
    "\n",
    "        input_ids.append(input)\n",
    "        attention_mask.append(attention)\n",
    "\n",
    "    df[\"input_ids\"] = input_ids\n",
    "    df[\"attention_mask\"] = attention_mask\n",
    "    return df\n",
    "\n",
    "def select_columns(df, columns:list):\n",
    "    \"\"\"\n",
    "    Select columns from a dataframe\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe\n",
    "        columns (list): list of columns to select\n",
    "\n",
    "    \"\"\"\n",
    "    df = df[columns].copy()\n",
    "    return df\n",
    "\n",
    "def tokenize_and_align_data(path_conll:str, new_id:dict):\n",
    "    \"\"\"\n",
    "    Tokenize the text and align the labels\n",
    "\n",
    "    Args:\n",
    "        path_conll (str): path to the conll file\n",
    "        new_id (dict): dictionary with the new id\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe containing the data\n",
    "    \"\"\"\n",
    "    #charge and create the df\n",
    "    df = charge_conll(path_conll)\n",
    "    df[\"formated_text\"] = df.apply(lambda x: turn_sentence_to_list(x[\"text\"]), axis=1 )\n",
    "    df[\"formated_labels\"] = df.apply(lambda x: turn_sentence_to_list(x[\"label\"]), axis=1)\n",
    "    # df = format_text(df)\n",
    "    # df = format_labels(df)\n",
    "    df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))\n",
    "\n",
    "    #tokenize and align the text\n",
    "    df[\"ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True))\n",
    "    df = create_iids_am(df)\n",
    "    df[\"word_ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True).word_ids())\n",
    "    df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"new_labels\"]), axis=1)\n",
    "    # print(df)\n",
    "    return df\n",
    "\n",
    "def apply_tokenization(conll_path:str, new_id:dict, columns:list, new_names:dict, save_path=False, output_save_csv=None):\n",
    "    \"\"\"\n",
    "    This function apply the tokenization and alignment to a conll file and select the columns we want to keep\n",
    "\n",
    "    Args:\n",
    "        conll_path (str): path to the conll file\n",
    "        new_id (dict): dictionary with the new id\n",
    "        columns (list): list of columns to keep\n",
    "        new_names (dict): dictionary with the new names of the columns\n",
    "        save_path (bool, optional): if True, save the dataframe in a csv file. Defaults to False.\n",
    "        output_save_csv ([type], optional): path to the csv file. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe containing the data\n",
    "    \"\"\"\n",
    "    df = tokenize_and_align_data(conll_path, new_id=new_id)\n",
    "    df = select_columns(df, columns)\n",
    "    df = df.rename(columns=new_names)\n",
    "    #we check the length of the two columns so that they are of the same dimensions\n",
    "    print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "    if save_path:\n",
    "        df.to_csv(output_save_csv, index=False)\n",
    "    return df\n",
    "\n",
    "def final_formating(df, start:int, end:int):\n",
    "    \"\"\"\n",
    "    Collect the data from a dataframe in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionnary containing the data\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        input_ids.append(df[\"input_ids\"][i])\n",
    "        attention_mask.append(df[\"attention_mask\"][i])\n",
    "        labels.append(df[\"aligned_labels\"][i])\n",
    "\n",
    "    data = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "    return data\n",
    "\n",
    "def download_and_treat_data(conll_path:str, new_id:dict, columns:list, new_names:dict, start:int, end:int, csv=False, output_save=None):\n",
    "    \"\"\"\n",
    "    This function compile all the previous one and treat automatically all data provided as conll format.\\n\n",
    "    The objective is to provide a dictionnary containing the data for a given range of sentences with only one function. \\n\n",
    "    The same results can be obtained by parts with apply_tokenization and final_formating functions. This one is more a convenience function.\\n\n",
    "    It is also possible to obtain a csv file if csv is set to True and a path is provided in output_save.\n",
    "\n",
    "    Args:\n",
    "        conll_path (str): path to the conll file\n",
    "        new_id (dict): dictionnary to change the labels\n",
    "        columns (list): columns to keep\n",
    "        new_names (dict): new names of the columns\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionnary containing the data\n",
    "    \"\"\"\n",
    "    df = apply_tokenization(conll_path, new_id=new_id, columns=columns, new_names=new_names)\n",
    "\n",
    "    if csv:\n",
    "        df.to_csv(output_save, index=False)\n",
    "\n",
    "    data = final_formating(df, start, end)\n",
    "    return data, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de 'aligned_labels' faux: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at Jean-Baptiste/camembert-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([31]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([31, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"formated_labels\",\"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "conll_path = r\"../../data/raw/data449.conll\"\n",
    "start = 0\n",
    "end = 449\n",
    "output_save = r\"../../data/intermediate/data449_token_V2.csv\"\n",
    "\n",
    "data, df_final = download_and_treat_data(conll_path=conll_path, new_id=new_id, columns=columns, new_names=new_names, start=start, end=end, csv=True, output_save=output_save)\n",
    "\n",
    "train_data, test_data, val_data = split_data(data)\n",
    "\n",
    "# Create a Dataset object\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "\n",
    "# Create a list of labels\n",
    "reverse_id = {v: k for k, v in new_id.items()}\n",
    "second_elements = [value for value in reverse_id.values()]\n",
    "label_list = second_elements\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_checkpoint = \"Jean-Baptiste/camembert-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\", num_labels=len(label_list), ignore_mismatched_sizes=True) #this last argument might be a mistake\n",
    "\n",
    "# import data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors='pt')\n",
    "\n",
    "# Dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text               [[, ', -, D, O, C, S, T, A, R, T, -, ', ,,  , ...\n",
      "formated_labels    [[, ', O, ', ,,  , ', O, ', ,,  , ', O, ', ,, ...\n",
      "label              [[, 0, ,,  , 0, ,,  , 0, ,,  , 0, ,,  , 0, ,, ...\n",
      "word_ids           [[, N, o, n, e, ,,  , 0, ,,  , 0, ,,  , 0, ,, ...\n",
      "input_ids          [[, 5, ,,  , 6, 7, ,,  , 3, 4, 2, ,,  , 5, 3, ...\n",
      "attention_mask     [[, 1, ,,  , 1, ,,  , 1, ,,  , 1, ,,  , 1, ,, ...\n",
      "aligned_labels     [[, -, 1, 0, 0, ,,  , 0, ,,  , 0, ,,  , 0, ,, ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# I try to check this dimension \n",
    "column_types = {\n",
    "    'text': list,\n",
    "    'formated_labels': list,\n",
    "    'label': list,\n",
    "    'word_ids': list,\n",
    "    'input_ids': list,\n",
    "    'attention_mask': list,\n",
    "    'aligned_labels': list,\n",
    "}\n",
    "\n",
    "\n",
    "df_dim = pd.read_csv(\"../../data/intermediate/data449_token_V2.csv\", converters=column_types)\n",
    "print(df_dim.iloc[0])\n",
    "\n",
    "# print(type(df_dim[\"formated_labels\"].iloc[0]))\n",
    "# print(df_dim[\"formated_labels\"].iloc[0])\n",
    "# df_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>NER_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155097</th>\n",
       "      <td>sur</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155098</th>\n",
       "      <td>la</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155099</th>\n",
       "      <td>paie</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155100</th>\n",
       "      <td>du</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155101</th>\n",
       "      <td>mois</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155102</th>\n",
       "      <td>de</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155103</th>\n",
       "      <td>décembre</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155104</th>\n",
       "      <td>2021.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155105</th>\n",
       "      <td>la</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155106</th>\n",
       "      <td>prime</td>\n",
       "      <td>B-PPV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155107</th>\n",
       "      <td>exceptionnelle</td>\n",
       "      <td>I-PPV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155108</th>\n",
       "      <td>est</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155109</th>\n",
       "      <td>octroyée</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155110</th>\n",
       "      <td>à</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155111</th>\n",
       "      <td>tous</td>\n",
       "      <td>B-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155112</th>\n",
       "      <td>les</td>\n",
       "      <td>I-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155113</th>\n",
       "      <td>salariés</td>\n",
       "      <td>I-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155114</th>\n",
       "      <td>de</td>\n",
       "      <td>I-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155115</th>\n",
       "      <td>l'entreprise</td>\n",
       "      <td>I-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155116</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text NER_tags\n",
       "155097             sur        O\n",
       "155098              la        O\n",
       "155099            paie        O\n",
       "155100              du        O\n",
       "155101            mois        O\n",
       "155102              de        O\n",
       "155103        décembre        O\n",
       "155104           2021.        O\n",
       "155105              la        O\n",
       "155106           prime    B-PPV\n",
       "155107  exceptionnelle    I-PPV\n",
       "155108             est        O\n",
       "155109        octroyée        O\n",
       "155110               à        O\n",
       "155111            tous   B-TOUS\n",
       "155112             les   I-TOUS\n",
       "155113        salariés   I-TOUS\n",
       "155114              de   I-TOUS\n",
       "155115    l'entreprise   I-TOUS\n",
       "155116               .        O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(conll_path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "lines = conll_data.strip().split('\\n')\n",
    "data = [line.split() for line in lines]\n",
    "# data\n",
    "\n",
    "# tokens = []\n",
    "# for line in data:\n",
    "#     if line != []:\n",
    "#         print(line)\n",
    "#         print(line[-1])\n",
    "\n",
    "# # Extract tokens and labels into separate lists\n",
    "\n",
    "tokens = [line[0] for line in data if line]\n",
    "labels = [line[-1] for line in data if line]\n",
    "\n",
    "# # Create a list of lists containing tokens and labels\n",
    "result = list(zip(tokens, labels))\n",
    "column_names = [\"text\", \"NER_tags\"]\n",
    "df = pd.DataFrame(result, columns=column_names)\n",
    "df.tail(20)\n",
    "\n",
    "# # df_org[\"formated_labels\"] = df_org[\"label\"].apply(lambda x: format_labels(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# training arguments\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# metric used for evaluation\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# We want to do a metrics function to compute the accuracy of the model\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/135 : < :, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 14.61 GiB of which 8.69 MiB is free. Process 3510010 has 7.75 GiB memory in use. Process 1476790 has 6.84 GiB memory in use. Of the allocated memory 5.83 GiB is allocated by PyTorch, and 84.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(p)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2776\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2779\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:2801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/models/camembert/modeling_camembert.py:1268\u001b[0m, in \u001b[0;36mCamembertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1268\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1280\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1282\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/models/camembert/modeling_camembert.py:897\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    888\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    890\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    891\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    892\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    896\u001b[0m )\n\u001b[0;32m--> 897\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    910\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/models/camembert/modeling_camembert.py:543\u001b[0m, in \u001b[0;36mCamembertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    534\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    535\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    536\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    541\u001b[0m     )\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/models/camembert/modeling_camembert.py:427\u001b[0m, in \u001b[0;36mCamembertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/models/camembert/modeling_camembert.py:354\u001b[0m, in \u001b[0;36mCamembertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    346\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 354\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    364\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/models/camembert/modeling_camembert.py:250\u001b[0m, in \u001b[0;36mCamembertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    247\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    253\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 14.61 GiB of which 8.69 MiB is free. Process 3510010 has 7.75 GiB memory in use. Process 1476790 has 6.84 GiB memory in use. Of the allocated memory 5.83 GiB is allocated by PyTorch, and 84.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## try with conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_conll(path:str):\n",
    "    # Initialize a list to store the data\n",
    "    conll_data = []\n",
    "\n",
    "    # Open the CoNLL file in read mode\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "        # Read each line in the file\n",
    "\n",
    "    # Split the data into sentences\n",
    "    sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "    # Initialize empty lists to store text and labels\n",
    "    text_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split('\\n')\n",
    "        text = \" \".join(token.split()[0] for token in tokens)\n",
    "        labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path to your CoNLL file\n",
    "conll_file_path = r'../../data/raw/data449.conll'  # Replace with your file path\n",
    "\n",
    "# Initialize a list to store the data\n",
    "conll_data = []\n",
    "\n",
    "# Open the CoNLL file in read mode\n",
    "with open(conll_file_path, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "    # Read each line in the file\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists to store text and labels\n",
    "text_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    text_list.append(text)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]\n",
    "\n",
    "# def turn_sentence_to_list(sentence):\n",
    "#     \"\"\"\n",
    "#     Turn a sentence into a list of words\n",
    "\n",
    "#     Args:\n",
    "#         sentence (str): sentence to tokenize\n",
    "\n",
    "#     Returns:\n",
    "#         list: list of words\n",
    "#     \"\"\"\n",
    "#     return sentence.split()\n",
    "\n",
    "\n",
    "def list_to_string(input_list, separator=' '):\n",
    "    \"\"\"\n",
    "    Convert a list of strings into a single string with elements separated by a specified separator.\n",
    "\n",
    "    :param input_list: List of strings to be converted.\n",
    "    :param separator: The separator to use between elements (default is a space).\n",
    "    :return: A single string containing the list elements.\n",
    "    \"\"\"\n",
    "    return separator.join(input_list)\n",
    "\n",
    "def change_ids(list_ids:list, new_id:dict):\n",
    "    return [new_id.get(id) for id in list_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "\n",
    "df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"new_labels\"][5])\n",
    "print(df[\"label\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check that we haven't left non-numeric labels\n",
    "\n",
    "\n",
    "def check_labels(list_labels:list):\n",
    "    for label in list_labels:\n",
    "        if type(label)!=int:\n",
    "            print(label)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# amount = [df[\"new_labels\"].apply(lambda x: check_labels(x))]\n",
    "\n",
    "def count_error(dataframe, column:str):\n",
    "    \"\"\"\n",
    "    Count the number of errors - a cell containing a non numerical value - in a column of a dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): dataframe to check\n",
    "        column (str): column to check\n",
    "\n",
    "    Returns:\n",
    "        int: number of errors\n",
    "    \"\"\"\n",
    "    return len(df[column])- sum(dataframe[column].apply(lambda x: check_labels(x)))\n",
    "\n",
    "# print(f\"nombre d'erreurs: {count_error(df, 'new_labels')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"text\"].tolist()\n",
    "start = 0\n",
    "\n",
    "for text in texts: \n",
    "    text = turn_sentence_to_list(text)\n",
    "    tokens = tokenizer(text, is_split_into_words=True)\n",
    "    tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "    length = len(tokens_mod)\n",
    "    if length > 512:\n",
    "        start += 1\n",
    "        print(length)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JB NER is probably not suited since we lost 182 texts, which is too much, due to context limit. We can eventually try to split the text into 512 tokens chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample data in CoNLL format\n",
    "conll_data = r'../../data/raw/data449.conll'\n",
    "\n",
    "with open(conll_data, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists and a variable to keep track of the current sentence length\n",
    "text_list = []\n",
    "labels_list = []\n",
    "current_length = 0\n",
    "\n",
    "# Define the maximum token length for a chunk\n",
    "max_chunk_length = 512\n",
    "\n",
    "# Initialize a dictionary to keep track of chunks\n",
    "chunked_data = defaultdict(list)\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    \n",
    "    # Check if adding this sentence will exceed the maximum chunk length\n",
    "    if current_length + len(tokens) <= max_chunk_length:\n",
    "        # Add this sentence to the current chunk\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "        current_length += len(tokens)\n",
    "    else:\n",
    "        # Start a new chunk\n",
    "        chunked_data['text'].append(text_list)\n",
    "        chunked_data['label'].append(labels_list)\n",
    "        \n",
    "        # Reset the current chunk\n",
    "        text_list = [text]\n",
    "        labels_list = [labels]\n",
    "        current_length = len(tokens)\n",
    "\n",
    "# Add the last chunk\n",
    "chunked_data['text'].append(text_list)\n",
    "chunked_data['label'].append(labels_list)\n",
    "\n",
    "# Create a dataframe from the chunked data\n",
    "df = pd.DataFrame(chunked_data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def format_text(df):\n",
    "    formated_text = []\n",
    "\n",
    "    text = df[\"text\"]\n",
    "    for i in text:\n",
    "        i = turn_sentence_to_list(i)\n",
    "        formated_text.append(i)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "    df[\"formated_text\"] = formated_text\n",
    "    return df\n",
    "\n",
    "# formated_text = []\n",
    "\n",
    "# text = df[\"text\"]\n",
    "# for i in text:\n",
    "#     i = turn_sentence_to_list(i)\n",
    "#     formated_text.append(i)\n",
    "\n",
    "# # we add it to the dataframe\n",
    "# df[\"formated_text\"] = formated_text\n",
    "# df = format_text(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we format the label to the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(df):\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    formated_labels = []\n",
    "\n",
    "    for label in labels:\n",
    "        # label = list_to_string(label)\n",
    "        label = turn_sentence_to_list(label)\n",
    "        formated_labels.append(label)\n",
    "        # print(label)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "\n",
    "    df[\"formated_labels\"] = formated_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, columns:list):\n",
    "    df = df[columns].copy()\n",
    "    return df\n",
    "\n",
    "# df = select_columns(df, columns=[\"formated_text\", \"formated_labels\"])\n",
    "\n",
    "# df = df.rename(columns={\"formated_text\": \"text\", \"formated_labels\": \"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the length of the two columns so that they are of the same dimensions\n",
    "\n",
    "start = 0\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    if len(df['text'][i]) != len(df['label'][i]):\n",
    "        start += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"les éléments dont la dimension diverge sont au nombre de\",start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[\"text\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(df[\"text\"][0], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_input = tokenizer(df['text'][0], is_split_into_words=True)\n",
    "print(token_input)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(token_input[\"input_ids\"])\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_ids = token_input.word_ids()\n",
    "# print(word_ids)\n",
    "\n",
    "def align_labels(word_ids:list, tag_list:list):\n",
    "    aligned_labels = []\n",
    "    for i in word_ids:\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "        else:\n",
    "            aligned_labels.append(tag_list[i])\n",
    "    return aligned_labels\n",
    "\n",
    "# aligned_labels = align_labels(word_ids, tag_list=df[\"new_labels\"][0])\n",
    "# aligned_labels = create_aligned_labels(word_ids, example=df)\n",
    "\n",
    "# print(aligned_labels)\n",
    "# print(len(aligned_labels))\n",
    "# print(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_ids\"] = df[\"text\"].apply(lambda x: tokenizer(x, is_split_into_words=True).word_ids())\n",
    "df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"label\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(tokens, aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iids_am(df):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for _,i in df.iterrows():\n",
    "        input = i[\"ids\"][\"input_ids\"]\n",
    "        attention = i[\"ids\"][\"attention_mask\"]\n",
    "\n",
    "        input_ids.append(input)\n",
    "        attention_mask.append(attention)\n",
    "\n",
    "    df[\"input_ids\"] = input_ids\n",
    "    df[\"attention_mask\"] = attention_mask\n",
    "    return df\n",
    "\n",
    "# df = create_iids_am(df)\n",
    "# df[[\"input_ids\", \"attention_mask\"]].head()\n",
    "# df_ids = df[[\"ids\"]].copy()\n",
    "# df_ids[\"ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "\n",
    "def tokenize_and_align_data(path_conll:str, new_id:dict):\n",
    "    #charge and create the df\n",
    "    df = charge_conll(path_conll)\n",
    "    df = format_text(df)\n",
    "    df = format_labels(df)\n",
    "    df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))\n",
    "\n",
    "    #tokenize and align the text\n",
    "    df[\"ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True))\n",
    "    df = create_iids_am(df)\n",
    "    df[\"word_ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True).word_ids())\n",
    "    df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"new_labels\"]), axis=1)\n",
    "    return df\n",
    "\n",
    "def apply_tokenization(conll_path:str, new_id:dict, columns:list, new_names:dict, save_path=False, output_save=None):\n",
    "    df = tokenize_and_align_data(conll_path, new_id=new_id)\n",
    "    df = select_columns(df, columns)\n",
    "    df = df.rename(columns=new_names)\n",
    "    #we check the length of the two columns so that they are of the same dimensions\n",
    "    # print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "    if save_path:\n",
    "        df.to_csv(output_save, index=False)\n",
    "    return df\n",
    "\n",
    "# df = apply_tokenization(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id, columns=columns, new_names=new_names, save_path=True, output_save=r\"../../data/intermediate/data499_token.csv\")\n",
    "# df.head()\n",
    "# df = tokenize_and_align_data(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id)\n",
    "# df = select_columns(df, columns)\n",
    "# df = df.rename(columns=new_names)\n",
    "# #we check the length of the two columns so that they are of the same dimensions\n",
    "# print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "# # df.to_csv(r\"../../data/intermediate/data499_token.csv\", index=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = df[\"text\"].tolist()\n",
    "\n",
    "for i in list_text:\n",
    "    if len(i) > 512:\n",
    "        print(len(i))\n",
    "    else:\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the objective is to collect all the inputs_ids, attention_mask and labels in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "def final_formating(df, start:int, end:int):\n",
    "    \"\"\"\n",
    "    Collect the data from a dataframe in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "\n",
    "    Returns:\n",
    "        list: list of the data\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        input_ids.append(df[\"input_ids\"][i])\n",
    "        attention_mask.append(df[\"attention_mask\"][i])\n",
    "        labels.append(df[\"aligned_labels\"][i])\n",
    "\n",
    "    data = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "    return data\n",
    "    \n",
    "# data = final_formating(df, 0, 5)\n",
    "# print(data)\n",
    "# for _,i in df.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_treat_data(conll_path:str, new_id:dict, columns:list, new_names:dict, start:int, end:int, csv=False, output_save=None):\n",
    "    \"\"\"\n",
    "    This function compile all the previous one and treat automatically all data provided as conll format.\\n\n",
    "    The objective is to provide a dictionnary containing the data for a given range of sentences with only one function. \\n\n",
    "    The same results can be obtained by parts with apply_tokenization and final_formating functions. This one is more a convenience function.\\n\n",
    "    It is also possible to obtain a csv file if csv is set to True and a path is provided in output_save.\n",
    "\n",
    "    Args:\n",
    "        conll_path (str): path to the conll file\n",
    "        new_id (dict): dictionnary to change the labels\n",
    "        columns (list): columns to keep\n",
    "        new_names (dict): new names of the columns\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionnary containing the data\n",
    "    \"\"\"\n",
    "    df = apply_tokenization(conll_path, new_id=new_id, columns=columns, new_names=new_names)\n",
    "\n",
    "    if csv:\n",
    "        df.to_csv(output_save, index=False)\n",
    "\n",
    "    data = final_formating(df, start, end)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"formated_labels\",\"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "conll_path = r\"..\\..\\data\\raw\\data449.conll\"\n",
    "start = 0\n",
    "end = 449\n",
    "\n",
    "data = download_and_treat_data(conll_path=conll_path, new_id=new_id, columns=columns, new_names=new_names, start=start, end=end, csv=True, output_save=r\"../../data/intermediate/data449_token.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we divide the data into train, test, and validation sets\n",
    "\n",
    "def split_data(data_dict, train_size=0.8, test_size=0.1, val_size=0.1, random_seed=None):\n",
    "    # Set a random seed for reproducibility\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Combine input_ids, attention_mask, and aligned_labels into a single list\n",
    "    combined_data = list(zip(data_dict['input_ids'], data_dict['attention_mask'], data_dict['labels']))\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    total_size = len(combined_data)\n",
    "    train_size = int(train_size * total_size)\n",
    "    test_size = int(test_size * total_size)\n",
    "    val_size = int(val_size * total_size)\n",
    "\n",
    "    # Split the data into train, test, and val sets\n",
    "    train_data = combined_data[:train_size]\n",
    "    test_data = combined_data[train_size:train_size + test_size]\n",
    "    val_data = combined_data[train_size + test_size:train_size + test_size + val_size]\n",
    "\n",
    "    # Unzip the data to restore the original structure\n",
    "    train_input_ids, train_attention_mask, train_aligned_labels = zip(*train_data)\n",
    "    test_input_ids, test_attention_mask, test_aligned_labels = zip(*test_data)\n",
    "    val_input_ids, val_attention_mask, val_aligned_labels = zip(*val_data)\n",
    "\n",
    "    # Create dictionaries for the train, test, and val sets\n",
    "    train_set = {\n",
    "        'input_ids': list(train_input_ids),\n",
    "        'attention_mask': list(train_attention_mask),\n",
    "        'labels': list(train_aligned_labels)\n",
    "    }\n",
    "    test_set = {\n",
    "        'input_ids': list(test_input_ids),\n",
    "        'attention_mask': list(test_attention_mask),\n",
    "        'labels': list(test_aligned_labels)\n",
    "    }\n",
    "    val_set = {\n",
    "        'input_ids': list(val_input_ids),\n",
    "        'attention_mask': list(val_attention_mask),\n",
    "        'labels': list(val_aligned_labels)\n",
    "    }\n",
    "\n",
    "    return train_set, test_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, val_data = split_data(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get rid of the list with a dimension superior to 512 as the model doesn't hold more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn into tensors our data so that BERT can read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "val_dataset = Dataset.from_dict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of labels\n",
    "reverse_id = {v: k for k, v in new_id.items()}\n",
    "second_elements = [value for value in reverse_id.values()]\n",
    "label_list = second_elements\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"Jean-Baptiste/camembert-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\", num_labels=len(label_list), ignore_mismatched_sizes=True) #this last argument might be a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**(next(iter(train_dataloader))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_token = pd.read_csv(r\"../../data/intermediate/data449_token.csv\")\n",
    "data_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(label_string):\n",
    "    # Split the string by spaces and remove any leading/trailing spaces\n",
    "    labels_list = label_string.strip().split()\n",
    "    return labels_list\n",
    "\n",
    "list_tags = data_token[\"formated_labels\"].apply(lambda x: string_to_list(x))\n",
    "\n",
    "print(list_tags)\n",
    "\n",
    "labels = [label_list[i] for i in list_tags]\n",
    "# metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to do a metrics function to compute the accuracy of the model\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drafty draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = df[['input_ids']].copy()\n",
    "print(df_token[\"input_ids\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"aligned_labels\"][0])\n",
    "df_test = df[\"aligned_labels\"].apply(lambda x :change_ids(x, new_id=new_id))\n",
    "print(df_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "aligned_labels = [-100 if i is None else example[f\"label\"][i] for i in word_ids]\n",
    "print(example[\"label\"])\n",
    "print(len(word_ids))\n",
    "print(len(aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(tokens_mod, aligned_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset for training from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you like it you can download it\n",
    "data = import_label_studio_data(\"../../data/raw/data449.json\")\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df[\"text\"])):\n",
    "    df[\"text\"][i] = turn_sentence_to_list(df[\"text\"][i])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    element = df[\"text\"][i]\n",
    "    length.append(len(element))\n",
    "    # print(len(df[\"text\"][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "plt.hist(length, bins=20, color = \"lightgreen\", edgecolor='black') \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Longeur (élément)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Histogram de la longueurs des phrases en élément')\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text': [\"evolution des salaires de base : enveloppe\"],\n",
    "    'label': [{'entities': [(22, 31, 'ATOT')]}]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to tokenize text while preserving whitespace\n",
    "def tokenize_text(row):\n",
    "    text = row['text']\n",
    "    tokens = []\n",
    "    start = 0\n",
    "\n",
    "    for start, end, entity in row['label']['entities']:\n",
    "        # Add non-entity text\n",
    "        tokens.extend(text[start:end].split())\n",
    "        start = end\n",
    "\n",
    "    # Add any remaining text after the last entity\n",
    "    tokens.extend(text[start:].split())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df['tokenized_text'] = df.apply(tokenize_text, axis=1)\n",
    "\n",
    "# Display the DataFrame with tokenized text while preserving whitespace\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df[\"label\"])):\n",
    "    print(df[\"label\"][i][\"entities\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'][0]\n",
    "text = turn_sentence_to_list(text)\n",
    "# print(type(text))\n",
    "tokens = tokenizer(text, is_split_into_words=True)\n",
    "# print(tokens)\n",
    "\n",
    "tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "print(tokens_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokens.word_ids()\n",
    "# aligned_labels = [-100 if i is None else text[\"label\"][i] for i in word_ids]\n",
    "for i in word_ids:\n",
    "    # print(i)\n",
    "    if i is None:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(text[\"label\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

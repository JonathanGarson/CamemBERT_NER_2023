{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning of BERT for NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the steps for the finetuning of French version of BERT, the objective is to extract specific information from french public business agreements. For this we have a dual approach, first we classify information and then only we use NER property of our model to extract wage variations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: This notebook is inspired from a masterclass and notebook provided by [Thomas Boehler](https://fr.linkedin.com/in/thomas-boehler-ba34a744) and also the work of my colleagues [Mouad Bernoussi](https://ma.linkedin.com/in/mouad-bernoussi-00aa91242) (see ./notebooks/external) and [Conrad Thiounn](https://github.com/cthiounn)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Play with PyTorch\n",
    "2. Play with the model \n",
    "3. Adapt data to BERT format (see Thomas Boehler)\n",
    "4. Create training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaner draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model_checkpoint = \"Jean-Baptiste/camembert-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\") #this last argument might be a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForTokenClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# This will print the device (either 'cuda' or 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# You can then move your model and data to this device like this:\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_conll(path:str):\n",
    "    # Initialize a list to store the data\n",
    "    conll_data = []\n",
    "\n",
    "    # Open the CoNLL file in read mode\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "        # Read each line in the file\n",
    "\n",
    "    # Split the data into sentences\n",
    "    sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "    # Initialize empty lists to store text and labels\n",
    "    text_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split('\\n')\n",
    "        text = \" \".join(token.split()[0] for token in tokens)\n",
    "        labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "    return df\n",
    "\n",
    "#we divide the data into train, test, and validation sets\n",
    "\n",
    "def split_data(data_dict, train_size=0.8, test_size=0.1, val_size=0.1, random_seed=None):\n",
    "    # Set a random seed for reproducibility\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Combine input_ids, attention_mask, and aligned_labels into a single list\n",
    "    combined_data = list(zip(data_dict['input_ids'], data_dict['attention_mask'], data_dict['labels']))\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    total_size = len(combined_data)\n",
    "    train_size = int(train_size * total_size)\n",
    "    test_size = int(test_size * total_size)\n",
    "    val_size = int(val_size * total_size)\n",
    "\n",
    "    # Split the data into train, test, and val sets\n",
    "    train_data = combined_data[:train_size]\n",
    "    test_data = combined_data[train_size:train_size + test_size]\n",
    "    val_data = combined_data[train_size + test_size:train_size + test_size + val_size]\n",
    "\n",
    "    # Unzip the data to restore the original structure\n",
    "    train_input_ids, train_attention_mask, train_aligned_labels = zip(*train_data)\n",
    "    test_input_ids, test_attention_mask, test_aligned_labels = zip(*test_data)\n",
    "    val_input_ids, val_attention_mask, val_aligned_labels = zip(*val_data)\n",
    "\n",
    "    # Create dictionaries for the train, test, and val sets\n",
    "    train_set = {\n",
    "        'input_ids': list(train_input_ids),\n",
    "        'attention_mask': list(train_attention_mask),\n",
    "        'labels': list(train_aligned_labels)\n",
    "    }\n",
    "    test_set = {\n",
    "        'input_ids': list(test_input_ids),\n",
    "        'attention_mask': list(test_attention_mask),\n",
    "        'labels': list(test_aligned_labels)\n",
    "    }\n",
    "    val_set = {\n",
    "        'input_ids': list(val_input_ids),\n",
    "        'attention_mask': list(val_attention_mask),\n",
    "        'labels': list(val_aligned_labels)\n",
    "    }\n",
    "\n",
    "    return train_set, test_set, val_set\n",
    "\n",
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of words\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of words\n",
    "    \"\"\"\n",
    "    return sentence.split()\n",
    "\n",
    "def format_text(df):\n",
    "    formated_text = []\n",
    "\n",
    "    text = df[\"text\"]\n",
    "    for i in text:\n",
    "        i = turn_sentence_to_list(i)\n",
    "        formated_text.append(i)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "    df[\"formated_text\"] = formated_text\n",
    "    return df\n",
    "\n",
    "def format_labels(df):\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    formated_labels = []\n",
    "\n",
    "    for label in labels:\n",
    "        label = list_to_string(label)\n",
    "        label = turn_sentence_to_list(label)\n",
    "        formated_labels.append(label)\n",
    "        # print(label)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "\n",
    "    df[\"formated_labels\"] = formated_labels\n",
    "    return df\n",
    "\n",
    "def change_ids(list_ids:list, new_id:dict):\n",
    "    \"\"\"\n",
    "    change the ids of a list of ids to a new id\n",
    "\n",
    "    Args:\n",
    "        list_ids (list): list of ids\n",
    "        new_id (dict): dictionary with the new id\n",
    "    \"\"\"\n",
    "    return [new_id.get(id) for id in list_ids]\n",
    "\n",
    "def align_labels(word_ids:list, tag_list:list):\n",
    "    \"\"\"\n",
    "    Align the labels with the words\n",
    "\n",
    "    Args:\n",
    "        word_ids (list): list of ids of the words\n",
    "        tag_list (list): list of tags\n",
    "    \"\"\"\n",
    "    aligned_labels = []\n",
    "    for i in word_ids:\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "        else:\n",
    "            aligned_labels.append(tag_list[i])\n",
    "    return aligned_labels\n",
    "\n",
    "def check_labels(list_labels:list):\n",
    "    for label in list_labels:\n",
    "        if type(label)!=int:\n",
    "            print(label)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def count_error(df, column:str):\n",
    "    \"\"\"\n",
    "    Count the number of errors - a cell containing a non numerical value - in a column of a dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): dataframe to check\n",
    "        column (str): column to check\n",
    "\n",
    "    Returns:\n",
    "        int: number of errors\n",
    "    \"\"\"\n",
    "    return len(df[column])- sum(df[column].apply(lambda x: check_labels(x)))\n",
    "\n",
    "def create_iids_am(df):\n",
    "    \"\"\"\n",
    "    Create the input ids and attention mask columns\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe containing the data with the input ids and attention mask columns\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for _,i in df.iterrows():\n",
    "        input = i[\"ids\"][\"input_ids\"]\n",
    "        attention = i[\"ids\"][\"attention_mask\"]\n",
    "\n",
    "        input_ids.append(input)\n",
    "        attention_mask.append(attention)\n",
    "\n",
    "    df[\"input_ids\"] = input_ids\n",
    "    df[\"attention_mask\"] = attention_mask\n",
    "    return df\n",
    "\n",
    "def select_columns(df, columns:list):\n",
    "    \"\"\"\n",
    "    Select columns from a dataframe\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe\n",
    "        columns (list): list of columns to select\n",
    "\n",
    "    \"\"\"\n",
    "    df = df[columns].copy()\n",
    "    return df\n",
    "\n",
    "def tokenize_and_align_data(path_conll:str, new_id:dict):\n",
    "    \"\"\"\n",
    "    Tokenize the text and align the labels\n",
    "\n",
    "    Args:\n",
    "        path_conll (str): path to the conll file\n",
    "        new_id (dict): dictionary with the new id\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe containing the data\n",
    "    \"\"\"\n",
    "    #charge and create the df\n",
    "    df = charge_conll(path_conll)\n",
    "    df = format_text(df)\n",
    "    df = format_labels(df)\n",
    "    df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))\n",
    "\n",
    "    #tokenize and align the text\n",
    "    df[\"ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True))\n",
    "    df = create_iids_am(df)\n",
    "    df[\"word_ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True).word_ids())\n",
    "    df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"new_labels\"]), axis=1)\n",
    "    return df\n",
    "\n",
    "def apply_tokenization(conll_path:str, new_id:dict, columns:list, new_names:dict, save_path=False, output_save_csv=None):\n",
    "    \"\"\"\n",
    "    This function apply the tokenization and alignment to a conll file and select the columns we want to keep\n",
    "\n",
    "    Args:\n",
    "        conll_path (str): path to the conll file\n",
    "        new_id (dict): dictionary with the new id\n",
    "        columns (list): list of columns to keep\n",
    "        new_names (dict): dictionary with the new names of the columns\n",
    "        save_path (bool, optional): if True, save the dataframe in a csv file. Defaults to False.\n",
    "        output_save_csv ([type], optional): path to the csv file. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe containing the data\n",
    "    \"\"\"\n",
    "    df = tokenize_and_align_data(conll_path, new_id=new_id)\n",
    "    df = select_columns(df, columns)\n",
    "    df = df.rename(columns=new_names)\n",
    "    #we check the length of the two columns so that they are of the same dimensions\n",
    "    print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "    if save_path:\n",
    "        df.to_csv(output_save_csv, index=False)\n",
    "    return df\n",
    "\n",
    "def final_formating(df, start:int, end:int):\n",
    "    \"\"\"\n",
    "    Collect the data from a dataframe in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionnary containing the data\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        input_ids.append(df[\"input_ids\"][i])\n",
    "        attention_mask.append(df[\"attention_mask\"][i])\n",
    "        labels.append(df[\"aligned_labels\"][i])\n",
    "\n",
    "    data = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "    return data\n",
    "\n",
    "def download_and_treat_data(conll_path:str, new_id:dict, columns:list, new_names:dict, start:int, end:int, csv=False, output_save=None):\n",
    "    \"\"\"\n",
    "    This function compile all the previous one and treat automatically all data provided as conll format.\\n\n",
    "    The objective is to provide a dictionnary containing the data for a given range of sentences with only one function. \\n\n",
    "    The same results can be obtained by parts with apply_tokenization and final_formating functions. This one is more a convenience function.\\n\n",
    "    It is also possible to obtain a csv file if csv is set to True and a path is provided in output_save.\n",
    "\n",
    "    Args:\n",
    "        conll_path (str): path to the conll file\n",
    "        new_id (dict): dictionnary to change the labels\n",
    "        columns (list): columns to keep\n",
    "        new_names (dict): new names of the columns\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionnary containing the data\n",
    "    \"\"\"\n",
    "    df = apply_tokenization(conll_path, new_id=new_id, columns=columns, new_names=new_names)\n",
    "\n",
    "    if csv:\n",
    "        df.to_csv(output_save, index=False)\n",
    "\n",
    "    data = final_formating(df, start, end)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_to_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m449\u001b[39m\n\u001b[1;32m      7\u001b[0m output_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/intermediate/data449_token_V2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_treat_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconll_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconll_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_save\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m train_data, test_data, val_data \u001b[38;5;241m=\u001b[39m split_data(data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a Dataset object\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 292\u001b[0m, in \u001b[0;36mdownload_and_treat_data\u001b[0;34m(conll_path, new_id, columns, new_names, start, end, csv, output_save)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_treat_data\u001b[39m(conll_path:\u001b[38;5;28mstr\u001b[39m, new_id:\u001b[38;5;28mdict\u001b[39m, columns:\u001b[38;5;28mlist\u001b[39m, new_names:\u001b[38;5;28mdict\u001b[39m, start:\u001b[38;5;28mint\u001b[39m, end:\u001b[38;5;28mint\u001b[39m, csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, output_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    275\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    This function compile all the previous one and treat automatically all data provided as conll format.\\n\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    The objective is to provide a dictionnary containing the data for a given range of sentences with only one function. \\n\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m        dict: dictionnary containing the data\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mapply_tokenization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconll_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m csv:\n\u001b[1;32m    295\u001b[0m         df\u001b[38;5;241m.\u001b[39mto_csv(output_save, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[21], line 240\u001b[0m, in \u001b[0;36mapply_tokenization\u001b[0;34m(conll_path, new_id, columns, new_names, save_path, output_save_csv)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_tokenization\u001b[39m(conll_path:\u001b[38;5;28mstr\u001b[39m, new_id:\u001b[38;5;28mdict\u001b[39m, columns:\u001b[38;5;28mlist\u001b[39m, new_names:\u001b[38;5;28mdict\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, output_save_csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    This function apply the tokenization and alignment to a conll file and select the columns we want to keep\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m        pd.DataFrame: dataframe containing the data\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_and_align_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconll_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     df \u001b[38;5;241m=\u001b[39m select_columns(df, columns)\n\u001b[1;32m    242\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39mnew_names)\n",
      "Cell \u001b[0;32mIn[21], line 215\u001b[0m, in \u001b[0;36mtokenize_and_align_data\u001b[0;34m(path_conll, new_id)\u001b[0m\n\u001b[1;32m    213\u001b[0m df \u001b[38;5;241m=\u001b[39m charge_conll(path_conll)\n\u001b[1;32m    214\u001b[0m df \u001b[38;5;241m=\u001b[39m format_text(df)\n\u001b[0;32m--> 215\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mformat_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: change_ids(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m), new_id))\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m#tokenize and align the text\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 109\u001b[0m, in \u001b[0;36mformat_labels\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    106\u001b[0m formated_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m--> 109\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlist_to_string\u001b[49m(label)\n\u001b[1;32m    110\u001b[0m     label \u001b[38;5;241m=\u001b[39m turn_sentence_to_list(label)\n\u001b[1;32m    111\u001b[0m     formated_labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_to_string' is not defined"
     ]
    }
   ],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"formated_labels\",\"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "conll_path = r\"../../data/raw/data449.conll\"\n",
    "start = 0\n",
    "end = 449\n",
    "output_save = r\"../../data/intermediate/data449_token_V2.csv\"\n",
    "\n",
    "data = download_and_treat_data(conll_path=conll_path, new_id=new_id, columns=columns, new_names=new_names, start=start, end=end, csv=True, output_save=output_save)\n",
    "\n",
    "train_data, test_data, val_data = split_data(data)\n",
    "\n",
    "# Create a Dataset object\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "\n",
    "# Create a list of labels\n",
    "reverse_id = {v: k for k, v in new_id.items()}\n",
    "second_elements = [value for value in reverse_id.values()]\n",
    "label_list = second_elements\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_checkpoint = \"Jean-Baptiste/camembert-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\", num_labels=len(label_list), ignore_mismatched_sizes=True) #this last argument might be a mistake\n",
    "\n",
    "# import data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors='pt')\n",
    "\n",
    "# Dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>formated_labels</th>\n",
       "      <th>label</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>aligned_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['-DOCSTART-', 'evolution', 'des', 'salaires',...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "      <td>[5, 67, 342, 5363, 7486, 11485, 26, 599, 14573...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['l’enveloppe', 'globale', 'd’augmentation', '...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 5, 5, 6, ...</td>\n",
       "      <td>[5, 17, 12, 16886, 4141, 18, 12, 5314, 20, 601...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['dispositions', 'au', 'regard', 'de', 'l’impl...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[5, 2756, 36, 897, 8, 17, 12, 16286, 8, 117, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nous', 'travaillons', 'sur', 'une', 'politiq...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[5, 63, 13807, 32, 28, 462, 8, 6016, 1344, 38,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['protocole', 'd’accord', 'négociation', 'annu...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, ...</td>\n",
       "      <td>[5, 5996, 18, 12, 1311, 8776, 5998, 3329, 325,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['-DOCSTART-', 'evolution', 'des', 'salaires',...   \n",
       "1  ['l’enveloppe', 'globale', 'd’augmentation', '...   \n",
       "2  ['dispositions', 'au', 'regard', 'de', 'l’impl...   \n",
       "3  ['nous', 'travaillons', 'sur', 'une', 'politiq...   \n",
       "4  ['protocole', 'd’accord', 'négociation', 'annu...   \n",
       "\n",
       "                                     formated_labels  \\\n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "\n",
       "                                               label  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...   \n",
       "\n",
       "                                            word_ids  \\\n",
       "0  [None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, ...   \n",
       "1  [None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 5, 5, 6, ...   \n",
       "2  [None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "3  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "4  [None, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573...   \n",
       "1  [5, 17, 12, 16886, 4141, 18, 12, 5314, 20, 601...   \n",
       "2  [5, 2756, 36, 897, 8, 17, 12, 16286, 8, 117, 1...   \n",
       "3  [5, 63, 13807, 32, 28, 462, 8, 6016, 1344, 38,...   \n",
       "4  [5, 5996, 18, 12, 1311, 8776, 5998, 3329, 325,...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      aligned_labels  \n",
       "0  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I try to check this dimension \n",
    "\n",
    "df_dim = pd.read_csv(\"../../data/intermediate/data449_token.csv\")\n",
    "\n",
    "print(type(df_dim[\"formated_labels\"][0]))\n",
    "df_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>NER_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155097</th>\n",
       "      <td>sur</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155098</th>\n",
       "      <td>la</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155099</th>\n",
       "      <td>paie</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155100</th>\n",
       "      <td>du</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155101</th>\n",
       "      <td>mois</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155102</th>\n",
       "      <td>de</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155103</th>\n",
       "      <td>décembre</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155104</th>\n",
       "      <td>2021.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155105</th>\n",
       "      <td>la</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155106</th>\n",
       "      <td>prime</td>\n",
       "      <td>B-PPV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155107</th>\n",
       "      <td>exceptionnelle</td>\n",
       "      <td>I-PPV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155108</th>\n",
       "      <td>est</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155109</th>\n",
       "      <td>octroyée</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155110</th>\n",
       "      <td>à</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155111</th>\n",
       "      <td>tous</td>\n",
       "      <td>B-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155112</th>\n",
       "      <td>les</td>\n",
       "      <td>I-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155113</th>\n",
       "      <td>salariés</td>\n",
       "      <td>I-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155114</th>\n",
       "      <td>de</td>\n",
       "      <td>I-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155115</th>\n",
       "      <td>l'entreprise</td>\n",
       "      <td>I-TOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155116</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text NER_tags\n",
       "155097             sur        O\n",
       "155098              la        O\n",
       "155099            paie        O\n",
       "155100              du        O\n",
       "155101            mois        O\n",
       "155102              de        O\n",
       "155103        décembre        O\n",
       "155104           2021.        O\n",
       "155105              la        O\n",
       "155106           prime    B-PPV\n",
       "155107  exceptionnelle    I-PPV\n",
       "155108             est        O\n",
       "155109        octroyée        O\n",
       "155110               à        O\n",
       "155111            tous   B-TOUS\n",
       "155112             les   I-TOUS\n",
       "155113        salariés   I-TOUS\n",
       "155114              de   I-TOUS\n",
       "155115    l'entreprise   I-TOUS\n",
       "155116               .        O"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(conll_path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "lines = conll_data.strip().split('\\n')\n",
    "data = [line.split() for line in lines]\n",
    "# data\n",
    "\n",
    "# tokens = []\n",
    "# for line in data:\n",
    "#     if line != []:\n",
    "#         print(line)\n",
    "#         print(line[-1])\n",
    "\n",
    "# # Extract tokens and labels into separate lists\n",
    "\n",
    "tokens = [line[0] for line in data if line]\n",
    "labels = [line[-1] for line in data if line]\n",
    "\n",
    "# # Create a list of lists containing tokens and labels\n",
    "result = list(zip(tokens, labels))\n",
    "column_names = [\"text\", \"NER_tags\"]\n",
    "df = pd.DataFrame(result, columns=column_names)\n",
    "df.tail(20)\n",
    "\n",
    "# # df_org[\"formated_labels\"] = df_org[\"label\"].apply(lambda x: format_labels(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# training arguments\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "# metric used for evaluation\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# We want to do a metrics function to compute the accuracy of the model\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## try with conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_conll(path:str):\n",
    "    # Initialize a list to store the data\n",
    "    conll_data = []\n",
    "\n",
    "    # Open the CoNLL file in read mode\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "        # Read each line in the file\n",
    "\n",
    "    # Split the data into sentences\n",
    "    sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "    # Initialize empty lists to store text and labels\n",
    "    text_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split('\\n')\n",
    "        text = \" \".join(token.split()[0] for token in tokens)\n",
    "        labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    -DOCSTART- evolution des salaires de base : en...   \n",
      "1    l’enveloppe globale d’augmentation des rémunér...   \n",
      "2    dispositions au regard de l’implication de tou...   \n",
      "3    nous travaillons sur une politique de rémunéra...   \n",
      "4    protocole d’accord négociation annuelle obliga...   \n",
      "..                                                 ...   \n",
      "444  negociation annuelle 2022. il a été convenu et...   \n",
      "445  négociations annuelles obligatoires. ii- dispo...   \n",
      "446  accord collectif 2022 sur les salaires , la du...   \n",
      "447  damart sa etablissement. article i : augmentat...   \n",
      "448  entre l’ues kiabi , représentée par , directeu...   \n",
      "\n",
      "                                                 label  \n",
      "0    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "1    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "2    O O O O O O O O O O O O O O O O B-SYND O B-DIR...  \n",
      "3    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "4    O O O O O O O O O B-DIR O O O O B-ENT I-ENT O ...  \n",
      "..                                                 ...  \n",
      "444  O O O O O O O O O O O O O O O O O O O B-OUV O ...  \n",
      "445  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "446  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "447  B-ENT O O O O O O O O O O O O O O B-OUV O B-OU...  \n",
      "448  O O B-ENT O O O O B-DIR I-DIR I-DIR I-DIR O B-...  \n",
      "\n",
      "[449 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path to your CoNLL file\n",
    "conll_file_path = r'../../data/raw/data449.conll'  # Replace with your file path\n",
    "\n",
    "# Initialize a list to store the data\n",
    "conll_data = []\n",
    "\n",
    "# Open the CoNLL file in read mode\n",
    "with open(conll_file_path, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "    # Read each line in the file\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists to store text and labels\n",
    "text_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    text_list.append(text)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]\n",
    "\n",
    "# def turn_sentence_to_list(sentence):\n",
    "#     \"\"\"\n",
    "#     Turn a sentence into a list of words\n",
    "\n",
    "#     Args:\n",
    "#         sentence (str): sentence to tokenize\n",
    "\n",
    "#     Returns:\n",
    "#         list: list of words\n",
    "#     \"\"\"\n",
    "#     return sentence.split()\n",
    "\n",
    "\n",
    "def list_to_string(input_list, separator=' '):\n",
    "    \"\"\"\n",
    "    Convert a list of strings into a single string with elements separated by a specified separator.\n",
    "\n",
    "    :param input_list: List of strings to be converted.\n",
    "    :param separator: The separator to use between elements (default is a space).\n",
    "    :return: A single string containing the list elements.\n",
    "    \"\"\"\n",
    "    return separator.join(input_list)\n",
    "\n",
    "def change_ids(list_ids:list, new_id:dict):\n",
    "    return [new_id.get(id) for id in list_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "\n",
    "df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "O O O O O O O O O O O O O O O B-ENT I-ENT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-NCAD I-NCAD O O O O O O O O O O O O O O O O O O O O O O O O AG AG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O CAD CAD O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ENT I-ENT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(df[\"new_labels\"][5])\n",
    "print(df[\"label\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check that we haven't left non-numeric labels\n",
    "\n",
    "\n",
    "def check_labels(list_labels:list):\n",
    "    for label in list_labels:\n",
    "        if type(label)!=int:\n",
    "            print(label)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# amount = [df[\"new_labels\"].apply(lambda x: check_labels(x))]\n",
    "\n",
    "def count_error(dataframe, column:str):\n",
    "    \"\"\"\n",
    "    Count the number of errors - a cell containing a non numerical value - in a column of a dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): dataframe to check\n",
    "        column (str): column to check\n",
    "\n",
    "    Returns:\n",
    "        int: number of errors\n",
    "    \"\"\"\n",
    "    return len(df[column])- sum(dataframe[column].apply(lambda x: check_labels(x)))\n",
    "\n",
    "# print(f\"nombre d'erreurs: {count_error(df, 'new_labels')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts: \n\u001b[1;32m----> 5\u001b[0m     text \u001b[39m=\u001b[39m turn_sentence_to_list(text)\n\u001b[0;32m      6\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(text, is_split_into_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     tokens_mod \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mturn_sentence_to_list\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mturn_sentence_to_list\u001b[39m(sentence):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    Turn a sentence into a list of tokens\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m        list: list of tokens\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sentence\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "texts = df[\"text\"].tolist()\n",
    "start = 0\n",
    "\n",
    "for text in texts: \n",
    "    text = turn_sentence_to_list(text)\n",
    "    tokens = tokenizer(text, is_split_into_words=True)\n",
    "    tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "    length = len(tokens_mod)\n",
    "    if length > 512:\n",
    "        start += 1\n",
    "        print(length)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JB NER is probably not suited since we lost 182 texts, which is too much, due to context limit. We can eventually try to split the text into 512 tokens chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    [-DOCSTART- evolution des salaires de base : e...   \n",
      "1    [l’enveloppe globale d’augmentation des rémuné...   \n",
      "2    [dispositions au regard de l’implication de to...   \n",
      "3    [nous travaillons sur une politique de rémunér...   \n",
      "4    [protocole d’accord négociation annuelle oblig...   \n",
      "..                                                 ...   \n",
      "404  [negociation annuelle 2022. article 1 – mesure...   \n",
      "405  [négociations annuelles obligatoires. ii- disp...   \n",
      "406  [accord collectif 2022 sur les salaires , la d...   \n",
      "407  [damart sa etablissement. article i : augmenta...   \n",
      "408  [entre l’ues kiabi , représentée par , directe...   \n",
      "\n",
      "                                                 label  \n",
      "0    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "1    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "2    [O O O O O O O O O O O O O O O O B-SYND O B-DI...  \n",
      "3    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "4    [O O O O O O O O O B-DIR O O O O B-ENT I-ENT O...  \n",
      "..                                                 ...  \n",
      "404  [O O O O O O O O O B-OUV O O O O O O O O O O O...  \n",
      "405  [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "406  [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "407  [B-ENT O O O O O O O O O O O O O O B-OUV O B-O...  \n",
      "408  [O O B-ENT O O O O B-DIR I-DIR I-DIR I-DIR O B...  \n",
      "\n",
      "[409 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample data in CoNLL format\n",
    "conll_data = r'../../data/raw/data449.conll'\n",
    "\n",
    "with open(conll_data, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists and a variable to keep track of the current sentence length\n",
    "text_list = []\n",
    "labels_list = []\n",
    "current_length = 0\n",
    "\n",
    "# Define the maximum token length for a chunk\n",
    "max_chunk_length = 512\n",
    "\n",
    "# Initialize a dictionary to keep track of chunks\n",
    "chunked_data = defaultdict(list)\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    \n",
    "    # Check if adding this sentence will exceed the maximum chunk length\n",
    "    if current_length + len(tokens) <= max_chunk_length:\n",
    "        # Add this sentence to the current chunk\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "        current_length += len(tokens)\n",
    "    else:\n",
    "        # Start a new chunk\n",
    "        chunked_data['text'].append(text_list)\n",
    "        chunked_data['label'].append(labels_list)\n",
    "        \n",
    "        # Reset the current chunk\n",
    "        text_list = [text]\n",
    "        labels_list = [labels]\n",
    "        current_length = len(tokens)\n",
    "\n",
    "# Add the last chunk\n",
    "chunked_data['text'].append(text_list)\n",
    "chunked_data['label'].append(labels_list)\n",
    "\n",
    "# Create a dataframe from the chunked data\n",
    "df = pd.DataFrame(chunked_data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def format_text(df):\n",
    "    formated_text = []\n",
    "\n",
    "    text = df[\"text\"]\n",
    "    for i in text:\n",
    "        i = turn_sentence_to_list(i)\n",
    "        formated_text.append(i)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "    df[\"formated_text\"] = formated_text\n",
    "    return df\n",
    "\n",
    "# formated_text = []\n",
    "\n",
    "# text = df[\"text\"]\n",
    "# for i in text:\n",
    "#     i = turn_sentence_to_list(i)\n",
    "#     formated_text.append(i)\n",
    "\n",
    "# # we add it to the dataframe\n",
    "# df[\"formated_text\"] = formated_text\n",
    "# df = format_text(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we format the label to the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(df):\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    formated_labels = []\n",
    "\n",
    "    for label in labels:\n",
    "        # label = list_to_string(label)\n",
    "        label = turn_sentence_to_list(label)\n",
    "        formated_labels.append(label)\n",
    "        # print(label)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "\n",
    "    df[\"formated_labels\"] = formated_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, columns:list):\n",
    "    df = df[columns].copy()\n",
    "    return df\n",
    "\n",
    "# df = select_columns(df, columns=[\"formated_text\", \"formated_labels\"])\n",
    "\n",
    "# df = df.rename(columns={\"formated_text\": \"text\", \"formated_labels\": \"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the length of the two columns so that they are of the same dimensions\n",
    "\n",
    "start = 0\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    if len(df['text'][i]) != len(df['label'][i]):\n",
    "        start += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"les éléments dont la dimension diverge sont au nombre de\",start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-, evolution, des, salaires, de, bas...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-DIR, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [-DOCSTART-, evolution, des, salaires, de, bas...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, B-DIR, O, O, O, O,...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"text\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(df[\"text\"][0], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['<s>', '▁-', 'D', 'OC', 'ST', 'ART', '-', '▁e', 'volution', '▁des', '▁salaires', '▁de', '▁base', '▁:', '▁enveloppe', '▁budgétaire', '▁:', '▁il', '▁est', '▁convenu', '▁entre', '▁les', '▁parties', '▁que', '▁l', '’', 'enveloppe', '▁budgétaire', '▁consacrée', '▁à', '▁l', '’', 'évolution', '▁des', '▁salaires', '▁de', '▁base', '▁des', '▁collaborateurs', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁', ',', '▁et', '▁remplissant', '▁par', '▁ailleurs', '▁les', '▁autres', '▁conditions', '▁habituelle', 's', '▁d', '’', 'éligibilité', '▁à', '▁une', '▁augmentation', '▁', ',', '▁représenter', 'a', '▁5', ',', '8', '▁%', '▁de', '▁la', '▁masse', '▁salariale', '.', '▁de', '▁fait', '▁', ',', '▁les', '▁primes', '▁exprimée', 's', '▁en', '▁pourcentage', '▁du', '▁salaire', '▁de', '▁base', '▁augmenter', 'ont', '▁ainsi', '▁en', '▁même', '▁temps', '▁que', '▁le', '▁salaire', '▁des', '▁collaborateurs', '▁concernés', '.', '▁pour', '▁rappel', '▁', ',', '▁conformément', '▁à', '▁la', '▁politique', '▁salariale', '▁d', '’', 'e', 'li', '▁l', 'illy', '▁&', '▁ci', 'e', '▁', ',', '▁le', '▁supervise', 'ur', '▁prend', '▁une', '▁décision', '▁quant', '▁à', '▁l', '’', 'évolution', '▁du', '▁salaire', '▁de', '▁base', '▁de', '▁son', '▁collaborateur', '▁dans', '▁son', '▁échelle', '▁', ',', '▁en', '▁prenant', '▁en', '▁considération', '▁:', '▁sa', '▁performance', '▁sur', '▁l', '’', 'année', '▁sa', '▁performance', '▁dans', '▁la', '▁durée', '*', '▁la', '▁performance', '▁des', '▁collègues', '▁situés', '▁au', '▁même', '▁niveau', '▁de', '▁poste', '▁le', '▁rapport', '▁entre', '▁sa', '▁position', '▁dans', '▁l', '’', 'échelle', '▁de', '▁salaire', '▁et', '▁sa', '▁performance', '▁dans', '▁la', '▁durée', '▁(', '▁*', 'à', '▁partir', '▁de', '▁20', '23', '▁', ',', '▁un', '▁collaborateur', '▁ne', '▁répondant', '▁pas', '▁aux', '▁attentes', '▁(', '▁non', '▁éligible', 's', '▁)', '▁pourrait', '▁néanmoins', '▁bénéficier', '▁d', '’', 'une', '▁augmentation', '▁de', '▁salaire', '▁pouvant', '▁aller', '▁jusqu', '’', 'à', '▁50', '▁%', '▁du', '▁maximum', '▁de', '▁la', '▁fourchette', '▁pre', 'vue', '▁pour', '▁son', '▁qui', 'nt', 'ile', '▁', ',', '▁et', '▁perce', 'vra', '▁la', '▁moitié', '▁de', '▁son', '▁variable', '▁)', '▁mesure', '▁exceptionnelle', '▁:', '▁les', '▁collaborateurs', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁', ',', '▁qui', '▁remplissent', '▁les', '▁conditions', '▁d', '’', 'éligibilité', '▁à', '▁une', '▁augmentation', '▁du', '▁salaire', '▁de', '▁base', '▁', ',', '▁et', '▁qui', '▁sont', '▁présents', '▁au', '▁1', 'er', '▁avril', '▁20', '23', '▁', ',', '▁bénéficier', 'ont', '▁d', '’', 'une', '▁garantie', '▁d', '’', 'augmentation', '▁du', '▁salaire', '▁de', '▁base', '▁de', '▁1300', '€', '▁brut', 's', '▁pas', '▁an', '.', '▁les', '▁collaborateurs', '▁éligible', 's', '▁ne', '▁pouvant', '▁bénéficier', '▁du', '▁montant', '▁total', '▁de', '▁cette', '▁mesure', '▁', ',', '▁car', '▁atteignant', '▁le', '▁plafond', '▁de', '▁leur', '▁échelle', '▁', ',', '▁se', '▁verront', '▁verser', '▁le', '▁complément', '▁sous', '▁forme', '▁de', '▁prime', '.', '▁le', '▁montant', '▁de', '▁l', '’', 'augmentation', '▁sera', '▁pro', 'ra', 'té', '▁pour', '▁les', '▁collaborateurs', '▁à', '▁temps', '▁partiel', 's', '.', '▁date', '▁d', '’', 'application', '▁:', '▁les', '▁évolutions', '▁de', '▁salaire', '▁prévues', '▁au', '▁présent', '▁article', '▁seront', '▁applicables', '▁au', '▁1', 'er', '▁mars', '▁20', '23', '.', '▁article', '▁2', '▁:', '▁prime', '▁exceptionnelle', '▁pour', '▁les', '▁collaborateurs', '▁dont', '▁la', '▁position', '▁dans', '▁l', '’', 'échelle', '▁est', '▁égale', '▁ou', '▁supérieure', '▁à', '▁1', ',', '25', '▁de', '▁leur', '▁échelle', '▁:', '▁pour', '▁les', '▁collaborateurs', '▁justifiant', '▁d', '’', 'une', '▁présence', '▁effective', '▁de', '▁3', '▁mois', '▁minimum', '▁en', '▁2022', '▁', ',', '▁présents', '▁au', '▁1', 'er', '▁avril', '▁20', '23', '▁', ',', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁et', '▁dont', '▁la', '▁position', '▁dans', '▁l', '’', 'échelle', '▁est', '▁supérieure', '▁à', '▁1', ',', '25', '▁', ',', '▁une', '▁prime', '▁de', '▁1300', '▁€', '▁brut', 's', '▁sera', '▁versée', '▁en', '▁mars', '▁20', '23', '.', '▁ce', '▁montant', '▁sera', '▁pro', 'ra', 'té', '▁pour', '▁les', '▁collaborateurs', '▁à', '▁temps', '▁partiel', 's', '.', '▁article', '▁3', '▁:', '▁promotions', '▁et', '▁ajustement', 's', '▁:', '▁en', '▁20', '23', '▁', ',', '▁l', '’', 'évolution', '▁professionnelle', '▁des', '▁collaborateurs', '▁reste', '▁une', '▁priorité', '.', '▁ainsi', '▁', ',', '▁0,3', '▁%', '▁de', '▁la', '▁masse', '▁salariale', '▁sera', '▁consacrée', '▁aux', '▁promotions', '▁et', '▁ajustement', 's', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "token_input = tokenizer(df['text'][0], is_split_into_words=True)\n",
    "print(token_input)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(token_input[\"input_ids\"])\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_ids = token_input.word_ids()\n",
    "# print(word_ids)\n",
    "\n",
    "def align_labels(word_ids:list, tag_list:list):\n",
    "    aligned_labels = []\n",
    "    for i in word_ids:\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "        else:\n",
    "            aligned_labels.append(tag_list[i])\n",
    "    return aligned_labels\n",
    "\n",
    "# aligned_labels = align_labels(word_ids, tag_list=df[\"new_labels\"][0])\n",
    "# aligned_labels = create_aligned_labels(word_ids, example=df)\n",
    "\n",
    "# print(aligned_labels)\n",
    "# print(len(aligned_labels))\n",
    "# print(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_ids\"] = df[\"text\"].apply(lambda x: tokenizer(x, is_split_into_words=True).word_ids())\n",
    "df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"label\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', -100),\n",
       " ('▁-', 'O'),\n",
       " ('D', 'O'),\n",
       " ('OC', 'O'),\n",
       " ('ST', 'O'),\n",
       " ('ART', 'O'),\n",
       " ('-', 'O'),\n",
       " ('▁e', 'O'),\n",
       " ('volution', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁salaires', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁enveloppe', 'O'),\n",
       " ('▁budgétaire', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁il', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁convenu', 'O'),\n",
       " ('▁entre', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁parties', 'O'),\n",
       " ('▁que', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('enveloppe', 'O'),\n",
       " ('▁budgétaire', 'O'),\n",
       " ('▁consacrée', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁salaires', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁des', 'B-TOUS'),\n",
       " ('▁collaborateurs', 'I-TOUS'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁remplissant', 'O'),\n",
       " ('▁par', 'O'),\n",
       " ('▁ailleurs', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁autres', 'O'),\n",
       " ('▁conditions', 'O'),\n",
       " ('▁habituelle', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('éligibilité', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁représenter', 'O'),\n",
       " ('a', 'O'),\n",
       " ('▁5', 'B-ATOT'),\n",
       " (',', 'B-ATOT'),\n",
       " ('8', 'B-ATOT'),\n",
       " ('▁%', 'I-ATOT'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁masse', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁fait', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁primes', 'O'),\n",
       " ('▁exprimée', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁pourcentage', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁augmenter', 'O'),\n",
       " ('ont', 'O'),\n",
       " ('▁ainsi', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁même', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁que', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁concernés', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁rappel', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁conformément', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁politique', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('▁d', 'B-ENT'),\n",
       " ('’', 'B-ENT'),\n",
       " ('e', 'B-ENT'),\n",
       " ('li', 'B-ENT'),\n",
       " ('▁l', 'I-ENT'),\n",
       " ('illy', 'I-ENT'),\n",
       " ('▁&', 'I-ENT'),\n",
       " ('▁ci', 'I-ENT'),\n",
       " ('e', 'I-ENT'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁supervise', 'O'),\n",
       " ('ur', 'O'),\n",
       " ('▁prend', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁décision', 'O'),\n",
       " ('▁quant', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁collaborateur', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁prenant', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁considération', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁sur', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('année', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁durée', 'O'),\n",
       " ('*', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collègues', 'O'),\n",
       " ('▁situés', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁même', 'O'),\n",
       " ('▁niveau', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁rapport', 'O'),\n",
       " ('▁entre', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁durée', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁*', 'O'),\n",
       " ('à', 'O'),\n",
       " ('▁partir', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁un', 'O'),\n",
       " ('▁collaborateur', 'O'),\n",
       " ('▁ne', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁pas', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁non', 'O'),\n",
       " ('▁éligible', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁pourrait', 'O'),\n",
       " ('▁néanmoins', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁pouvant', 'O'),\n",
       " ('▁aller', 'O'),\n",
       " ('▁jusqu', 'O'),\n",
       " ('’', 'O'),\n",
       " ('à', 'O'),\n",
       " ('▁50', 'O'),\n",
       " ('▁%', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁maximum', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁fourchette', 'O'),\n",
       " ('▁pre', 'O'),\n",
       " ('vue', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('nt', 'O'),\n",
       " ('ile', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁perce', 'O'),\n",
       " ('vra', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁moitié', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁variable', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁mesure', 'O'),\n",
       " ('▁exceptionnelle', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('▁remplissent', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁conditions', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('éligibilité', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('▁sont', 'O'),\n",
       " ('▁présents', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('er', 'O'),\n",
       " ('▁avril', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('ont', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁garantie', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('augmentation', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁1300', 'O'),\n",
       " ('€', 'O'),\n",
       " ('▁brut', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁pas', 'O'),\n",
       " ('▁an', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁éligible', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁ne', 'O'),\n",
       " ('▁pouvant', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁total', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁cette', 'O'),\n",
       " ('▁mesure', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁car', 'O'),\n",
       " ('▁atteignant', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁plafond', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁leur', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁se', 'O'),\n",
       " ('▁verront', 'O'),\n",
       " ('▁verser', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁complément', 'O'),\n",
       " ('▁sous', 'O'),\n",
       " ('▁forme', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('augmentation', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁pro', 'O'),\n",
       " ('ra', 'O'),\n",
       " ('té', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁partiel', 'O'),\n",
       " ('s', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁date', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('application', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁évolutions', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁prévues', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁présent', 'O'),\n",
       " ('▁article', 'O'),\n",
       " ('▁seront', 'O'),\n",
       " ('▁applicables', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'B-DATE'),\n",
       " ('er', 'B-DATE'),\n",
       " ('▁mars', 'I-DATE'),\n",
       " ('▁20', 'I-DATE'),\n",
       " ('23', 'I-DATE'),\n",
       " ('.', 'I-DATE'),\n",
       " ('▁article', 'O'),\n",
       " ('▁2', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('▁exceptionnelle', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁dont', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁égale', 'O'),\n",
       " ('▁ou', 'O'),\n",
       " ('▁supérieure', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁1', 'O'),\n",
       " (',', 'O'),\n",
       " ('25', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁leur', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁justifiant', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁présence', 'O'),\n",
       " ('▁effective', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁3', 'O'),\n",
       " ('▁mois', 'O'),\n",
       " ('▁minimum', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁2022', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁présents', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('er', 'O'),\n",
       " ('▁avril', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁dont', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁supérieure', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁1', 'O'),\n",
       " (',', 'O'),\n",
       " ('25', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁1300', 'O'),\n",
       " ('▁€', 'O'),\n",
       " ('▁brut', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁versée', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁mars', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁ce', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁pro', 'O'),\n",
       " ('ra', 'O'),\n",
       " ('té', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁partiel', 'O'),\n",
       " ('s', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁article', 'O'),\n",
       " ('▁3', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁promotions', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁ajustement', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁professionnelle', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁reste', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁priorité', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁ainsi', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁0,3', 'O'),\n",
       " ('▁%', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁masse', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁consacrée', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁promotions', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁ajustement', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁', 'O'),\n",
       " ('.', 'O'),\n",
       " ('</s>', -100)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens, aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iids_am(df):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for _,i in df.iterrows():\n",
    "        input = i[\"ids\"][\"input_ids\"]\n",
    "        attention = i[\"ids\"][\"attention_mask\"]\n",
    "\n",
    "        input_ids.append(input)\n",
    "        attention_mask.append(attention)\n",
    "\n",
    "    df[\"input_ids\"] = input_ids\n",
    "    df[\"attention_mask\"] = attention_mask\n",
    "    return df\n",
    "\n",
    "# df = create_iids_am(df)\n",
    "# df[[\"input_ids\", \"attention_mask\"]].head()\n",
    "# df_ids = df[[\"ids\"]].copy()\n",
    "# df_ids[\"ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "\n",
    "def tokenize_and_align_data(path_conll:str, new_id:dict):\n",
    "    #charge and create the df\n",
    "    df = charge_conll(path_conll)\n",
    "    df = format_text(df)\n",
    "    df = format_labels(df)\n",
    "    df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))\n",
    "\n",
    "    #tokenize and align the text\n",
    "    df[\"ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True))\n",
    "    df = create_iids_am(df)\n",
    "    df[\"word_ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True).word_ids())\n",
    "    df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"new_labels\"]), axis=1)\n",
    "    return df\n",
    "\n",
    "def apply_tokenization(conll_path:str, new_id:dict, columns:list, new_names:dict, save_path=False, output_save=None):\n",
    "    df = tokenize_and_align_data(conll_path, new_id=new_id)\n",
    "    df = select_columns(df, columns)\n",
    "    df = df.rename(columns=new_names)\n",
    "    #we check the length of the two columns so that they are of the same dimensions\n",
    "    # print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "    if save_path:\n",
    "        df.to_csv(output_save, index=False)\n",
    "    return df\n",
    "\n",
    "# df = apply_tokenization(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id, columns=columns, new_names=new_names, save_path=True, output_save=r\"../../data/intermediate/data499_token.csv\")\n",
    "# df.head()\n",
    "# df = tokenize_and_align_data(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id)\n",
    "# df = select_columns(df, columns)\n",
    "# df = df.rename(columns=new_names)\n",
    "# #we check the length of the two columns so that they are of the same dimensions\n",
    "# print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "# # df.to_csv(r\"../../data/intermediate/data499_token.csv\", index=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "list_text = df[\"text\"].tolist()\n",
    "\n",
    "for i in list_text:\n",
    "    if len(i) > 512:\n",
    "        print(len(i))\n",
    "    else:\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the objective is to collect all the inputs_ids, attention_mask and labels in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "def final_formating(df, start:int, end:int):\n",
    "    \"\"\"\n",
    "    Collect the data from a dataframe in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "\n",
    "    Returns:\n",
    "        list: list of the data\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        input_ids.append(df[\"input_ids\"][i])\n",
    "        attention_mask.append(df[\"attention_mask\"][i])\n",
    "        labels.append(df[\"aligned_labels\"][i])\n",
    "\n",
    "    data = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "    return data\n",
    "    \n",
    "# data = final_formating(df, 0, 5)\n",
    "# print(data)\n",
    "# for _,i in df.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_treat_data(conll_path:str, new_id:dict, columns:list, new_names:dict, start:int, end:int, csv=False, output_save=None):\n",
    "    \"\"\"\n",
    "    This function compile all the previous one and treat automatically all data provided as conll format.\\n\n",
    "    The objective is to provide a dictionnary containing the data for a given range of sentences with only one function. \\n\n",
    "    The same results can be obtained by parts with apply_tokenization and final_formating functions. This one is more a convenience function.\\n\n",
    "    It is also possible to obtain a csv file if csv is set to True and a path is provided in output_save.\n",
    "\n",
    "    Args:\n",
    "        conll_path (str): path to the conll file\n",
    "        new_id (dict): dictionnary to change the labels\n",
    "        columns (list): columns to keep\n",
    "        new_names (dict): new names of the columns\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionnary containing the data\n",
    "    \"\"\"\n",
    "    df = apply_tokenization(conll_path, new_id=new_id, columns=columns, new_names=new_names)\n",
    "\n",
    "    if csv:\n",
    "        df.to_csv(output_save, index=False)\n",
    "\n",
    "    data = final_formating(df, start, end)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"formated_labels\",\"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "conll_path = r\"..\\..\\data\\raw\\data449.conll\"\n",
    "start = 0\n",
    "end = 449\n",
    "\n",
    "data = download_and_treat_data(conll_path=conll_path, new_id=new_id, columns=columns, new_names=new_names, start=start, end=end, csv=True, output_save=r\"../../data/intermediate/data449_token.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we divide the data into train, test, and validation sets\n",
    "\n",
    "def split_data(data_dict, train_size=0.8, test_size=0.1, val_size=0.1, random_seed=None):\n",
    "    # Set a random seed for reproducibility\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Combine input_ids, attention_mask, and aligned_labels into a single list\n",
    "    combined_data = list(zip(data_dict['input_ids'], data_dict['attention_mask'], data_dict['labels']))\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    total_size = len(combined_data)\n",
    "    train_size = int(train_size * total_size)\n",
    "    test_size = int(test_size * total_size)\n",
    "    val_size = int(val_size * total_size)\n",
    "\n",
    "    # Split the data into train, test, and val sets\n",
    "    train_data = combined_data[:train_size]\n",
    "    test_data = combined_data[train_size:train_size + test_size]\n",
    "    val_data = combined_data[train_size + test_size:train_size + test_size + val_size]\n",
    "\n",
    "    # Unzip the data to restore the original structure\n",
    "    train_input_ids, train_attention_mask, train_aligned_labels = zip(*train_data)\n",
    "    test_input_ids, test_attention_mask, test_aligned_labels = zip(*test_data)\n",
    "    val_input_ids, val_attention_mask, val_aligned_labels = zip(*val_data)\n",
    "\n",
    "    # Create dictionaries for the train, test, and val sets\n",
    "    train_set = {\n",
    "        'input_ids': list(train_input_ids),\n",
    "        'attention_mask': list(train_attention_mask),\n",
    "        'labels': list(train_aligned_labels)\n",
    "    }\n",
    "    test_set = {\n",
    "        'input_ids': list(test_input_ids),\n",
    "        'attention_mask': list(test_attention_mask),\n",
    "        'labels': list(test_aligned_labels)\n",
    "    }\n",
    "    val_set = {\n",
    "        'input_ids': list(val_input_ids),\n",
    "        'attention_mask': list(val_attention_mask),\n",
    "        'labels': list(val_aligned_labels)\n",
    "    }\n",
    "\n",
    "    return train_set, test_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, val_data = split_data(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get rid of the list with a dimension superior to 512 as the model doesn't hold more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn into tensors our data so that BERT can read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "val_dataset = Dataset.from_dict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'SYND', 'DIR', 'DATE', 'ENT', 'CAD', 'INT', 'OUV', 'NCAD', 'NOUV', 'TOUS', 'AG CAD', 'AG INT', 'AG OUV', 'AG NCAD', 'AG NOUV', 'AI CAD', 'AI INT', 'AI OUV', 'AI NCAD', 'AI NOUV', 'AG', 'AI', 'ATOT', 'ATOT CAD', 'ATOT INT', 'ATOT OUV', 'ATOT NCAD', 'ATOT NOUV', 'PPV', 'PPVm']\n"
     ]
    }
   ],
   "source": [
    "# list of labels\n",
    "reverse_id = {v: k for k, v in new_id.items()}\n",
    "second_elements = [value for value in reverse_id.values()]\n",
    "label_list = second_elements\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at Jean-Baptiste/camembert-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([31]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([31, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"Jean-Baptiste/camembert-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\", num_labels=len(label_list), ignore_mismatched_sizes=True) #this last argument might be a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11265,  3995,  ...,     0,     0,     0],\n",
       "        [  101,  2033, 28632,  ...,     0,     0,     0],\n",
       "        [  101, 11265,  3995,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 15802,  9530,  ...,     0,     0,     0],\n",
       "        [  101,  8292,  4674,  ...,     0,     0,     0],\n",
       "        [  101, 15802,  8145,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,    45,  1172,  ...,    14,     8,     6],\n",
       "        [    5,  1909,    18,  ...,     1,     1,     1],\n",
       "        [    5,  1833,    18,  ...,   359,    38,     6],\n",
       "        ...,\n",
       "        [    5,    36,   788,  ...,     1,     1,     1],\n",
       "        [    5,  1909,  2844,  ...,    12, 10616,     6],\n",
       "        [    5,    13,  8776,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,  ...,    0,    0, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ...,    0,    0, -100],\n",
       "        ...,\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ...,    0,    0, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(3.3504, grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.0529, -0.1031,  0.1047,  ...,  0.1475, -0.0377, -0.0375],\n",
       "         [ 0.1612, -0.1735, -0.0050,  ...,  0.1062, -0.0600,  0.0174],\n",
       "         [ 0.1106, -0.1662, -0.0390,  ...,  0.0700, -0.0178, -0.0115],\n",
       "         ...,\n",
       "         [ 0.0520, -0.1052,  0.1048,  ...,  0.1475, -0.0432, -0.0359],\n",
       "         [ 0.0520, -0.1052,  0.1048,  ...,  0.1475, -0.0432, -0.0359],\n",
       "         [ 0.0520, -0.1052,  0.1048,  ...,  0.1475, -0.0432, -0.0359]],\n",
       "\n",
       "        [[ 0.0498, -0.1091,  0.0947,  ...,  0.1516, -0.0395, -0.0410],\n",
       "         [ 0.1987, -0.2244,  0.0456,  ...,  0.0494, -0.1514,  0.0197],\n",
       "         [ 0.2241, -0.1993, -0.0188,  ...,  0.1209, -0.0761,  0.0410],\n",
       "         ...,\n",
       "         [ 0.1219, -0.2625,  0.0173,  ...,  0.0550, -0.2250,  0.0222],\n",
       "         [ 0.0968, -0.2706, -0.0236,  ...,  0.0212, -0.1703,  0.0339],\n",
       "         [ 0.0487, -0.1112,  0.0943,  ...,  0.1519, -0.0444, -0.0391]],\n",
       "\n",
       "        [[ 0.0504, -0.1170,  0.0762,  ...,  0.1504, -0.0285, -0.0391],\n",
       "         [ 0.1403, -0.0980,  0.0648,  ..., -0.0610, -0.1406,  0.1266],\n",
       "         [ 0.0076, -0.1391, -0.0398,  ..., -0.0016, -0.1382,  0.0796],\n",
       "         ...,\n",
       "         [ 0.0493, -0.1196,  0.0767,  ...,  0.1510, -0.0344, -0.0381],\n",
       "         [ 0.0493, -0.1196,  0.0767,  ...,  0.1510, -0.0344, -0.0381],\n",
       "         [ 0.0493, -0.1196,  0.0767,  ...,  0.1510, -0.0344, -0.0381]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0445, -0.1401,  0.0961,  ...,  0.1346, -0.0732, -0.0577],\n",
       "         [ 0.2484, -0.1019,  0.0623,  ...,  0.0516, -0.1619,  0.0519],\n",
       "         [ 0.1966, -0.0947, -0.0566,  ...,  0.0961, -0.0706,  0.0795],\n",
       "         ...,\n",
       "         [ 0.1432, -0.2774,  0.0468,  ...,  0.0308, -0.1710, -0.1144],\n",
       "         [ 0.2230, -0.2884,  0.1456,  ...,  0.0099, -0.1732, -0.0992],\n",
       "         [ 0.0448, -0.1423,  0.0965,  ...,  0.1357, -0.0775, -0.0565]],\n",
       "\n",
       "        [[ 0.0333, -0.1292,  0.1118,  ...,  0.1367, -0.0453, -0.0598],\n",
       "         [ 0.2102, -0.2396,  0.0389,  ..., -0.0537, -0.1653,  0.0139],\n",
       "         [ 0.1319, -0.3303, -0.0007,  ..., -0.0528, -0.1814,  0.0318],\n",
       "         ...,\n",
       "         [ 0.0340, -0.1323,  0.1135,  ...,  0.1369, -0.0523, -0.0587],\n",
       "         [ 0.0340, -0.1323,  0.1135,  ...,  0.1369, -0.0523, -0.0587],\n",
       "         [ 0.0340, -0.1323,  0.1135,  ...,  0.1369, -0.0523, -0.0587]],\n",
       "\n",
       "        [[ 0.0407, -0.1213,  0.0954,  ...,  0.1414, -0.0408, -0.0405],\n",
       "         [ 0.1530, -0.1762,  0.2299,  ..., -0.0142, -0.2202,  0.0161],\n",
       "         [ 0.1266, -0.1948, -0.0085,  ...,  0.0059, -0.1186,  0.0116],\n",
       "         ...,\n",
       "         [ 0.0396, -0.1241,  0.0960,  ...,  0.1413, -0.0462, -0.0386],\n",
       "         [ 0.0396, -0.1241,  0.0960,  ...,  0.1413, -0.0462, -0.0386],\n",
       "         [ 0.0396, -0.1241,  0.0960,  ...,  0.1413, -0.0462, -0.0386]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**(next(iter(train_dataloader))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garsonj\\AppData\\Local\\Temp\\ipykernel_9988\\1996466334.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>formated_labels</th>\n",
       "      <th>label</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>aligned_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['-DOCSTART-', 'evolution', 'des', 'salaires',...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "      <td>[5, 67, 342, 5363, 7486, 11485, 26, 599, 14573...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['l’enveloppe', 'globale', 'd’augmentation', '...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 5, 5, 6, ...</td>\n",
       "      <td>[5, 17, 12, 16886, 4141, 18, 12, 5314, 20, 601...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['dispositions', 'au', 'regard', 'de', 'l’impl...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[5, 2756, 36, 897, 8, 17, 12, 16286, 8, 117, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nous', 'travaillons', 'sur', 'une', 'politiq...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[5, 63, 13807, 32, 28, 462, 8, 6016, 1344, 38,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['protocole', 'd’accord', 'négociation', 'annu...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, ...</td>\n",
       "      <td>[5, 5996, 18, 12, 1311, 8776, 5998, 3329, 325,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['-DOCSTART-', 'evolution', 'des', 'salaires',...   \n",
       "1  ['l’enveloppe', 'globale', 'd’augmentation', '...   \n",
       "2  ['dispositions', 'au', 'regard', 'de', 'l’impl...   \n",
       "3  ['nous', 'travaillons', 'sur', 'une', 'politiq...   \n",
       "4  ['protocole', 'd’accord', 'négociation', 'annu...   \n",
       "\n",
       "                                     formated_labels  \\\n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "\n",
       "                                               label  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...   \n",
       "\n",
       "                                            word_ids  \\\n",
       "0  [None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, ...   \n",
       "1  [None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 5, 5, 6, ...   \n",
       "2  [None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "3  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "4  [None, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573...   \n",
       "1  [5, 17, 12, 16886, 4141, 18, 12, 5314, 20, 601...   \n",
       "2  [5, 2756, 36, 897, 8, 17, 12, 16286, 8, 117, 1...   \n",
       "3  [5, 63, 13807, 32, 28, 462, 8, 6016, 1344, 38,...   \n",
       "4  [5, 5996, 18, 12, 1311, 8776, 5998, 3329, 325,...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      aligned_labels  \n",
       "0  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_token = pd.read_csv(r\"../../data/intermediate/data449_token.csv\")\n",
    "data_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [['O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O...\n",
      "1      [['O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O...\n",
      "2      [['O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O...\n",
      "3      [['O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O...\n",
      "4      [['O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O...\n",
      "                             ...                        \n",
      "444    [['O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O...\n",
      "445    [['O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O...\n",
      "446    [['O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O...\n",
      "447    [['B-ENT',, 'O',, 'O',, 'O',, 'O',, 'O',, 'O',...\n",
      "448    [['O',, 'O',, 'B-ENT',, 'O',, 'O',, 'O',, 'O',...\n",
      "Name: formated_labels, Length: 449, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m list_tags \u001b[39m=\u001b[39m data_token[\u001b[39m\"\u001b[39m\u001b[39mformated_labels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: string_to_list(x))\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(list_tags)\n\u001b[1;32m---> 10\u001b[0m labels \u001b[39m=\u001b[39m [label_list[i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m list_tags]\n\u001b[0;32m     11\u001b[0m \u001b[39m# metric.compute(predictions=[labels], references=[labels])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m list_tags \u001b[39m=\u001b[39m data_token[\u001b[39m\"\u001b[39m\u001b[39mformated_labels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: string_to_list(x))\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(list_tags)\n\u001b[1;32m---> 10\u001b[0m labels \u001b[39m=\u001b[39m [label_list[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m list_tags]\n\u001b[0;32m     11\u001b[0m \u001b[39m# metric.compute(predictions=[labels], references=[labels])\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "def string_to_list(label_string):\n",
    "    # Split the string by spaces and remove any leading/trailing spaces\n",
    "    labels_list = label_string.strip().split()\n",
    "    return labels_list\n",
    "\n",
    "list_tags = data_token[\"formated_labels\"].apply(lambda x: string_to_list(x))\n",
    "\n",
    "print(list_tags)\n",
    "\n",
    "labels = [label_list[i] for i in list_tags]\n",
    "# metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to do a metrics function to compute the accuracy of the model\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70402d9bf4d94b53a4e1ce739c027c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a968bb421ff541e9add8d82eb6585923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ENT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: AI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: OUV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: INT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: CAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: TOUS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: NCAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: AG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: DIR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PPV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: SYND seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ATOT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PPVm seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5152171850204468, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.914308354086484, 'eval_runtime': 73.4582, 'eval_samples_per_second': 0.599, 'eval_steps_per_second': 0.082, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drafty draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "df_token = df[['input_ids']].copy()\n",
    "print(df_token[\"input_ids\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOUS', 'I-TOUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ATOT', 'B-ATOT', 'B-ATOT', 'I-ATOT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENT', 'B-ENT', 'B-ENT', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', -100]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 23, 23, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"aligned_labels\"][0])\n",
    "df_test = df[\"aligned_labels\"].apply(lambda x :change_ids(x, new_id=new_id))\n",
    "print(df_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOUS', 'I-TOUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ATOT', 'I-ATOT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "527\n",
      "527\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "aligned_labels = [-100 if i is None else example[f\"label\"][i] for i in word_ids]\n",
    "print(example[\"label\"])\n",
    "print(len(word_ids))\n",
    "print(len(aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(tokens_mod, aligned_labels))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_mod' is not defined"
     ]
    }
   ],
   "source": [
    "list(zip(tokens_mod, aligned_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset for training from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you like it you can download it\n",
    "data = import_label_studio_data(\"../../data/raw/data449.json\")\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[evolution, des, salaires, de, base, :, envelo...</td>\n",
       "      <td>{'entities': [(322, 326, 'ATOT'), (161, 179, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>{'entities': [(229, 237, 'OUV'), (239, 247, 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>{'entities': [(101, 105, 'SYND'), (110, 122, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>{'entities': [(165, 172, 'SYND'), (364, 371, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>{'entities': [(73, 82, 'DIR'), (108, 119, 'ENT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [evolution, des, salaires, de, base, :, envelo...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \n",
       "0  {'entities': [(322, 326, 'ATOT'), (161, 179, '...  \n",
       "1  {'entities': [(229, 237, 'OUV'), (239, 247, 'O...  \n",
       "2  {'entities': [(101, 105, 'SYND'), (110, 122, '...  \n",
       "3  {'entities': [(165, 172, 'SYND'), (364, 371, '...  \n",
       "4  {'entities': [(73, 82, 'DIR'), (108, 119, 'ENT...  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df[\"text\"])):\n",
    "    df[\"text\"][i] = turn_sentence_to_list(df[\"text\"][i])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    element = df[\"text\"][i]\n",
    "    length.append(len(element))\n",
    "    # print(len(df[\"text\"][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "plt.hist(length, bins=20, color = \"lightgreen\", edgecolor='black') \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Longeur (élément)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Histogram de la longueurs des phrases en élément')\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text': [\"evolution des salaires de base : enveloppe\"],\n",
    "    'label': [{'entities': [(22, 31, 'ATOT')]}]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to tokenize text while preserving whitespace\n",
    "def tokenize_text(row):\n",
    "    text = row['text']\n",
    "    tokens = []\n",
    "    start = 0\n",
    "\n",
    "    for start, end, entity in row['label']['entities']:\n",
    "        # Add non-entity text\n",
    "        tokens.extend(text[start:end].split())\n",
    "        start = end\n",
    "\n",
    "    # Add any remaining text after the last entity\n",
    "    tokens.extend(text[start:].split())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df['tokenized_text'] = df.apply(tokenize_text, axis=1)\n",
    "\n",
    "# Display the DataFrame with tokenized text while preserving whitespace\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df[\"label\"])):\n",
    "    print(df[\"label\"][i][\"entities\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'][0]\n",
    "text = turn_sentence_to_list(text)\n",
    "# print(type(text))\n",
    "tokens = tokenizer(text, is_split_into_words=True)\n",
    "# print(tokens)\n",
    "\n",
    "tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "print(tokens_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokens.word_ids()\n",
    "# aligned_labels = [-100 if i is None else text[\"label\"][i] for i in word_ids]\n",
    "for i in word_ids:\n",
    "    # print(i)\n",
    "    if i is None:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(text[\"label\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

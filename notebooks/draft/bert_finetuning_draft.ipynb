{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning of BERT for NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the steps for the finetuning of French version of BERT, the objective is to extract specific information from french public business agreements. For this we have a dual approach, first we classify information and then only we use NER property of our model to extract wage variations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: This notebook is inspired from a masterclass and notebook provided by [Thomas Boehler](https://fr.linkedin.com/in/thomas-boehler-ba34a744) and also the work of my colleagues [Mouad Bernoussi](https://ma.linkedin.com/in/mouad-bernoussi-00aa91242) (see ./notebooks/external) and [Conrad Thiounn](https://github.com/cthiounn)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Play with PyTorch\n",
    "2. Play with the model \n",
    "3. Adapt data to BERT format (see Thomas Boehler)\n",
    "4. Create training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../../src/\")\n",
    "from spacy_utils import import_label_studio_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try with conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_conll(path:str):\n",
    "    # Initialize a list to store the data\n",
    "    conll_data = []\n",
    "\n",
    "    # Open the CoNLL file in read mode\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "        # Read each line in the file\n",
    "\n",
    "    # Split the data into sentences\n",
    "    sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "    # Initialize empty lists to store text and labels\n",
    "    text_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split('\\n')\n",
    "        text = \" \".join(token.split()[0] for token in tokens)\n",
    "        labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    -DOCSTART- evolution des salaires de base : en...   \n",
      "1    l’enveloppe globale d’augmentation des rémunér...   \n",
      "2    dispositions au regard de l’implication de tou...   \n",
      "3    nous travaillons sur une politique de rémunéra...   \n",
      "4    protocole d’accord négociation annuelle obliga...   \n",
      "..                                                 ...   \n",
      "444  negociation annuelle 2022. il a été convenu et...   \n",
      "445  négociations annuelles obligatoires. ii- dispo...   \n",
      "446  accord collectif 2022 sur les salaires , la du...   \n",
      "447  damart sa etablissement. article i : augmentat...   \n",
      "448  entre l’ues kiabi , représentée par , directeu...   \n",
      "\n",
      "                                                 label  \n",
      "0    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "1    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "2    O O O O O O O O O O O O O O O O B-SYND O B-DIR...  \n",
      "3    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "4    O O O O O O O O O B-DIR O O O O B-ENT I-ENT O ...  \n",
      "..                                                 ...  \n",
      "444  O O O O O O O O O O O O O O O O O O O B-OUV O ...  \n",
      "445  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "446  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "447  B-ENT O O O O O O O O O O O O O O B-OUV O B-OU...  \n",
      "448  O O B-ENT O O O O B-DIR I-DIR I-DIR I-DIR O B-...  \n",
      "\n",
      "[449 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path to your CoNLL file\n",
    "conll_file_path = r'../../data/raw/data449.conll'  # Replace with your file path\n",
    "\n",
    "# Initialize a list to store the data\n",
    "conll_data = []\n",
    "\n",
    "# Open the CoNLL file in read mode\n",
    "with open(conll_file_path, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "    # Read each line in the file\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists to store text and labels\n",
    "text_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    text_list.append(text)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]\n",
    "\n",
    "# def turn_sentence_to_list(sentence):\n",
    "#     \"\"\"\n",
    "#     Turn a sentence into a list of words\n",
    "\n",
    "#     Args:\n",
    "#         sentence (str): sentence to tokenize\n",
    "\n",
    "#     Returns:\n",
    "#         list: list of words\n",
    "#     \"\"\"\n",
    "#     return sentence.split()\n",
    "\n",
    "\n",
    "def list_to_string(input_list, separator=' '):\n",
    "    \"\"\"\n",
    "    Convert a list of strings into a single string with elements separated by a specified separator.\n",
    "\n",
    "    :param input_list: List of strings to be converted.\n",
    "    :param separator: The separator to use between elements (default is a space).\n",
    "    :return: A single string containing the list elements.\n",
    "    \"\"\"\n",
    "    return separator.join(input_list)\n",
    "\n",
    "def change_ids(list_ids:list, new_id:dict):\n",
    "    return [new_id.get(id) for id in list_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "\n",
    "df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "O O O O O O O O O O O O O O O B-ENT I-ENT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-NCAD I-NCAD O O O O O O O O O O O O O O O O O O O O O O O O AG AG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O CAD CAD O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ENT I-ENT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(df[\"new_labels\"][5])\n",
    "print(df[\"label\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'erreurs: 0\n"
     ]
    }
   ],
   "source": [
    "# we check that we haven't left non-numeric labels\n",
    "\n",
    "\n",
    "def check_labels(list_labels:list):\n",
    "    for label in list_labels:\n",
    "        if type(label)!=int:\n",
    "            print(label)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# amount = [df[\"new_labels\"].apply(lambda x: check_labels(x))]\n",
    "\n",
    "def count_error(dataframe, column:str):\n",
    "    \"\"\"\n",
    "    Count the number of errors - a cell containing a non numerical value - in a column of a dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): dataframe to check\n",
    "        column (str): column to check\n",
    "\n",
    "    Returns:\n",
    "        int: number of errors\n",
    "    \"\"\"\n",
    "    return len(df[column])- sum(dataframe[column].apply(lambda x: check_labels(x)))\n",
    "\n",
    "print(f\"nombre d'erreurs: {count_error(df, 'new_labels')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts: \n\u001b[1;32m----> 5\u001b[0m     text \u001b[39m=\u001b[39m turn_sentence_to_list(text)\n\u001b[0;32m      6\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(text, is_split_into_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     tokens_mod \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mturn_sentence_to_list\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mturn_sentence_to_list\u001b[39m(sentence):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    Turn a sentence into a list of tokens\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m        list: list of tokens\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sentence\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "texts = df[\"text\"].tolist()\n",
    "start = 0\n",
    "\n",
    "for text in texts: \n",
    "    text = turn_sentence_to_list(text)\n",
    "    tokens = tokenizer(text, is_split_into_words=True)\n",
    "    tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "    length = len(tokens_mod)\n",
    "    if length > 512:\n",
    "        start += 1\n",
    "        print(length)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JB NER is probably not suited since we lost 182 texts, which is too much, due to context limit. We can eventually try to split the text into 512 tokens chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    [-DOCSTART- evolution des salaires de base : e...   \n",
      "1    [l’enveloppe globale d’augmentation des rémuné...   \n",
      "2    [dispositions au regard de l’implication de to...   \n",
      "3    [nous travaillons sur une politique de rémunér...   \n",
      "4    [protocole d’accord négociation annuelle oblig...   \n",
      "..                                                 ...   \n",
      "404  [negociation annuelle 2022. article 1 – mesure...   \n",
      "405  [négociations annuelles obligatoires. ii- disp...   \n",
      "406  [accord collectif 2022 sur les salaires , la d...   \n",
      "407  [damart sa etablissement. article i : augmenta...   \n",
      "408  [entre l’ues kiabi , représentée par , directe...   \n",
      "\n",
      "                                                 label  \n",
      "0    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "1    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "2    [O O O O O O O O O O O O O O O O B-SYND O B-DI...  \n",
      "3    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "4    [O O O O O O O O O B-DIR O O O O B-ENT I-ENT O...  \n",
      "..                                                 ...  \n",
      "404  [O O O O O O O O O B-OUV O O O O O O O O O O O...  \n",
      "405  [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "406  [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "407  [B-ENT O O O O O O O O O O O O O O B-OUV O B-O...  \n",
      "408  [O O B-ENT O O O O B-DIR I-DIR I-DIR I-DIR O B...  \n",
      "\n",
      "[409 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample data in CoNLL format\n",
    "conll_data = r'../../data/raw/data449.conll'\n",
    "\n",
    "with open(conll_data, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists and a variable to keep track of the current sentence length\n",
    "text_list = []\n",
    "labels_list = []\n",
    "current_length = 0\n",
    "\n",
    "# Define the maximum token length for a chunk\n",
    "max_chunk_length = 512\n",
    "\n",
    "# Initialize a dictionary to keep track of chunks\n",
    "chunked_data = defaultdict(list)\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    \n",
    "    # Check if adding this sentence will exceed the maximum chunk length\n",
    "    if current_length + len(tokens) <= max_chunk_length:\n",
    "        # Add this sentence to the current chunk\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "        current_length += len(tokens)\n",
    "    else:\n",
    "        # Start a new chunk\n",
    "        chunked_data['text'].append(text_list)\n",
    "        chunked_data['label'].append(labels_list)\n",
    "        \n",
    "        # Reset the current chunk\n",
    "        text_list = [text]\n",
    "        labels_list = [labels]\n",
    "        current_length = len(tokens)\n",
    "\n",
    "# Add the last chunk\n",
    "chunked_data['text'].append(text_list)\n",
    "chunked_data['label'].append(labels_list)\n",
    "\n",
    "# Create a dataframe from the chunked data\n",
    "df = pd.DataFrame(chunked_data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def format_text(df):\n",
    "    formated_text = []\n",
    "\n",
    "    text = df[\"text\"]\n",
    "    for i in text:\n",
    "        i = turn_sentence_to_list(i)\n",
    "        formated_text.append(i)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "    df[\"formated_text\"] = formated_text\n",
    "    return df\n",
    "\n",
    "formated_text = []\n",
    "\n",
    "# text = df[\"text\"]\n",
    "# for i in text:\n",
    "#     i = turn_sentence_to_list(i)\n",
    "#     formated_text.append(i)\n",
    "\n",
    "# # we add it to the dataframe\n",
    "# df[\"formated_text\"] = formated_text\n",
    "# df = format_text(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we format the label to the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(df):\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    formated_labels = []\n",
    "\n",
    "    for label in labels:\n",
    "        # label = list_to_string(label)\n",
    "        label = turn_sentence_to_list(label)\n",
    "        formated_labels.append(label)\n",
    "        # print(label)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "\n",
    "    df[\"formated_labels\"] = formated_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, columns:list):\n",
    "    df = df[columns].copy()\n",
    "    return df\n",
    "\n",
    "df = select_columns(df, columns=[\"formated_text\", \"formated_labels\"])\n",
    "\n",
    "# df = df.rename(columns={\"formated_text\": \"text\", \"formated_labels\": \"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les éléments dont la dimension diverge sont au nombre de 0\n"
     ]
    }
   ],
   "source": [
    "# We check the length of the two columns so that they are of the same dimensions\n",
    "\n",
    "start = 0\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    if len(df['text'][i]) != len(df['label'][i]):\n",
    "        start += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"les éléments dont la dimension diverge sont au nombre de\",start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-, evolution, des, salaires, de, bas...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-DIR, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [-DOCSTART-, evolution, des, salaires, de, bas...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, B-DIR, O, O, O, O,...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"text\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(df[\"text\"][0], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['<s>', '▁-', 'D', 'OC', 'ST', 'ART', '-', '▁e', 'volution', '▁des', '▁salaires', '▁de', '▁base', '▁:', '▁enveloppe', '▁budgétaire', '▁:', '▁il', '▁est', '▁convenu', '▁entre', '▁les', '▁parties', '▁que', '▁l', '’', 'enveloppe', '▁budgétaire', '▁consacrée', '▁à', '▁l', '’', 'évolution', '▁des', '▁salaires', '▁de', '▁base', '▁des', '▁collaborateurs', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁', ',', '▁et', '▁remplissant', '▁par', '▁ailleurs', '▁les', '▁autres', '▁conditions', '▁habituelle', 's', '▁d', '’', 'éligibilité', '▁à', '▁une', '▁augmentation', '▁', ',', '▁représenter', 'a', '▁5', ',', '8', '▁%', '▁de', '▁la', '▁masse', '▁salariale', '.', '▁de', '▁fait', '▁', ',', '▁les', '▁primes', '▁exprimée', 's', '▁en', '▁pourcentage', '▁du', '▁salaire', '▁de', '▁base', '▁augmenter', 'ont', '▁ainsi', '▁en', '▁même', '▁temps', '▁que', '▁le', '▁salaire', '▁des', '▁collaborateurs', '▁concernés', '.', '▁pour', '▁rappel', '▁', ',', '▁conformément', '▁à', '▁la', '▁politique', '▁salariale', '▁d', '’', 'e', 'li', '▁l', 'illy', '▁&', '▁ci', 'e', '▁', ',', '▁le', '▁supervise', 'ur', '▁prend', '▁une', '▁décision', '▁quant', '▁à', '▁l', '’', 'évolution', '▁du', '▁salaire', '▁de', '▁base', '▁de', '▁son', '▁collaborateur', '▁dans', '▁son', '▁échelle', '▁', ',', '▁en', '▁prenant', '▁en', '▁considération', '▁:', '▁sa', '▁performance', '▁sur', '▁l', '’', 'année', '▁sa', '▁performance', '▁dans', '▁la', '▁durée', '*', '▁la', '▁performance', '▁des', '▁collègues', '▁situés', '▁au', '▁même', '▁niveau', '▁de', '▁poste', '▁le', '▁rapport', '▁entre', '▁sa', '▁position', '▁dans', '▁l', '’', 'échelle', '▁de', '▁salaire', '▁et', '▁sa', '▁performance', '▁dans', '▁la', '▁durée', '▁(', '▁*', 'à', '▁partir', '▁de', '▁20', '23', '▁', ',', '▁un', '▁collaborateur', '▁ne', '▁répondant', '▁pas', '▁aux', '▁attentes', '▁(', '▁non', '▁éligible', 's', '▁)', '▁pourrait', '▁néanmoins', '▁bénéficier', '▁d', '’', 'une', '▁augmentation', '▁de', '▁salaire', '▁pouvant', '▁aller', '▁jusqu', '’', 'à', '▁50', '▁%', '▁du', '▁maximum', '▁de', '▁la', '▁fourchette', '▁pre', 'vue', '▁pour', '▁son', '▁qui', 'nt', 'ile', '▁', ',', '▁et', '▁perce', 'vra', '▁la', '▁moitié', '▁de', '▁son', '▁variable', '▁)', '▁mesure', '▁exceptionnelle', '▁:', '▁les', '▁collaborateurs', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁', ',', '▁qui', '▁remplissent', '▁les', '▁conditions', '▁d', '’', 'éligibilité', '▁à', '▁une', '▁augmentation', '▁du', '▁salaire', '▁de', '▁base', '▁', ',', '▁et', '▁qui', '▁sont', '▁présents', '▁au', '▁1', 'er', '▁avril', '▁20', '23', '▁', ',', '▁bénéficier', 'ont', '▁d', '’', 'une', '▁garantie', '▁d', '’', 'augmentation', '▁du', '▁salaire', '▁de', '▁base', '▁de', '▁1300', '€', '▁brut', 's', '▁pas', '▁an', '.', '▁les', '▁collaborateurs', '▁éligible', 's', '▁ne', '▁pouvant', '▁bénéficier', '▁du', '▁montant', '▁total', '▁de', '▁cette', '▁mesure', '▁', ',', '▁car', '▁atteignant', '▁le', '▁plafond', '▁de', '▁leur', '▁échelle', '▁', ',', '▁se', '▁verront', '▁verser', '▁le', '▁complément', '▁sous', '▁forme', '▁de', '▁prime', '.', '▁le', '▁montant', '▁de', '▁l', '’', 'augmentation', '▁sera', '▁pro', 'ra', 'té', '▁pour', '▁les', '▁collaborateurs', '▁à', '▁temps', '▁partiel', 's', '.', '▁date', '▁d', '’', 'application', '▁:', '▁les', '▁évolutions', '▁de', '▁salaire', '▁prévues', '▁au', '▁présent', '▁article', '▁seront', '▁applicables', '▁au', '▁1', 'er', '▁mars', '▁20', '23', '.', '▁article', '▁2', '▁:', '▁prime', '▁exceptionnelle', '▁pour', '▁les', '▁collaborateurs', '▁dont', '▁la', '▁position', '▁dans', '▁l', '’', 'échelle', '▁est', '▁égale', '▁ou', '▁supérieure', '▁à', '▁1', ',', '25', '▁de', '▁leur', '▁échelle', '▁:', '▁pour', '▁les', '▁collaborateurs', '▁justifiant', '▁d', '’', 'une', '▁présence', '▁effective', '▁de', '▁3', '▁mois', '▁minimum', '▁en', '▁2022', '▁', ',', '▁présents', '▁au', '▁1', 'er', '▁avril', '▁20', '23', '▁', ',', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁et', '▁dont', '▁la', '▁position', '▁dans', '▁l', '’', 'échelle', '▁est', '▁supérieure', '▁à', '▁1', ',', '25', '▁', ',', '▁une', '▁prime', '▁de', '▁1300', '▁€', '▁brut', 's', '▁sera', '▁versée', '▁en', '▁mars', '▁20', '23', '.', '▁ce', '▁montant', '▁sera', '▁pro', 'ra', 'té', '▁pour', '▁les', '▁collaborateurs', '▁à', '▁temps', '▁partiel', 's', '.', '▁article', '▁3', '▁:', '▁promotions', '▁et', '▁ajustement', 's', '▁:', '▁en', '▁20', '23', '▁', ',', '▁l', '’', 'évolution', '▁professionnelle', '▁des', '▁collaborateurs', '▁reste', '▁une', '▁priorité', '.', '▁ainsi', '▁', ',', '▁0,3', '▁%', '▁de', '▁la', '▁masse', '▁salariale', '▁sera', '▁consacrée', '▁aux', '▁promotions', '▁et', '▁ajustement', 's', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "token_input = tokenizer(df['text'][0], is_split_into_words=True)\n",
    "print(token_input)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 17, 17, 18, 19, 20, 21, 21, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 37, 38, 39, 40, 41, 41, 42, 42, 42, 43, 44, 45, 46, 46, 47, 47, 48, 48, 48, 49, 50, 51, 52, 53, 53, 54, 55, 56, 56, 57, 58, 59, 59, 60, 61, 62, 63, 64, 65, 66, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 76, 77, 78, 79, 79, 80, 81, 82, 83, 84, 85, 85, 85, 85, 86, 86, 87, 88, 88, 89, 89, 90, 91, 91, 92, 93, 94, 95, 96, 97, 97, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 117, 117, 118, 119, 120, 121, 122, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 139, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 149, 150, 151, 152, 152, 153, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 163, 164, 165, 166, 167, 168, 168, 168, 169, 170, 171, 172, 173, 174, 174, 174, 175, 176, 177, 178, 179, 180, 181, 182, 182, 183, 184, 185, 185, 185, 186, 186, 187, 188, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 205, 206, 207, 208, 209, 210, 210, 210, 211, 212, 213, 214, 215, 216, 217, 218, 218, 219, 220, 221, 222, 223, 224, 224, 225, 226, 226, 227, 227, 228, 228, 229, 229, 229, 230, 231, 231, 231, 232, 233, 234, 235, 236, 237, 237, 238, 238, 239, 240, 240, 241, 242, 243, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 253, 254, 255, 256, 257, 258, 259, 260, 261, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 270, 271, 272, 273, 274, 274, 274, 275, 276, 276, 276, 277, 278, 279, 280, 281, 282, 282, 282, 283, 284, 284, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 297, 298, 299, 299, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 312, 312, 313, 314, 315, 316, 317, 318, 318, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 327, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 336, 337, 338, 339, 339, 340, 341, 341, 342, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 353, 353, 354, 355, 356, 357, 357, 357, 358, 358, 359, 360, 361, 362, 363, 364, 364, 365, 366, 367, 368, 369, 369, 369, 370, 371, 372, 373, 373, 373, 374, 375, 376, 377, 378, 379, 379, 379, 380, 381, 382, 383, 384, 385, 385, 386, 387, 388, 388, 389, 389, 390, 390, 390, 391, 392, 393, 394, 395, 396, 396, 397, 398, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 410, 411, 411, None]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 23, 23, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "527\n",
      "527\n"
     ]
    }
   ],
   "source": [
    "word_ids = token_input.word_ids()\n",
    "print(word_ids)\n",
    "\n",
    "def align_labels(word_ids:list, tag_list:list):\n",
    "    aligned_labels = []\n",
    "    for i in word_ids:\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "        else:\n",
    "            aligned_labels.append(tag_list[i])\n",
    "    return aligned_labels\n",
    "\n",
    "aligned_labels = align_labels(word_ids, tag_list=df[\"new_labels\"][0])\n",
    "# aligned_labels = create_aligned_labels(word_ids, example=df)\n",
    "\n",
    "print(aligned_labels)\n",
    "print(len(aligned_labels))\n",
    "print(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_ids\"] = df[\"text\"].apply(lambda x: tokenizer(x, is_split_into_words=True).word_ids())\n",
    "df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"label\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', -100),\n",
       " ('▁-', 'O'),\n",
       " ('D', 'O'),\n",
       " ('OC', 'O'),\n",
       " ('ST', 'O'),\n",
       " ('ART', 'O'),\n",
       " ('-', 'O'),\n",
       " ('▁e', 'O'),\n",
       " ('volution', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁salaires', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁enveloppe', 'O'),\n",
       " ('▁budgétaire', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁il', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁convenu', 'O'),\n",
       " ('▁entre', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁parties', 'O'),\n",
       " ('▁que', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('enveloppe', 'O'),\n",
       " ('▁budgétaire', 'O'),\n",
       " ('▁consacrée', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁salaires', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁des', 'B-TOUS'),\n",
       " ('▁collaborateurs', 'I-TOUS'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁remplissant', 'O'),\n",
       " ('▁par', 'O'),\n",
       " ('▁ailleurs', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁autres', 'O'),\n",
       " ('▁conditions', 'O'),\n",
       " ('▁habituelle', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('éligibilité', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁représenter', 'O'),\n",
       " ('a', 'O'),\n",
       " ('▁5', 'B-ATOT'),\n",
       " (',', 'B-ATOT'),\n",
       " ('8', 'B-ATOT'),\n",
       " ('▁%', 'I-ATOT'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁masse', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁fait', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁primes', 'O'),\n",
       " ('▁exprimée', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁pourcentage', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁augmenter', 'O'),\n",
       " ('ont', 'O'),\n",
       " ('▁ainsi', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁même', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁que', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁concernés', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁rappel', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁conformément', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁politique', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('▁d', 'B-ENT'),\n",
       " ('’', 'B-ENT'),\n",
       " ('e', 'B-ENT'),\n",
       " ('li', 'B-ENT'),\n",
       " ('▁l', 'I-ENT'),\n",
       " ('illy', 'I-ENT'),\n",
       " ('▁&', 'I-ENT'),\n",
       " ('▁ci', 'I-ENT'),\n",
       " ('e', 'I-ENT'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁supervise', 'O'),\n",
       " ('ur', 'O'),\n",
       " ('▁prend', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁décision', 'O'),\n",
       " ('▁quant', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁collaborateur', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁prenant', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁considération', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁sur', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('année', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁durée', 'O'),\n",
       " ('*', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collègues', 'O'),\n",
       " ('▁situés', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁même', 'O'),\n",
       " ('▁niveau', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁rapport', 'O'),\n",
       " ('▁entre', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁durée', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁*', 'O'),\n",
       " ('à', 'O'),\n",
       " ('▁partir', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁un', 'O'),\n",
       " ('▁collaborateur', 'O'),\n",
       " ('▁ne', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁pas', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁non', 'O'),\n",
       " ('▁éligible', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁pourrait', 'O'),\n",
       " ('▁néanmoins', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁pouvant', 'O'),\n",
       " ('▁aller', 'O'),\n",
       " ('▁jusqu', 'O'),\n",
       " ('’', 'O'),\n",
       " ('à', 'O'),\n",
       " ('▁50', 'O'),\n",
       " ('▁%', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁maximum', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁fourchette', 'O'),\n",
       " ('▁pre', 'O'),\n",
       " ('vue', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('nt', 'O'),\n",
       " ('ile', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁perce', 'O'),\n",
       " ('vra', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁moitié', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁variable', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁mesure', 'O'),\n",
       " ('▁exceptionnelle', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('▁remplissent', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁conditions', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('éligibilité', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('▁sont', 'O'),\n",
       " ('▁présents', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('er', 'O'),\n",
       " ('▁avril', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('ont', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁garantie', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('augmentation', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁1300', 'O'),\n",
       " ('€', 'O'),\n",
       " ('▁brut', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁pas', 'O'),\n",
       " ('▁an', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁éligible', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁ne', 'O'),\n",
       " ('▁pouvant', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁total', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁cette', 'O'),\n",
       " ('▁mesure', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁car', 'O'),\n",
       " ('▁atteignant', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁plafond', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁leur', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁se', 'O'),\n",
       " ('▁verront', 'O'),\n",
       " ('▁verser', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁complément', 'O'),\n",
       " ('▁sous', 'O'),\n",
       " ('▁forme', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('augmentation', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁pro', 'O'),\n",
       " ('ra', 'O'),\n",
       " ('té', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁partiel', 'O'),\n",
       " ('s', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁date', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('application', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁évolutions', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁prévues', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁présent', 'O'),\n",
       " ('▁article', 'O'),\n",
       " ('▁seront', 'O'),\n",
       " ('▁applicables', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'B-DATE'),\n",
       " ('er', 'B-DATE'),\n",
       " ('▁mars', 'I-DATE'),\n",
       " ('▁20', 'I-DATE'),\n",
       " ('23', 'I-DATE'),\n",
       " ('.', 'I-DATE'),\n",
       " ('▁article', 'O'),\n",
       " ('▁2', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('▁exceptionnelle', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁dont', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁égale', 'O'),\n",
       " ('▁ou', 'O'),\n",
       " ('▁supérieure', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁1', 'O'),\n",
       " (',', 'O'),\n",
       " ('25', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁leur', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁justifiant', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁présence', 'O'),\n",
       " ('▁effective', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁3', 'O'),\n",
       " ('▁mois', 'O'),\n",
       " ('▁minimum', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁2022', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁présents', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('er', 'O'),\n",
       " ('▁avril', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁dont', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁supérieure', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁1', 'O'),\n",
       " (',', 'O'),\n",
       " ('25', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁1300', 'O'),\n",
       " ('▁€', 'O'),\n",
       " ('▁brut', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁versée', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁mars', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁ce', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁pro', 'O'),\n",
       " ('ra', 'O'),\n",
       " ('té', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁partiel', 'O'),\n",
       " ('s', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁article', 'O'),\n",
       " ('▁3', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁promotions', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁ajustement', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁professionnelle', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁reste', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁priorité', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁ainsi', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁0,3', 'O'),\n",
       " ('▁%', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁masse', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁consacrée', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁promotions', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁ajustement', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁', 'O'),\n",
       " ('.', 'O'),\n",
       " ('</s>', -100)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens, aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de 'aligned_labels' faux: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>aligned_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-, evolution, des, salaires, de, bas...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 5, 5, 6, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>[negociation, annuelle, 2022., il, a, été, con...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>[négociations, annuelles, obligatoires., ii-, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 2, 2, 3, 3, 3, 4, 5, 5, 6, 7, 8, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>[accord, collectif, 2022, sur, les, salaires, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 8, 9, 10,...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>[damart, sa, etablissement., article, i, :, au...</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 6, 7, 7, ...</td>\n",
       "      <td>[-100, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>[entre, l’ues, kiabi, ,, représentée, par, ,, ...</td>\n",
       "      <td>[0, 0, 3, 0, 0, 0, 0, 2, 2, 2, 2, 0, 3, 0, 0, ...</td>\n",
       "      <td>[None, 0, 1, 1, 1, 2, 2, 3, 3, 4, 5, 6, 6, 7, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    [-DOCSTART-, evolution, des, salaires, de, bas...   \n",
       "1    [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2    [dispositions, au, regard, de, l’implication, ...   \n",
       "3    [nous, travaillons, sur, une, politique, de, r...   \n",
       "4    [protocole, d’accord, négociation, annuelle, o...   \n",
       "..                                                 ...   \n",
       "444  [negociation, annuelle, 2022., il, a, été, con...   \n",
       "445  [négociations, annuelles, obligatoires., ii-, ...   \n",
       "446  [accord, collectif, 2022, sur, les, salaires, ...   \n",
       "447  [damart, sa, etablissement., article, i, :, au...   \n",
       "448  [entre, l’ues, kiabi, ,, représentée, par, ,, ...   \n",
       "\n",
       "                                                 label  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...   \n",
       "..                                                 ...   \n",
       "444  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "445  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "446  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "447  [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "448  [0, 0, 3, 0, 0, 0, 0, 2, 2, 2, 2, 0, 3, 0, 0, ...   \n",
       "\n",
       "                                              word_ids  \\\n",
       "0    [None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, ...   \n",
       "1    [None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 5, 5, 6, ...   \n",
       "2    [None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10,...   \n",
       "3    [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "4    [None, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, ...   \n",
       "..                                                 ...   \n",
       "444  [None, 0, 0, 0, 0, 1, 2, 2, 3, 4, 5, 6, 7, 8, ...   \n",
       "445  [None, 0, 1, 2, 2, 3, 3, 3, 4, 5, 5, 6, 7, 8, ...   \n",
       "446  [None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 8, 9, 10,...   \n",
       "447  [None, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 6, 7, 7, ...   \n",
       "448  [None, 0, 1, 1, 1, 2, 2, 3, 3, 4, 5, 6, 6, 7, ...   \n",
       "\n",
       "                                        aligned_labels  \n",
       "0    [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1    [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2    [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3    [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4    [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  \n",
       "..                                                 ...  \n",
       "444  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "445  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "446  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "447  [-100, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "448  [-100, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 2, ...  \n",
       "\n",
       "[449 rows x 4 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"new_labels\", \"word_ids\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "\n",
    "def tokenize_and_align_data(path_conll:str, new_id:dict):\n",
    "    #charge and create the df\n",
    "    df = charge_conll(path_conll)\n",
    "    df = format_text(df)\n",
    "    df = format_labels(df)\n",
    "    df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))\n",
    "\n",
    "    #tokenize and align the text\n",
    "    df[\"word_ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, is_split_into_words=True).word_ids())\n",
    "    df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"new_labels\"]), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = tokenize_and_align_data(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id)\n",
    "# df = select_and_rename_data(df, selected_columns=[\"formated_text\", \"new_labels\", \"word_ids\", \"aligned_labels\"], new_names=new_names)\n",
    "df = select_columns(df, columns)\n",
    "df = df.rename(columns=new_names)\n",
    "#we check the length of the two columns so that they are of the same dimensions\n",
    "print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOUS', 'I-TOUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ATOT', 'B-ATOT', 'B-ATOT', 'I-ATOT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENT', 'B-ENT', 'B-ENT', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', -100]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 23, 23, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"aligned_labels\"][0])\n",
    "df_test = df[\"aligned_labels\"].apply(lambda x :change_ids(x, new_id=new_id))\n",
    "print(df_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOUS', 'I-TOUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ATOT', 'I-ATOT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "527\n",
      "527\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "aligned_labels = [-100 if i is None else example[f\"label\"][i] for i in word_ids]\n",
    "print(example[\"label\"])\n",
    "print(len(word_ids))\n",
    "print(len(aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(tokens_mod, aligned_labels))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_mod' is not defined"
     ]
    }
   ],
   "source": [
    "list(zip(tokens_mod, aligned_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset for training from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you like it you can download it\n",
    "data = import_label_studio_data(\"../../data/raw/data449.json\")\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[evolution, des, salaires, de, base, :, envelo...</td>\n",
       "      <td>{'entities': [(322, 326, 'ATOT'), (161, 179, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>{'entities': [(229, 237, 'OUV'), (239, 247, 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>{'entities': [(101, 105, 'SYND'), (110, 122, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>{'entities': [(165, 172, 'SYND'), (364, 371, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>{'entities': [(73, 82, 'DIR'), (108, 119, 'ENT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [evolution, des, salaires, de, base, :, envelo...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \n",
       "0  {'entities': [(322, 326, 'ATOT'), (161, 179, '...  \n",
       "1  {'entities': [(229, 237, 'OUV'), (239, 247, 'O...  \n",
       "2  {'entities': [(101, 105, 'SYND'), (110, 122, '...  \n",
       "3  {'entities': [(165, 172, 'SYND'), (364, 371, '...  \n",
       "4  {'entities': [(73, 82, 'DIR'), (108, 119, 'ENT...  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df[\"text\"])):\n",
    "    df[\"text\"][i] = turn_sentence_to_list(df[\"text\"][i])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    element = df[\"text\"][i]\n",
    "    length.append(len(element))\n",
    "    # print(len(df[\"text\"][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "plt.hist(length, bins=20, color = \"lightgreen\", edgecolor='black') \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Longeur (élément)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Histogram de la longueurs des phrases en élément')\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text': [\"evolution des salaires de base : enveloppe\"],\n",
    "    'label': [{'entities': [(22, 31, 'ATOT')]}]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to tokenize text while preserving whitespace\n",
    "def tokenize_text(row):\n",
    "    text = row['text']\n",
    "    tokens = []\n",
    "    start = 0\n",
    "\n",
    "    for start, end, entity in row['label']['entities']:\n",
    "        # Add non-entity text\n",
    "        tokens.extend(text[start:end].split())\n",
    "        start = end\n",
    "\n",
    "    # Add any remaining text after the last entity\n",
    "    tokens.extend(text[start:].split())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df['tokenized_text'] = df.apply(tokenize_text, axis=1)\n",
    "\n",
    "# Display the DataFrame with tokenized text while preserving whitespace\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df[\"label\"])):\n",
    "    print(df[\"label\"][i][\"entities\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'][0]\n",
    "text = turn_sentence_to_list(text)\n",
    "# print(type(text))\n",
    "tokens = tokenizer(text, is_split_into_words=True)\n",
    "# print(tokens)\n",
    "\n",
    "tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "print(tokens_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokens.word_ids()\n",
    "# aligned_labels = [-100 if i is None else text[\"label\"][i] for i in word_ids]\n",
    "for i in word_ids:\n",
    "    # print(i)\n",
    "    if i is None:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(text[\"label\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

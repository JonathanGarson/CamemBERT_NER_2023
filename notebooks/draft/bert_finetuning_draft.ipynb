{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning of BERT for NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the steps for the finetuning of French version of BERT, the objective is to extract specific information from french public business agreements. For this we have a dual approach, first we classify information and then only we use NER property of our model to extract wage variations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: This notebook is inspired from a masterclass and notebook provided by [Thomas Boehler](https://fr.linkedin.com/in/thomas-boehler-ba34a744) and also the work of my colleagues [Mouad Bernoussi](https://ma.linkedin.com/in/mouad-bernoussi-00aa91242) (see ./notebooks/external) and [Conrad Thiounn](https://github.com/cthiounn)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Play with PyTorch\n",
    "2. Play with the model \n",
    "3. Adapt data to BERT format (see Thomas Boehler)\n",
    "4. Create training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try with conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_conll(path:str):\n",
    "    # Initialize a list to store the data\n",
    "    conll_data = []\n",
    "\n",
    "    # Open the CoNLL file in read mode\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "        # Read each line in the file\n",
    "\n",
    "    # Split the data into sentences\n",
    "    sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "    # Initialize empty lists to store text and labels\n",
    "    text_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split('\\n')\n",
    "        text = \" \".join(token.split()[0] for token in tokens)\n",
    "        labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    -DOCSTART- evolution des salaires de base : en...   \n",
      "1    l’enveloppe globale d’augmentation des rémunér...   \n",
      "2    dispositions au regard de l’implication de tou...   \n",
      "3    nous travaillons sur une politique de rémunéra...   \n",
      "4    protocole d’accord négociation annuelle obliga...   \n",
      "..                                                 ...   \n",
      "444  negociation annuelle 2022. il a été convenu et...   \n",
      "445  négociations annuelles obligatoires. ii- dispo...   \n",
      "446  accord collectif 2022 sur les salaires , la du...   \n",
      "447  damart sa etablissement. article i : augmentat...   \n",
      "448  entre l’ues kiabi , représentée par , directeu...   \n",
      "\n",
      "                                                 label  \n",
      "0    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "1    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "2    O O O O O O O O O O O O O O O O B-SYND O B-DIR...  \n",
      "3    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "4    O O O O O O O O O B-DIR O O O O B-ENT I-ENT O ...  \n",
      "..                                                 ...  \n",
      "444  O O O O O O O O O O O O O O O O O O O B-OUV O ...  \n",
      "445  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "446  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "447  B-ENT O O O O O O O O O O O O O O B-OUV O B-OU...  \n",
      "448  O O B-ENT O O O O B-DIR I-DIR I-DIR I-DIR O B-...  \n",
      "\n",
      "[449 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path to your CoNLL file\n",
    "conll_file_path = r'../../data/raw/data449.conll'  # Replace with your file path\n",
    "\n",
    "# Initialize a list to store the data\n",
    "conll_data = []\n",
    "\n",
    "# Open the CoNLL file in read mode\n",
    "with open(conll_file_path, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "    # Read each line in the file\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists to store text and labels\n",
    "text_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    text_list.append(text)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]\n",
    "\n",
    "# def turn_sentence_to_list(sentence):\n",
    "#     \"\"\"\n",
    "#     Turn a sentence into a list of words\n",
    "\n",
    "#     Args:\n",
    "#         sentence (str): sentence to tokenize\n",
    "\n",
    "#     Returns:\n",
    "#         list: list of words\n",
    "#     \"\"\"\n",
    "#     return sentence.split()\n",
    "\n",
    "\n",
    "def list_to_string(input_list, separator=' '):\n",
    "    \"\"\"\n",
    "    Convert a list of strings into a single string with elements separated by a specified separator.\n",
    "\n",
    "    :param input_list: List of strings to be converted.\n",
    "    :param separator: The separator to use between elements (default is a space).\n",
    "    :return: A single string containing the list elements.\n",
    "    \"\"\"\n",
    "    return separator.join(input_list)\n",
    "\n",
    "def change_ids(list_ids:list, new_id:dict):\n",
    "    return [new_id.get(id) for id in list_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "\n",
    "df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "O O O O O O O O O O O O O O O B-ENT I-ENT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-NCAD I-NCAD O O O O O O O O O O O O O O O O O O O O O O O O AG AG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O CAD CAD O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ENT I-ENT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(df[\"new_labels\"][5])\n",
    "print(df[\"label\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check that we haven't left non-numeric labels\n",
    "\n",
    "\n",
    "def check_labels(list_labels:list):\n",
    "    for label in list_labels:\n",
    "        if type(label)!=int:\n",
    "            print(label)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# amount = [df[\"new_labels\"].apply(lambda x: check_labels(x))]\n",
    "\n",
    "def count_error(dataframe, column:str):\n",
    "    \"\"\"\n",
    "    Count the number of errors - a cell containing a non numerical value - in a column of a dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): dataframe to check\n",
    "        column (str): column to check\n",
    "\n",
    "    Returns:\n",
    "        int: number of errors\n",
    "    \"\"\"\n",
    "    return len(df[column])- sum(dataframe[column].apply(lambda x: check_labels(x)))\n",
    "\n",
    "# print(f\"nombre d'erreurs: {count_error(df, 'new_labels')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts: \n\u001b[1;32m----> 5\u001b[0m     text \u001b[39m=\u001b[39m turn_sentence_to_list(text)\n\u001b[0;32m      6\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(text, is_split_into_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     tokens_mod \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mturn_sentence_to_list\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mturn_sentence_to_list\u001b[39m(sentence):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    Turn a sentence into a list of tokens\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m        list: list of tokens\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sentence\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "texts = df[\"text\"].tolist()\n",
    "start = 0\n",
    "\n",
    "for text in texts: \n",
    "    text = turn_sentence_to_list(text)\n",
    "    tokens = tokenizer(text, is_split_into_words=True)\n",
    "    tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "    length = len(tokens_mod)\n",
    "    if length > 512:\n",
    "        start += 1\n",
    "        print(length)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JB NER is probably not suited since we lost 182 texts, which is too much, due to context limit. We can eventually try to split the text into 512 tokens chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    [-DOCSTART- evolution des salaires de base : e...   \n",
      "1    [l’enveloppe globale d’augmentation des rémuné...   \n",
      "2    [dispositions au regard de l’implication de to...   \n",
      "3    [nous travaillons sur une politique de rémunér...   \n",
      "4    [protocole d’accord négociation annuelle oblig...   \n",
      "..                                                 ...   \n",
      "404  [negociation annuelle 2022. article 1 – mesure...   \n",
      "405  [négociations annuelles obligatoires. ii- disp...   \n",
      "406  [accord collectif 2022 sur les salaires , la d...   \n",
      "407  [damart sa etablissement. article i : augmenta...   \n",
      "408  [entre l’ues kiabi , représentée par , directe...   \n",
      "\n",
      "                                                 label  \n",
      "0    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "1    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "2    [O O O O O O O O O O O O O O O O B-SYND O B-DI...  \n",
      "3    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "4    [O O O O O O O O O B-DIR O O O O B-ENT I-ENT O...  \n",
      "..                                                 ...  \n",
      "404  [O O O O O O O O O B-OUV O O O O O O O O O O O...  \n",
      "405  [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "406  [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "407  [B-ENT O O O O O O O O O O O O O O B-OUV O B-O...  \n",
      "408  [O O B-ENT O O O O B-DIR I-DIR I-DIR I-DIR O B...  \n",
      "\n",
      "[409 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample data in CoNLL format\n",
    "conll_data = r'../../data/raw/data449.conll'\n",
    "\n",
    "with open(conll_data, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists and a variable to keep track of the current sentence length\n",
    "text_list = []\n",
    "labels_list = []\n",
    "current_length = 0\n",
    "\n",
    "# Define the maximum token length for a chunk\n",
    "max_chunk_length = 512\n",
    "\n",
    "# Initialize a dictionary to keep track of chunks\n",
    "chunked_data = defaultdict(list)\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    \n",
    "    # Check if adding this sentence will exceed the maximum chunk length\n",
    "    if current_length + len(tokens) <= max_chunk_length:\n",
    "        # Add this sentence to the current chunk\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "        current_length += len(tokens)\n",
    "    else:\n",
    "        # Start a new chunk\n",
    "        chunked_data['text'].append(text_list)\n",
    "        chunked_data['label'].append(labels_list)\n",
    "        \n",
    "        # Reset the current chunk\n",
    "        text_list = [text]\n",
    "        labels_list = [labels]\n",
    "        current_length = len(tokens)\n",
    "\n",
    "# Add the last chunk\n",
    "chunked_data['text'].append(text_list)\n",
    "chunked_data['label'].append(labels_list)\n",
    "\n",
    "# Create a dataframe from the chunked data\n",
    "df = pd.DataFrame(chunked_data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def format_text(df):\n",
    "    formated_text = []\n",
    "\n",
    "    text = df[\"text\"]\n",
    "    for i in text:\n",
    "        i = turn_sentence_to_list(i)\n",
    "        formated_text.append(i)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "    df[\"formated_text\"] = formated_text\n",
    "    return df\n",
    "\n",
    "# formated_text = []\n",
    "\n",
    "# text = df[\"text\"]\n",
    "# for i in text:\n",
    "#     i = turn_sentence_to_list(i)\n",
    "#     formated_text.append(i)\n",
    "\n",
    "# # we add it to the dataframe\n",
    "# df[\"formated_text\"] = formated_text\n",
    "# df = format_text(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we format the label to the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(df):\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    formated_labels = []\n",
    "\n",
    "    for label in labels:\n",
    "        # label = list_to_string(label)\n",
    "        label = turn_sentence_to_list(label)\n",
    "        formated_labels.append(label)\n",
    "        # print(label)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "\n",
    "    df[\"formated_labels\"] = formated_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, columns:list):\n",
    "    df = df[columns].copy()\n",
    "    return df\n",
    "\n",
    "# df = select_columns(df, columns=[\"formated_text\", \"formated_labels\"])\n",
    "\n",
    "# df = df.rename(columns={\"formated_text\": \"text\", \"formated_labels\": \"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les éléments dont la dimension diverge sont au nombre de 0\n"
     ]
    }
   ],
   "source": [
    "# We check the length of the two columns so that they are of the same dimensions\n",
    "\n",
    "start = 0\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    if len(df['text'][i]) != len(df['label'][i]):\n",
    "        start += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"les éléments dont la dimension diverge sont au nombre de\",start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-, evolution, des, salaires, de, bas...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-DIR, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [-DOCSTART-, evolution, des, salaires, de, bas...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, B-DIR, O, O, O, O,...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"text\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(df[\"text\"][0], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['<s>', '▁-', 'D', 'OC', 'ST', 'ART', '-', '▁e', 'volution', '▁des', '▁salaires', '▁de', '▁base', '▁:', '▁enveloppe', '▁budgétaire', '▁:', '▁il', '▁est', '▁convenu', '▁entre', '▁les', '▁parties', '▁que', '▁l', '’', 'enveloppe', '▁budgétaire', '▁consacrée', '▁à', '▁l', '’', 'évolution', '▁des', '▁salaires', '▁de', '▁base', '▁des', '▁collaborateurs', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁', ',', '▁et', '▁remplissant', '▁par', '▁ailleurs', '▁les', '▁autres', '▁conditions', '▁habituelle', 's', '▁d', '’', 'éligibilité', '▁à', '▁une', '▁augmentation', '▁', ',', '▁représenter', 'a', '▁5', ',', '8', '▁%', '▁de', '▁la', '▁masse', '▁salariale', '.', '▁de', '▁fait', '▁', ',', '▁les', '▁primes', '▁exprimée', 's', '▁en', '▁pourcentage', '▁du', '▁salaire', '▁de', '▁base', '▁augmenter', 'ont', '▁ainsi', '▁en', '▁même', '▁temps', '▁que', '▁le', '▁salaire', '▁des', '▁collaborateurs', '▁concernés', '.', '▁pour', '▁rappel', '▁', ',', '▁conformément', '▁à', '▁la', '▁politique', '▁salariale', '▁d', '’', 'e', 'li', '▁l', 'illy', '▁&', '▁ci', 'e', '▁', ',', '▁le', '▁supervise', 'ur', '▁prend', '▁une', '▁décision', '▁quant', '▁à', '▁l', '’', 'évolution', '▁du', '▁salaire', '▁de', '▁base', '▁de', '▁son', '▁collaborateur', '▁dans', '▁son', '▁échelle', '▁', ',', '▁en', '▁prenant', '▁en', '▁considération', '▁:', '▁sa', '▁performance', '▁sur', '▁l', '’', 'année', '▁sa', '▁performance', '▁dans', '▁la', '▁durée', '*', '▁la', '▁performance', '▁des', '▁collègues', '▁situés', '▁au', '▁même', '▁niveau', '▁de', '▁poste', '▁le', '▁rapport', '▁entre', '▁sa', '▁position', '▁dans', '▁l', '’', 'échelle', '▁de', '▁salaire', '▁et', '▁sa', '▁performance', '▁dans', '▁la', '▁durée', '▁(', '▁*', 'à', '▁partir', '▁de', '▁20', '23', '▁', ',', '▁un', '▁collaborateur', '▁ne', '▁répondant', '▁pas', '▁aux', '▁attentes', '▁(', '▁non', '▁éligible', 's', '▁)', '▁pourrait', '▁néanmoins', '▁bénéficier', '▁d', '’', 'une', '▁augmentation', '▁de', '▁salaire', '▁pouvant', '▁aller', '▁jusqu', '’', 'à', '▁50', '▁%', '▁du', '▁maximum', '▁de', '▁la', '▁fourchette', '▁pre', 'vue', '▁pour', '▁son', '▁qui', 'nt', 'ile', '▁', ',', '▁et', '▁perce', 'vra', '▁la', '▁moitié', '▁de', '▁son', '▁variable', '▁)', '▁mesure', '▁exceptionnelle', '▁:', '▁les', '▁collaborateurs', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁', ',', '▁qui', '▁remplissent', '▁les', '▁conditions', '▁d', '’', 'éligibilité', '▁à', '▁une', '▁augmentation', '▁du', '▁salaire', '▁de', '▁base', '▁', ',', '▁et', '▁qui', '▁sont', '▁présents', '▁au', '▁1', 'er', '▁avril', '▁20', '23', '▁', ',', '▁bénéficier', 'ont', '▁d', '’', 'une', '▁garantie', '▁d', '’', 'augmentation', '▁du', '▁salaire', '▁de', '▁base', '▁de', '▁1300', '€', '▁brut', 's', '▁pas', '▁an', '.', '▁les', '▁collaborateurs', '▁éligible', 's', '▁ne', '▁pouvant', '▁bénéficier', '▁du', '▁montant', '▁total', '▁de', '▁cette', '▁mesure', '▁', ',', '▁car', '▁atteignant', '▁le', '▁plafond', '▁de', '▁leur', '▁échelle', '▁', ',', '▁se', '▁verront', '▁verser', '▁le', '▁complément', '▁sous', '▁forme', '▁de', '▁prime', '.', '▁le', '▁montant', '▁de', '▁l', '’', 'augmentation', '▁sera', '▁pro', 'ra', 'té', '▁pour', '▁les', '▁collaborateurs', '▁à', '▁temps', '▁partiel', 's', '.', '▁date', '▁d', '’', 'application', '▁:', '▁les', '▁évolutions', '▁de', '▁salaire', '▁prévues', '▁au', '▁présent', '▁article', '▁seront', '▁applicables', '▁au', '▁1', 'er', '▁mars', '▁20', '23', '.', '▁article', '▁2', '▁:', '▁prime', '▁exceptionnelle', '▁pour', '▁les', '▁collaborateurs', '▁dont', '▁la', '▁position', '▁dans', '▁l', '’', 'échelle', '▁est', '▁égale', '▁ou', '▁supérieure', '▁à', '▁1', ',', '25', '▁de', '▁leur', '▁échelle', '▁:', '▁pour', '▁les', '▁collaborateurs', '▁justifiant', '▁d', '’', 'une', '▁présence', '▁effective', '▁de', '▁3', '▁mois', '▁minimum', '▁en', '▁2022', '▁', ',', '▁présents', '▁au', '▁1', 'er', '▁avril', '▁20', '23', '▁', ',', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁et', '▁dont', '▁la', '▁position', '▁dans', '▁l', '’', 'échelle', '▁est', '▁supérieure', '▁à', '▁1', ',', '25', '▁', ',', '▁une', '▁prime', '▁de', '▁1300', '▁€', '▁brut', 's', '▁sera', '▁versée', '▁en', '▁mars', '▁20', '23', '.', '▁ce', '▁montant', '▁sera', '▁pro', 'ra', 'té', '▁pour', '▁les', '▁collaborateurs', '▁à', '▁temps', '▁partiel', 's', '.', '▁article', '▁3', '▁:', '▁promotions', '▁et', '▁ajustement', 's', '▁:', '▁en', '▁20', '23', '▁', ',', '▁l', '’', 'évolution', '▁professionnelle', '▁des', '▁collaborateurs', '▁reste', '▁une', '▁priorité', '.', '▁ainsi', '▁', ',', '▁0,3', '▁%', '▁de', '▁la', '▁masse', '▁salariale', '▁sera', '▁consacrée', '▁aux', '▁promotions', '▁et', '▁ajustement', 's', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "token_input = tokenizer(df['text'][0], is_split_into_words=True)\n",
    "print(token_input)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(token_input[\"input_ids\"])\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_ids = token_input.word_ids()\n",
    "# print(word_ids)\n",
    "\n",
    "def align_labels(word_ids:list, tag_list:list):\n",
    "    aligned_labels = []\n",
    "    for i in word_ids:\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "        else:\n",
    "            aligned_labels.append(tag_list[i])\n",
    "    return aligned_labels\n",
    "\n",
    "# aligned_labels = align_labels(word_ids, tag_list=df[\"new_labels\"][0])\n",
    "# aligned_labels = create_aligned_labels(word_ids, example=df)\n",
    "\n",
    "# print(aligned_labels)\n",
    "# print(len(aligned_labels))\n",
    "# print(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_ids\"] = df[\"text\"].apply(lambda x: tokenizer(x, is_split_into_words=True).word_ids())\n",
    "df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"label\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', -100),\n",
       " ('▁-', 'O'),\n",
       " ('D', 'O'),\n",
       " ('OC', 'O'),\n",
       " ('ST', 'O'),\n",
       " ('ART', 'O'),\n",
       " ('-', 'O'),\n",
       " ('▁e', 'O'),\n",
       " ('volution', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁salaires', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁enveloppe', 'O'),\n",
       " ('▁budgétaire', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁il', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁convenu', 'O'),\n",
       " ('▁entre', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁parties', 'O'),\n",
       " ('▁que', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('enveloppe', 'O'),\n",
       " ('▁budgétaire', 'O'),\n",
       " ('▁consacrée', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁salaires', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁des', 'B-TOUS'),\n",
       " ('▁collaborateurs', 'I-TOUS'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁remplissant', 'O'),\n",
       " ('▁par', 'O'),\n",
       " ('▁ailleurs', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁autres', 'O'),\n",
       " ('▁conditions', 'O'),\n",
       " ('▁habituelle', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('éligibilité', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁représenter', 'O'),\n",
       " ('a', 'O'),\n",
       " ('▁5', 'B-ATOT'),\n",
       " (',', 'B-ATOT'),\n",
       " ('8', 'B-ATOT'),\n",
       " ('▁%', 'I-ATOT'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁masse', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁fait', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁primes', 'O'),\n",
       " ('▁exprimée', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁pourcentage', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁augmenter', 'O'),\n",
       " ('ont', 'O'),\n",
       " ('▁ainsi', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁même', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁que', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁concernés', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁rappel', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁conformément', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁politique', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('▁d', 'B-ENT'),\n",
       " ('’', 'B-ENT'),\n",
       " ('e', 'B-ENT'),\n",
       " ('li', 'B-ENT'),\n",
       " ('▁l', 'I-ENT'),\n",
       " ('illy', 'I-ENT'),\n",
       " ('▁&', 'I-ENT'),\n",
       " ('▁ci', 'I-ENT'),\n",
       " ('e', 'I-ENT'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁supervise', 'O'),\n",
       " ('ur', 'O'),\n",
       " ('▁prend', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁décision', 'O'),\n",
       " ('▁quant', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁collaborateur', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁prenant', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁considération', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁sur', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('année', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁durée', 'O'),\n",
       " ('*', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collègues', 'O'),\n",
       " ('▁situés', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁même', 'O'),\n",
       " ('▁niveau', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁rapport', 'O'),\n",
       " ('▁entre', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁durée', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁*', 'O'),\n",
       " ('à', 'O'),\n",
       " ('▁partir', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁un', 'O'),\n",
       " ('▁collaborateur', 'O'),\n",
       " ('▁ne', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁pas', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁non', 'O'),\n",
       " ('▁éligible', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁pourrait', 'O'),\n",
       " ('▁néanmoins', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁pouvant', 'O'),\n",
       " ('▁aller', 'O'),\n",
       " ('▁jusqu', 'O'),\n",
       " ('’', 'O'),\n",
       " ('à', 'O'),\n",
       " ('▁50', 'O'),\n",
       " ('▁%', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁maximum', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁fourchette', 'O'),\n",
       " ('▁pre', 'O'),\n",
       " ('vue', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('nt', 'O'),\n",
       " ('ile', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁perce', 'O'),\n",
       " ('vra', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁moitié', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁variable', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁mesure', 'O'),\n",
       " ('▁exceptionnelle', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('▁remplissent', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁conditions', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('éligibilité', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('▁sont', 'O'),\n",
       " ('▁présents', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('er', 'O'),\n",
       " ('▁avril', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('ont', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁garantie', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('augmentation', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁1300', 'O'),\n",
       " ('€', 'O'),\n",
       " ('▁brut', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁pas', 'O'),\n",
       " ('▁an', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁éligible', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁ne', 'O'),\n",
       " ('▁pouvant', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁total', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁cette', 'O'),\n",
       " ('▁mesure', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁car', 'O'),\n",
       " ('▁atteignant', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁plafond', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁leur', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁se', 'O'),\n",
       " ('▁verront', 'O'),\n",
       " ('▁verser', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁complément', 'O'),\n",
       " ('▁sous', 'O'),\n",
       " ('▁forme', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('augmentation', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁pro', 'O'),\n",
       " ('ra', 'O'),\n",
       " ('té', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁partiel', 'O'),\n",
       " ('s', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁date', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('application', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁évolutions', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁prévues', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁présent', 'O'),\n",
       " ('▁article', 'O'),\n",
       " ('▁seront', 'O'),\n",
       " ('▁applicables', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'B-DATE'),\n",
       " ('er', 'B-DATE'),\n",
       " ('▁mars', 'I-DATE'),\n",
       " ('▁20', 'I-DATE'),\n",
       " ('23', 'I-DATE'),\n",
       " ('.', 'I-DATE'),\n",
       " ('▁article', 'O'),\n",
       " ('▁2', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('▁exceptionnelle', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁dont', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁égale', 'O'),\n",
       " ('▁ou', 'O'),\n",
       " ('▁supérieure', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁1', 'O'),\n",
       " (',', 'O'),\n",
       " ('25', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁leur', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁justifiant', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁présence', 'O'),\n",
       " ('▁effective', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁3', 'O'),\n",
       " ('▁mois', 'O'),\n",
       " ('▁minimum', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁2022', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁présents', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('er', 'O'),\n",
       " ('▁avril', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁dont', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁supérieure', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁1', 'O'),\n",
       " (',', 'O'),\n",
       " ('25', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁1300', 'O'),\n",
       " ('▁€', 'O'),\n",
       " ('▁brut', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁versée', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁mars', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁ce', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁pro', 'O'),\n",
       " ('ra', 'O'),\n",
       " ('té', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁partiel', 'O'),\n",
       " ('s', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁article', 'O'),\n",
       " ('▁3', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁promotions', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁ajustement', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁professionnelle', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁reste', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁priorité', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁ainsi', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁0,3', 'O'),\n",
       " ('▁%', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁masse', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁consacrée', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁promotions', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁ajustement', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁', 'O'),\n",
       " ('.', 'O'),\n",
       " ('</s>', -100)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens, aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 67, 342, 5363, 7486, 11485, 26, 599, 14573...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5, 17, 12, 16886, 4141, 18, 12, 5314, 20, 601...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 2756, 36, 897, 8, 17, 12, 16286, 8, 117, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5, 63, 13807, 32, 28, 462, 8, 6016, 1344, 38,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5, 5996, 18, 12, 1311, 8776, 5998, 3329, 325,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573...   \n",
       "1  [5, 17, 12, 16886, 4141, 18, 12, 5314, 20, 601...   \n",
       "2  [5, 2756, 36, 897, 8, 17, 12, 16286, 8, 117, 1...   \n",
       "3  [5, 63, 13807, 32, 28, 462, 8, 6016, 1344, 38,...   \n",
       "4  [5, 5996, 18, 12, 1311, 8776, 5998, 3329, 325,...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_iids_am(df):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for _,i in df.iterrows():\n",
    "        input = i[\"ids\"][\"input_ids\"]\n",
    "        attention = i[\"ids\"][\"attention_mask\"]\n",
    "\n",
    "        input_ids.append(input)\n",
    "        attention_mask.append(attention)\n",
    "\n",
    "    df[\"input_ids\"] = input_ids\n",
    "    df[\"attention_mask\"] = attention_mask\n",
    "    return df\n",
    "\n",
    "df = create_iids_am(df)\n",
    "df[[\"input_ids\", \"attention_mask\"]].head()\n",
    "# df_ids = df[[\"ids\"]].copy()\n",
    "# df_ids[\"ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de 'aligned_labels' faux: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>aligned_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-, evolution, des, salaires, de, bas...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, ...</td>\n",
       "      <td>[101, 1011, 9986, 14117, 2102, 1011, 6622, 407...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[101, 1048, 1521, 4372, 15985, 7361, 5051, 379...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 4, 4, 5, 6, 6, 7, 8, ...</td>\n",
       "      <td>[101, 22137, 2015, 8740, 7634, 2139, 1048, 152...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 1, 1, 1, 1, 2, 3, 4, 4, 4, 5, 6, ...</td>\n",
       "      <td>[101, 2053, 2271, 19817, 12462, 20343, 2015, 7...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...</td>\n",
       "      <td>[None, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, ...</td>\n",
       "      <td>[101, 8778, 2063, 1040, 1521, 15802, 11265, 39...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [-DOCSTART-, evolution, des, salaires, de, bas...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...   \n",
       "\n",
       "                                            word_ids  \\\n",
       "0  [None, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, ...   \n",
       "1  [None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, ...   \n",
       "2  [None, 0, 0, 1, 2, 3, 4, 4, 4, 5, 6, 6, 7, 8, ...   \n",
       "3  [None, 0, 0, 1, 1, 1, 1, 2, 3, 4, 4, 4, 5, 6, ...   \n",
       "4  [None, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1011, 9986, 14117, 2102, 1011, 6622, 407...   \n",
       "1  [101, 1048, 1521, 4372, 15985, 7361, 5051, 379...   \n",
       "2  [101, 22137, 2015, 8740, 7634, 2139, 1048, 152...   \n",
       "3  [101, 2053, 2271, 19817, 12462, 20343, 2015, 7...   \n",
       "4  [101, 8778, 2063, 1040, 1521, 15802, 11265, 39...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      aligned_labels  \n",
       "0  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "\n",
    "def tokenize_and_align_data(path_conll:str, new_id:dict):\n",
    "    #charge and create the df\n",
    "    df = charge_conll(path_conll)\n",
    "    df = format_text(df)\n",
    "    df = format_labels(df)\n",
    "    df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))\n",
    "\n",
    "    #tokenize and align the text\n",
    "    df[\"ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True))\n",
    "    df = create_iids_am(df)\n",
    "    df[\"word_ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True).word_ids())\n",
    "    df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"new_labels\"]), axis=1)\n",
    "    return df\n",
    "\n",
    "def apply_tokenization(conll_path:str, new_id:dict, columns:list, new_names:dict, save_path=False, output_save=None):\n",
    "    df = tokenize_and_align_data(conll_path, new_id=new_id)\n",
    "    df = select_columns(df, columns)\n",
    "    df = df.rename(columns=new_names)\n",
    "    #we check the length of the two columns so that they are of the same dimensions\n",
    "    print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "    if save_path:\n",
    "        df.to_csv(output_save, index=False)\n",
    "    return df\n",
    "\n",
    "df = apply_tokenization(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id, columns=columns, new_names=new_names, save_path=True, output_save=r\"../../data/intermediate/data499_token.csv\")\n",
    "df.head()\n",
    "# df = tokenize_and_align_data(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id)\n",
    "# df = select_columns(df, columns)\n",
    "# df = df.rename(columns=new_names)\n",
    "# #we check the length of the two columns so that they are of the same dimensions\n",
    "# print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "# # df.to_csv(r\"../../data/intermediate/data499_token.csv\", index=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "list_text = df[\"text\"].tolist()\n",
    "\n",
    "for i in list_text:\n",
    "    if len(i) > 512:\n",
    "        print(len(i))\n",
    "    else:\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the objective is to collect all the inputs_ids, attention_mask and labels in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "def final_formating(df, start:int, end:int):\n",
    "    \"\"\"\n",
    "    Collect the data from a dataframe in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "\n",
    "    Returns:\n",
    "        list: list of the data\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        input_ids.append(df[\"input_ids\"][i])\n",
    "        attention_mask.append(df[\"attention_mask\"][i])\n",
    "        labels.append(df[\"aligned_labels\"][i])\n",
    "\n",
    "    data = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "    return data\n",
    "    \n",
    "# data = final_formating(df, 0, 5)\n",
    "# print(data)\n",
    "# for _,i in df.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_treat_data(conll_path:str, new_id:dict, columns:list, new_names:dict, start:int, end:int):\n",
    "    \"\"\"\n",
    "    This function compile all the previous one and treat automatically all data provided as conll format.\\n\n",
    "    The objective is to provide a dictionnary containing the data for a given range of sentences with only one function. \\n\n",
    "    The same results can be obtained by parts with apply_tokenization and final_formating functions. This one is more a convenience function.\n",
    "\n",
    "    Args:\n",
    "        conll_path (str): path to the conll file\n",
    "        new_id (dict): dictionnary to change the labels\n",
    "        columns (list): columns to keep\n",
    "        new_names (dict): new names of the columns\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionnary containing the data\n",
    "    \"\"\"\n",
    "    df = apply_tokenization(conll_path, new_id=new_id, columns=columns, new_names=new_names)\n",
    "    data = final_formating(df, start, end)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de 'aligned_labels' faux: 0\n"
     ]
    }
   ],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "conll_path = r\"..\\..\\data\\raw\\data449.conll\"\n",
    "start = 0\n",
    "end = 449\n",
    "\n",
    "data = download_and_treat_data(conll_path=conll_path, new_id=new_id, columns=columns, new_names=new_names, start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we divide the data into train, test, and validation sets\n",
    "\n",
    "def split_data(data_dict, train_size=0.8, test_size=0.1, val_size=0.1, random_seed=None):\n",
    "    # Set a random seed for reproducibility\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Combine input_ids, attention_mask, and aligned_labels into a single list\n",
    "    combined_data = list(zip(data_dict['input_ids'], data_dict['attention_mask'], data_dict['labels']))\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    total_size = len(combined_data)\n",
    "    train_size = int(train_size * total_size)\n",
    "    test_size = int(test_size * total_size)\n",
    "    val_size = int(val_size * total_size)\n",
    "\n",
    "    # Split the data into train, test, and val sets\n",
    "    train_data = combined_data[:train_size]\n",
    "    test_data = combined_data[train_size:train_size + test_size]\n",
    "    val_data = combined_data[train_size + test_size:train_size + test_size + val_size]\n",
    "\n",
    "    # Unzip the data to restore the original structure\n",
    "    train_input_ids, train_attention_mask, train_aligned_labels = zip(*train_data)\n",
    "    test_input_ids, test_attention_mask, test_aligned_labels = zip(*test_data)\n",
    "    val_input_ids, val_attention_mask, val_aligned_labels = zip(*val_data)\n",
    "\n",
    "    # Create dictionaries for the train, test, and val sets\n",
    "    train_set = {\n",
    "        'input_ids': list(train_input_ids),\n",
    "        'attention_mask': list(train_attention_mask),\n",
    "        'labels': list(train_aligned_labels)\n",
    "    }\n",
    "    test_set = {\n",
    "        'input_ids': list(test_input_ids),\n",
    "        'attention_mask': list(test_attention_mask),\n",
    "        'labels': list(test_aligned_labels)\n",
    "    }\n",
    "    val_set = {\n",
    "        'input_ids': list(val_input_ids),\n",
    "        'attention_mask': list(val_attention_mask),\n",
    "        'labels': list(val_aligned_labels)\n",
    "    }\n",
    "\n",
    "    return train_set, test_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, val_data = split_data(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get rid of the list with a dimension superior to 512 as the model doesn't hold more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn into tensors our data so that BERT can read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "val_dataset = Dataset.from_dict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'SYND', 'DIR', 'DATE', 'ENT', 'CAD', 'INT', 'OUV', 'NCAD', 'NOUV', 'TOUS', 'AG CAD', 'AG INT', 'AG OUV', 'AG NCAD', 'AG NOUV', 'AI CAD', 'AI INT', 'AI OUV', 'AI NCAD', 'AI NOUV', 'AG', 'AI', 'ATOT', 'ATOT CAD', 'ATOT INT', 'ATOT OUV', 'ATOT NCAD', 'ATOT NOUV', 'PPV', 'PPVm']\n"
     ]
    }
   ],
   "source": [
    "# list of labels\n",
    "reverse_id = {v: k for k, v in new_id.items()}\n",
    "second_elements = [value for value in reverse_id.values()]\n",
    "label_list = second_elements\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at Jean-Baptiste/camembert-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([31]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([31, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"Jean-Baptiste/camembert-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\", num_labels=len(label_list), ignore_mismatched_sizes=True) #this last argument might be a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11265,  3995,  ...,     0,     0,     0],\n",
       "        [  101,  2033, 28632,  ...,     0,     0,     0],\n",
       "        [  101, 11265,  3995,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 15802,  9530,  ...,     0,     0,     0],\n",
       "        [  101,  8292,  4674,  ...,     0,     0,     0],\n",
       "        [  101, 15802,  8145,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11265,  3995,  ...,  2603,  1012,   102],\n",
       "        [  101, 15802,  3523,  ...,  1048,  1521,   102],\n",
       "        [  101, 15802, 28573,  ..., 12032,  6648,   102],\n",
       "        ...,\n",
       "        [  101,  3393, 16183,  ...,  3370,  3802,   102],\n",
       "        [  101, 15802, 11265,  ...,  2102,  4372,   102],\n",
       "        [  101, 11265,  3995,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,  ...,    0,    0, -100],\n",
       "        [-100,    0,    0,  ...,    0,    0, -100],\n",
       "        [-100,    0,    0,  ...,    0,    0, -100],\n",
       "        ...,\n",
       "        [-100,    0,    0,  ...,    0,    0, -100],\n",
       "        [-100,    0,    0,  ...,    0,    0, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(3.4393, grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.0660,  0.1158, -0.2266,  ...,  0.1506, -0.1803,  0.0611],\n",
       "         [-0.0005,  0.1099, -0.2516,  ...,  0.1760, -0.1128,  0.2022],\n",
       "         [ 0.0554,  0.1673, -0.3202,  ...,  0.1732, -0.0896,  0.1673],\n",
       "         ...,\n",
       "         [ 0.0371,  0.1019, -0.2497,  ...,  0.1846, -0.1285,  0.2170],\n",
       "         [ 0.0308,  0.1380, -0.3178,  ...,  0.1385, -0.1088,  0.1669],\n",
       "         [ 0.0698,  0.1126, -0.2297,  ...,  0.1580, -0.1785,  0.0630]],\n",
       "\n",
       "        [[ 0.0830,  0.1151, -0.2358,  ...,  0.1582, -0.1713,  0.0922],\n",
       "         [ 0.0167,  0.0629, -0.2582,  ...,  0.2050, -0.1119,  0.2376],\n",
       "         [ 0.0467,  0.0902, -0.2634,  ...,  0.2100, -0.0909,  0.2062],\n",
       "         ...,\n",
       "         [ 0.0830,  0.1151, -0.2358,  ...,  0.1582, -0.1713,  0.0922],\n",
       "         [ 0.0830,  0.1151, -0.2358,  ...,  0.1582, -0.1713,  0.0922],\n",
       "         [ 0.0849,  0.1131, -0.2383,  ...,  0.1637, -0.1713,  0.0919]],\n",
       "\n",
       "        [[ 0.0602,  0.1345, -0.2245,  ...,  0.1717, -0.1822,  0.0706],\n",
       "         [-0.0141,  0.1709, -0.2519,  ...,  0.1468, -0.1248,  0.1312],\n",
       "         [ 0.0401,  0.0940, -0.2199,  ...,  0.1110, -0.1369,  0.1935],\n",
       "         ...,\n",
       "         [ 0.0602,  0.1345, -0.2245,  ...,  0.1717, -0.1822,  0.0706],\n",
       "         [ 0.0602,  0.1345, -0.2245,  ...,  0.1717, -0.1822,  0.0706],\n",
       "         [ 0.0632,  0.1319, -0.2274,  ...,  0.1777, -0.1812,  0.0700]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0650,  0.1422, -0.2222,  ...,  0.1607, -0.1574,  0.0506],\n",
       "         [ 0.0228,  0.1557, -0.1744,  ...,  0.2551, -0.1193,  0.1776],\n",
       "         [ 0.0151,  0.0724, -0.1432,  ...,  0.2319, -0.1216,  0.1980],\n",
       "         ...,\n",
       "         [ 0.0825,  0.1911, -0.3095,  ...,  0.1296, -0.0872,  0.1538],\n",
       "         [ 0.0272,  0.1359, -0.3105,  ...,  0.1911, -0.0816,  0.1204],\n",
       "         [ 0.0688,  0.1415, -0.2255,  ...,  0.1673, -0.1540,  0.0529]],\n",
       "\n",
       "        [[ 0.0623,  0.1776, -0.1991,  ...,  0.1602, -0.1686,  0.0231],\n",
       "         [-0.0126,  0.0788, -0.2297,  ...,  0.2543, -0.0827,  0.2086],\n",
       "         [-0.0114,  0.1014, -0.1532,  ...,  0.2192, -0.0645,  0.1449],\n",
       "         ...,\n",
       "         [-0.0502,  0.1310, -0.1232,  ...,  0.1644, -0.0438,  0.0017],\n",
       "         [-0.0087,  0.1795, -0.1413,  ...,  0.1692, -0.0368,  0.0071],\n",
       "         [ 0.0647,  0.1786, -0.2034,  ...,  0.1668, -0.1681,  0.0245]],\n",
       "\n",
       "        [[ 0.0417,  0.1069, -0.2063,  ...,  0.1708, -0.2010,  0.0660],\n",
       "         [-0.0417,  0.1776, -0.3596,  ...,  0.1591, -0.1144,  0.1650],\n",
       "         [ 0.0366,  0.0531, -0.3269,  ...,  0.1890, -0.1154,  0.2196],\n",
       "         ...,\n",
       "         [ 0.0417,  0.1069, -0.2063,  ...,  0.1708, -0.2010,  0.0660],\n",
       "         [ 0.0417,  0.1069, -0.2063,  ...,  0.1708, -0.2010,  0.0660],\n",
       "         [ 0.0435,  0.1054, -0.2087,  ...,  0.1770, -0.2006,  0.0648]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**(next(iter(train_dataloader))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[204], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[0;32m      5\u001b[0m model_name \u001b[39m=\u001b[39m model_checkpoint\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      7\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m-finetuned-ner\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     evaluation_strategy \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     11\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     12\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m     \u001b[39m# push_to_hub=True,\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32m<string>:115\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, include_tokens_per_second)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\training_args.py:1436\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1430\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[0;32m   1431\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1433\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1434\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1435\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1436\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1437\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1438\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1439\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1440\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1441\u001b[0m ):\n\u001b[0;32m   1442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1443\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1444\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1445\u001b[0m     )\n\u001b[0;32m   1447\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1448\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1449\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1457\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\training_args.py:1901\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1899\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1901\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[0;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\training_args.py:1801\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1800\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1801\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   1802\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1803\u001b[0m         )\n\u001b[0;32m   1804\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1805\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drafty draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "df_token = df[['input_ids']].copy()\n",
    "print(df_token[\"input_ids\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOUS', 'I-TOUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ATOT', 'B-ATOT', 'B-ATOT', 'I-ATOT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENT', 'B-ENT', 'B-ENT', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', -100]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 23, 23, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"aligned_labels\"][0])\n",
    "df_test = df[\"aligned_labels\"].apply(lambda x :change_ids(x, new_id=new_id))\n",
    "print(df_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOUS', 'I-TOUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ATOT', 'I-ATOT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "527\n",
      "527\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "aligned_labels = [-100 if i is None else example[f\"label\"][i] for i in word_ids]\n",
    "print(example[\"label\"])\n",
    "print(len(word_ids))\n",
    "print(len(aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(tokens_mod, aligned_labels))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_mod' is not defined"
     ]
    }
   ],
   "source": [
    "list(zip(tokens_mod, aligned_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset for training from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you like it you can download it\n",
    "data = import_label_studio_data(\"../../data/raw/data449.json\")\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[evolution, des, salaires, de, base, :, envelo...</td>\n",
       "      <td>{'entities': [(322, 326, 'ATOT'), (161, 179, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>{'entities': [(229, 237, 'OUV'), (239, 247, 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>{'entities': [(101, 105, 'SYND'), (110, 122, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>{'entities': [(165, 172, 'SYND'), (364, 371, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>{'entities': [(73, 82, 'DIR'), (108, 119, 'ENT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [evolution, des, salaires, de, base, :, envelo...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \n",
       "0  {'entities': [(322, 326, 'ATOT'), (161, 179, '...  \n",
       "1  {'entities': [(229, 237, 'OUV'), (239, 247, 'O...  \n",
       "2  {'entities': [(101, 105, 'SYND'), (110, 122, '...  \n",
       "3  {'entities': [(165, 172, 'SYND'), (364, 371, '...  \n",
       "4  {'entities': [(73, 82, 'DIR'), (108, 119, 'ENT...  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df[\"text\"])):\n",
    "    df[\"text\"][i] = turn_sentence_to_list(df[\"text\"][i])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    element = df[\"text\"][i]\n",
    "    length.append(len(element))\n",
    "    # print(len(df[\"text\"][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "plt.hist(length, bins=20, color = \"lightgreen\", edgecolor='black') \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Longeur (élément)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Histogram de la longueurs des phrases en élément')\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text': [\"evolution des salaires de base : enveloppe\"],\n",
    "    'label': [{'entities': [(22, 31, 'ATOT')]}]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to tokenize text while preserving whitespace\n",
    "def tokenize_text(row):\n",
    "    text = row['text']\n",
    "    tokens = []\n",
    "    start = 0\n",
    "\n",
    "    for start, end, entity in row['label']['entities']:\n",
    "        # Add non-entity text\n",
    "        tokens.extend(text[start:end].split())\n",
    "        start = end\n",
    "\n",
    "    # Add any remaining text after the last entity\n",
    "    tokens.extend(text[start:].split())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df['tokenized_text'] = df.apply(tokenize_text, axis=1)\n",
    "\n",
    "# Display the DataFrame with tokenized text while preserving whitespace\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df[\"label\"])):\n",
    "    print(df[\"label\"][i][\"entities\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'][0]\n",
    "text = turn_sentence_to_list(text)\n",
    "# print(type(text))\n",
    "tokens = tokenizer(text, is_split_into_words=True)\n",
    "# print(tokens)\n",
    "\n",
    "tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "print(tokens_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokens.word_ids()\n",
    "# aligned_labels = [-100 if i is None else text[\"label\"][i] for i in word_ids]\n",
    "for i in word_ids:\n",
    "    # print(i)\n",
    "    if i is None:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(text[\"label\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

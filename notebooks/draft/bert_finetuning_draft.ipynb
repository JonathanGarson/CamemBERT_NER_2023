{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning of BERT for NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the steps for the finetuning of French version of BERT, the objective is to extract specific information from french public business agreements. For this we have a dual approach, first we classify information and then only we use NER property of our model to extract wage variations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: This notebook is inspired from a masterclass and notebook provided by [Thomas Boehler](https://fr.linkedin.com/in/thomas-boehler-ba34a744) and also the work of my colleagues [Mouad Bernoussi](https://ma.linkedin.com/in/mouad-bernoussi-00aa91242) (see ./notebooks/external) and [Conrad Thiounn](https://github.com/cthiounn)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Play with PyTorch\n",
    "2. Play with the model \n",
    "3. Adapt data to BERT format (see Thomas Boehler)\n",
    "4. Create training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try with conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_conll(path:str):\n",
    "    # Initialize a list to store the data\n",
    "    conll_data = []\n",
    "\n",
    "    # Open the CoNLL file in read mode\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        conll_data = file.read()\n",
    "        # Read each line in the file\n",
    "\n",
    "    # Split the data into sentences\n",
    "    sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "    # Initialize empty lists to store text and labels\n",
    "    text_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split('\\n')\n",
    "        text = \" \".join(token.split()[0] for token in tokens)\n",
    "        labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    -DOCSTART- evolution des salaires de base : en...   \n",
      "1    l’enveloppe globale d’augmentation des rémunér...   \n",
      "2    dispositions au regard de l’implication de tou...   \n",
      "3    nous travaillons sur une politique de rémunéra...   \n",
      "4    protocole d’accord négociation annuelle obliga...   \n",
      "..                                                 ...   \n",
      "444  negociation annuelle 2022. il a été convenu et...   \n",
      "445  négociations annuelles obligatoires. ii- dispo...   \n",
      "446  accord collectif 2022 sur les salaires , la du...   \n",
      "447  damart sa etablissement. article i : augmentat...   \n",
      "448  entre l’ues kiabi , représentée par , directeu...   \n",
      "\n",
      "                                                 label  \n",
      "0    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "1    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "2    O O O O O O O O O O O O O O O O B-SYND O B-DIR...  \n",
      "3    O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "4    O O O O O O O O O B-DIR O O O O B-ENT I-ENT O ...  \n",
      "..                                                 ...  \n",
      "444  O O O O O O O O O O O O O O O O O O O B-OUV O ...  \n",
      "445  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "446  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
      "447  B-ENT O O O O O O O O O O O O O O B-OUV O B-OU...  \n",
      "448  O O B-ENT O O O O B-DIR I-DIR I-DIR I-DIR O B-...  \n",
      "\n",
      "[449 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path to your CoNLL file\n",
    "conll_file_path = r'../../data/raw/data449.conll'  # Replace with your file path\n",
    "\n",
    "# Initialize a list to store the data\n",
    "conll_data = []\n",
    "\n",
    "# Open the CoNLL file in read mode\n",
    "with open(conll_file_path, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "    # Read each line in the file\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists to store text and labels\n",
    "text_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    text_list.append(text)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame({'text': text_list, 'label': labels_list})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]\n",
    "\n",
    "# def turn_sentence_to_list(sentence):\n",
    "#     \"\"\"\n",
    "#     Turn a sentence into a list of words\n",
    "\n",
    "#     Args:\n",
    "#         sentence (str): sentence to tokenize\n",
    "\n",
    "#     Returns:\n",
    "#         list: list of words\n",
    "#     \"\"\"\n",
    "#     return sentence.split()\n",
    "\n",
    "\n",
    "def list_to_string(input_list, separator=' '):\n",
    "    \"\"\"\n",
    "    Convert a list of strings into a single string with elements separated by a specified separator.\n",
    "\n",
    "    :param input_list: List of strings to be converted.\n",
    "    :param separator: The separator to use between elements (default is a space).\n",
    "    :return: A single string containing the list elements.\n",
    "    \"\"\"\n",
    "    return separator.join(input_list)\n",
    "\n",
    "def change_ids(list_ids:list, new_id:dict):\n",
    "    return [new_id.get(id) for id in list_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "\n",
    "df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "O O O O O O O O O O O O O O O B-ENT I-ENT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-NCAD I-NCAD O O O O O O O O O O O O O O O O O O O O O O O O AG AG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O CAD CAD O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-ENT I-ENT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(df[\"new_labels\"][5])\n",
    "print(df[\"label\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check that we haven't left non-numeric labels\n",
    "\n",
    "\n",
    "def check_labels(list_labels:list):\n",
    "    for label in list_labels:\n",
    "        if type(label)!=int:\n",
    "            print(label)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# amount = [df[\"new_labels\"].apply(lambda x: check_labels(x))]\n",
    "\n",
    "def count_error(dataframe, column:str):\n",
    "    \"\"\"\n",
    "    Count the number of errors - a cell containing a non numerical value - in a column of a dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): dataframe to check\n",
    "        column (str): column to check\n",
    "\n",
    "    Returns:\n",
    "        int: number of errors\n",
    "    \"\"\"\n",
    "    return len(df[column])- sum(dataframe[column].apply(lambda x: check_labels(x)))\n",
    "\n",
    "# print(f\"nombre d'erreurs: {count_error(df, 'new_labels')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts: \n\u001b[1;32m----> 5\u001b[0m     text \u001b[39m=\u001b[39m turn_sentence_to_list(text)\n\u001b[0;32m      6\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(text, is_split_into_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     tokens_mod \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mturn_sentence_to_list\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mturn_sentence_to_list\u001b[39m(sentence):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    Turn a sentence into a list of tokens\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m        list: list of tokens\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sentence\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "texts = df[\"text\"].tolist()\n",
    "start = 0\n",
    "\n",
    "for text in texts: \n",
    "    text = turn_sentence_to_list(text)\n",
    "    tokens = tokenizer(text, is_split_into_words=True)\n",
    "    tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "    length = len(tokens_mod)\n",
    "    if length > 512:\n",
    "        start += 1\n",
    "        print(length)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JB NER is probably not suited since we lost 182 texts, which is too much, due to context limit. We can eventually try to split the text into 512 tokens chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    [-DOCSTART- evolution des salaires de base : e...   \n",
      "1    [l’enveloppe globale d’augmentation des rémuné...   \n",
      "2    [dispositions au regard de l’implication de to...   \n",
      "3    [nous travaillons sur une politique de rémunér...   \n",
      "4    [protocole d’accord négociation annuelle oblig...   \n",
      "..                                                 ...   \n",
      "404  [negociation annuelle 2022. article 1 – mesure...   \n",
      "405  [négociations annuelles obligatoires. ii- disp...   \n",
      "406  [accord collectif 2022 sur les salaires , la d...   \n",
      "407  [damart sa etablissement. article i : augmenta...   \n",
      "408  [entre l’ues kiabi , représentée par , directe...   \n",
      "\n",
      "                                                 label  \n",
      "0    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "1    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "2    [O O O O O O O O O O O O O O O O B-SYND O B-DI...  \n",
      "3    [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "4    [O O O O O O O O O B-DIR O O O O B-ENT I-ENT O...  \n",
      "..                                                 ...  \n",
      "404  [O O O O O O O O O B-OUV O O O O O O O O O O O...  \n",
      "405  [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "406  [O O O O O O O O O O O O O O O O O O O O O O O...  \n",
      "407  [B-ENT O O O O O O O O O O O O O O B-OUV O B-O...  \n",
      "408  [O O B-ENT O O O O B-DIR I-DIR I-DIR I-DIR O B...  \n",
      "\n",
      "[409 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample data in CoNLL format\n",
    "conll_data = r'../../data/raw/data449.conll'\n",
    "\n",
    "with open(conll_data, 'r', encoding='utf-8') as file:\n",
    "    conll_data = file.read()\n",
    "\n",
    "# Split the data into sentences\n",
    "sentences = conll_data.strip().split('\\n\\n')\n",
    "\n",
    "# Initialize empty lists and a variable to keep track of the current sentence length\n",
    "text_list = []\n",
    "labels_list = []\n",
    "current_length = 0\n",
    "\n",
    "# Define the maximum token length for a chunk\n",
    "max_chunk_length = 512\n",
    "\n",
    "# Initialize a dictionary to keep track of chunks\n",
    "chunked_data = defaultdict(list)\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split('\\n')\n",
    "    text = \" \".join(token.split()[0] for token in tokens)\n",
    "    labels = \" \".join(token.split()[-1] for token in tokens)\n",
    "    \n",
    "    # Check if adding this sentence will exceed the maximum chunk length\n",
    "    if current_length + len(tokens) <= max_chunk_length:\n",
    "        # Add this sentence to the current chunk\n",
    "        text_list.append(text)\n",
    "        labels_list.append(labels)\n",
    "        current_length += len(tokens)\n",
    "    else:\n",
    "        # Start a new chunk\n",
    "        chunked_data['text'].append(text_list)\n",
    "        chunked_data['label'].append(labels_list)\n",
    "        \n",
    "        # Reset the current chunk\n",
    "        text_list = [text]\n",
    "        labels_list = [labels]\n",
    "        current_length = len(tokens)\n",
    "\n",
    "# Add the last chunk\n",
    "chunked_data['text'].append(text_list)\n",
    "chunked_data['label'].append(labels_list)\n",
    "\n",
    "# Create a dataframe from the chunked data\n",
    "df = pd.DataFrame(chunked_data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def format_text(df):\n",
    "    formated_text = []\n",
    "\n",
    "    text = df[\"text\"]\n",
    "    for i in text:\n",
    "        i = turn_sentence_to_list(i)\n",
    "        formated_text.append(i)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "    df[\"formated_text\"] = formated_text\n",
    "    return df\n",
    "\n",
    "# formated_text = []\n",
    "\n",
    "# text = df[\"text\"]\n",
    "# for i in text:\n",
    "#     i = turn_sentence_to_list(i)\n",
    "#     formated_text.append(i)\n",
    "\n",
    "# # we add it to the dataframe\n",
    "# df[\"formated_text\"] = formated_text\n",
    "# df = format_text(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we format the label to the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(df):\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    formated_labels = []\n",
    "\n",
    "    for label in labels:\n",
    "        # label = list_to_string(label)\n",
    "        label = turn_sentence_to_list(label)\n",
    "        formated_labels.append(label)\n",
    "        # print(label)\n",
    "\n",
    "    # we add it to the dataframe\n",
    "\n",
    "    df[\"formated_labels\"] = formated_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, columns:list):\n",
    "    df = df[columns].copy()\n",
    "    return df\n",
    "\n",
    "# df = select_columns(df, columns=[\"formated_text\", \"formated_labels\"])\n",
    "\n",
    "# df = df.rename(columns={\"formated_text\": \"text\", \"formated_labels\": \"label\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les éléments dont la dimension diverge sont au nombre de 0\n"
     ]
    }
   ],
   "source": [
    "# We check the length of the two columns so that they are of the same dimensions\n",
    "\n",
    "start = 0\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    if len(df['text'][i]) != len(df['label'][i]):\n",
    "        start += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"les éléments dont la dimension diverge sont au nombre de\",start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-, evolution, des, salaires, de, bas...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-DIR, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [-DOCSTART-, evolution, des, salaires, de, bas...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, B-DIR, O, O, O, O,...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"text\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(df[\"text\"][0], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['<s>', '▁-', 'D', 'OC', 'ST', 'ART', '-', '▁e', 'volution', '▁des', '▁salaires', '▁de', '▁base', '▁:', '▁enveloppe', '▁budgétaire', '▁:', '▁il', '▁est', '▁convenu', '▁entre', '▁les', '▁parties', '▁que', '▁l', '’', 'enveloppe', '▁budgétaire', '▁consacrée', '▁à', '▁l', '’', 'évolution', '▁des', '▁salaires', '▁de', '▁base', '▁des', '▁collaborateurs', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁', ',', '▁et', '▁remplissant', '▁par', '▁ailleurs', '▁les', '▁autres', '▁conditions', '▁habituelle', 's', '▁d', '’', 'éligibilité', '▁à', '▁une', '▁augmentation', '▁', ',', '▁représenter', 'a', '▁5', ',', '8', '▁%', '▁de', '▁la', '▁masse', '▁salariale', '.', '▁de', '▁fait', '▁', ',', '▁les', '▁primes', '▁exprimée', 's', '▁en', '▁pourcentage', '▁du', '▁salaire', '▁de', '▁base', '▁augmenter', 'ont', '▁ainsi', '▁en', '▁même', '▁temps', '▁que', '▁le', '▁salaire', '▁des', '▁collaborateurs', '▁concernés', '.', '▁pour', '▁rappel', '▁', ',', '▁conformément', '▁à', '▁la', '▁politique', '▁salariale', '▁d', '’', 'e', 'li', '▁l', 'illy', '▁&', '▁ci', 'e', '▁', ',', '▁le', '▁supervise', 'ur', '▁prend', '▁une', '▁décision', '▁quant', '▁à', '▁l', '’', 'évolution', '▁du', '▁salaire', '▁de', '▁base', '▁de', '▁son', '▁collaborateur', '▁dans', '▁son', '▁échelle', '▁', ',', '▁en', '▁prenant', '▁en', '▁considération', '▁:', '▁sa', '▁performance', '▁sur', '▁l', '’', 'année', '▁sa', '▁performance', '▁dans', '▁la', '▁durée', '*', '▁la', '▁performance', '▁des', '▁collègues', '▁situés', '▁au', '▁même', '▁niveau', '▁de', '▁poste', '▁le', '▁rapport', '▁entre', '▁sa', '▁position', '▁dans', '▁l', '’', 'échelle', '▁de', '▁salaire', '▁et', '▁sa', '▁performance', '▁dans', '▁la', '▁durée', '▁(', '▁*', 'à', '▁partir', '▁de', '▁20', '23', '▁', ',', '▁un', '▁collaborateur', '▁ne', '▁répondant', '▁pas', '▁aux', '▁attentes', '▁(', '▁non', '▁éligible', 's', '▁)', '▁pourrait', '▁néanmoins', '▁bénéficier', '▁d', '’', 'une', '▁augmentation', '▁de', '▁salaire', '▁pouvant', '▁aller', '▁jusqu', '’', 'à', '▁50', '▁%', '▁du', '▁maximum', '▁de', '▁la', '▁fourchette', '▁pre', 'vue', '▁pour', '▁son', '▁qui', 'nt', 'ile', '▁', ',', '▁et', '▁perce', 'vra', '▁la', '▁moitié', '▁de', '▁son', '▁variable', '▁)', '▁mesure', '▁exceptionnelle', '▁:', '▁les', '▁collaborateurs', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁', ',', '▁qui', '▁remplissent', '▁les', '▁conditions', '▁d', '’', 'éligibilité', '▁à', '▁une', '▁augmentation', '▁du', '▁salaire', '▁de', '▁base', '▁', ',', '▁et', '▁qui', '▁sont', '▁présents', '▁au', '▁1', 'er', '▁avril', '▁20', '23', '▁', ',', '▁bénéficier', 'ont', '▁d', '’', 'une', '▁garantie', '▁d', '’', 'augmentation', '▁du', '▁salaire', '▁de', '▁base', '▁de', '▁1300', '€', '▁brut', 's', '▁pas', '▁an', '.', '▁les', '▁collaborateurs', '▁éligible', 's', '▁ne', '▁pouvant', '▁bénéficier', '▁du', '▁montant', '▁total', '▁de', '▁cette', '▁mesure', '▁', ',', '▁car', '▁atteignant', '▁le', '▁plafond', '▁de', '▁leur', '▁échelle', '▁', ',', '▁se', '▁verront', '▁verser', '▁le', '▁complément', '▁sous', '▁forme', '▁de', '▁prime', '.', '▁le', '▁montant', '▁de', '▁l', '’', 'augmentation', '▁sera', '▁pro', 'ra', 'té', '▁pour', '▁les', '▁collaborateurs', '▁à', '▁temps', '▁partiel', 's', '.', '▁date', '▁d', '’', 'application', '▁:', '▁les', '▁évolutions', '▁de', '▁salaire', '▁prévues', '▁au', '▁présent', '▁article', '▁seront', '▁applicables', '▁au', '▁1', 'er', '▁mars', '▁20', '23', '.', '▁article', '▁2', '▁:', '▁prime', '▁exceptionnelle', '▁pour', '▁les', '▁collaborateurs', '▁dont', '▁la', '▁position', '▁dans', '▁l', '’', 'échelle', '▁est', '▁égale', '▁ou', '▁supérieure', '▁à', '▁1', ',', '25', '▁de', '▁leur', '▁échelle', '▁:', '▁pour', '▁les', '▁collaborateurs', '▁justifiant', '▁d', '’', 'une', '▁présence', '▁effective', '▁de', '▁3', '▁mois', '▁minimum', '▁en', '▁2022', '▁', ',', '▁présents', '▁au', '▁1', 'er', '▁avril', '▁20', '23', '▁', ',', '▁répondant', '▁aux', '▁attentes', '▁du', '▁poste', '▁et', '▁dont', '▁la', '▁position', '▁dans', '▁l', '’', 'échelle', '▁est', '▁supérieure', '▁à', '▁1', ',', '25', '▁', ',', '▁une', '▁prime', '▁de', '▁1300', '▁€', '▁brut', 's', '▁sera', '▁versée', '▁en', '▁mars', '▁20', '23', '.', '▁ce', '▁montant', '▁sera', '▁pro', 'ra', 'té', '▁pour', '▁les', '▁collaborateurs', '▁à', '▁temps', '▁partiel', 's', '.', '▁article', '▁3', '▁:', '▁promotions', '▁et', '▁ajustement', 's', '▁:', '▁en', '▁20', '23', '▁', ',', '▁l', '’', 'évolution', '▁professionnelle', '▁des', '▁collaborateurs', '▁reste', '▁une', '▁priorité', '.', '▁ainsi', '▁', ',', '▁0,3', '▁%', '▁de', '▁la', '▁masse', '▁salariale', '▁sera', '▁consacrée', '▁aux', '▁promotions', '▁et', '▁ajustement', 's', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "token_input = tokenizer(df['text'][0], is_split_into_words=True)\n",
    "print(token_input)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(token_input[\"input_ids\"])\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_ids = token_input.word_ids()\n",
    "# print(word_ids)\n",
    "\n",
    "def align_labels(word_ids:list, tag_list:list):\n",
    "    aligned_labels = []\n",
    "    for i in word_ids:\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "        else:\n",
    "            aligned_labels.append(tag_list[i])\n",
    "    return aligned_labels\n",
    "\n",
    "# aligned_labels = align_labels(word_ids, tag_list=df[\"new_labels\"][0])\n",
    "# aligned_labels = create_aligned_labels(word_ids, example=df)\n",
    "\n",
    "# print(aligned_labels)\n",
    "# print(len(aligned_labels))\n",
    "# print(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_ids\"] = df[\"text\"].apply(lambda x: tokenizer(x, is_split_into_words=True).word_ids())\n",
    "df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"label\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', -100),\n",
       " ('▁-', 'O'),\n",
       " ('D', 'O'),\n",
       " ('OC', 'O'),\n",
       " ('ST', 'O'),\n",
       " ('ART', 'O'),\n",
       " ('-', 'O'),\n",
       " ('▁e', 'O'),\n",
       " ('volution', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁salaires', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁enveloppe', 'O'),\n",
       " ('▁budgétaire', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁il', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁convenu', 'O'),\n",
       " ('▁entre', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁parties', 'O'),\n",
       " ('▁que', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('enveloppe', 'O'),\n",
       " ('▁budgétaire', 'O'),\n",
       " ('▁consacrée', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁salaires', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁des', 'B-TOUS'),\n",
       " ('▁collaborateurs', 'I-TOUS'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁remplissant', 'O'),\n",
       " ('▁par', 'O'),\n",
       " ('▁ailleurs', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁autres', 'O'),\n",
       " ('▁conditions', 'O'),\n",
       " ('▁habituelle', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('éligibilité', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁représenter', 'O'),\n",
       " ('a', 'O'),\n",
       " ('▁5', 'B-ATOT'),\n",
       " (',', 'B-ATOT'),\n",
       " ('8', 'B-ATOT'),\n",
       " ('▁%', 'I-ATOT'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁masse', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁fait', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁primes', 'O'),\n",
       " ('▁exprimée', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁pourcentage', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁augmenter', 'O'),\n",
       " ('ont', 'O'),\n",
       " ('▁ainsi', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁même', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁que', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁concernés', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁rappel', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁conformément', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁politique', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('▁d', 'B-ENT'),\n",
       " ('’', 'B-ENT'),\n",
       " ('e', 'B-ENT'),\n",
       " ('li', 'B-ENT'),\n",
       " ('▁l', 'I-ENT'),\n",
       " ('illy', 'I-ENT'),\n",
       " ('▁&', 'I-ENT'),\n",
       " ('▁ci', 'I-ENT'),\n",
       " ('e', 'I-ENT'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁supervise', 'O'),\n",
       " ('ur', 'O'),\n",
       " ('▁prend', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁décision', 'O'),\n",
       " ('▁quant', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁collaborateur', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁prenant', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁considération', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁sur', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('année', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁durée', 'O'),\n",
       " ('*', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collègues', 'O'),\n",
       " ('▁situés', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁même', 'O'),\n",
       " ('▁niveau', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁rapport', 'O'),\n",
       " ('▁entre', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁sa', 'O'),\n",
       " ('▁performance', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁durée', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁*', 'O'),\n",
       " ('à', 'O'),\n",
       " ('▁partir', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁un', 'O'),\n",
       " ('▁collaborateur', 'O'),\n",
       " ('▁ne', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁pas', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁non', 'O'),\n",
       " ('▁éligible', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁pourrait', 'O'),\n",
       " ('▁néanmoins', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁pouvant', 'O'),\n",
       " ('▁aller', 'O'),\n",
       " ('▁jusqu', 'O'),\n",
       " ('’', 'O'),\n",
       " ('à', 'O'),\n",
       " ('▁50', 'O'),\n",
       " ('▁%', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁maximum', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁fourchette', 'O'),\n",
       " ('▁pre', 'O'),\n",
       " ('vue', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('nt', 'O'),\n",
       " ('ile', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁perce', 'O'),\n",
       " ('vra', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁moitié', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁son', 'O'),\n",
       " ('▁variable', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁mesure', 'O'),\n",
       " ('▁exceptionnelle', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('▁remplissent', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁conditions', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('éligibilité', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁augmentation', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁qui', 'O'),\n",
       " ('▁sont', 'O'),\n",
       " ('▁présents', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('er', 'O'),\n",
       " ('▁avril', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('ont', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁garantie', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('augmentation', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁base', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁1300', 'O'),\n",
       " ('€', 'O'),\n",
       " ('▁brut', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁pas', 'O'),\n",
       " ('▁an', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁éligible', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁ne', 'O'),\n",
       " ('▁pouvant', 'O'),\n",
       " ('▁bénéficier', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁total', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁cette', 'O'),\n",
       " ('▁mesure', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁car', 'O'),\n",
       " ('▁atteignant', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁plafond', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁leur', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁se', 'O'),\n",
       " ('▁verront', 'O'),\n",
       " ('▁verser', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁complément', 'O'),\n",
       " ('▁sous', 'O'),\n",
       " ('▁forme', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁le', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('augmentation', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁pro', 'O'),\n",
       " ('ra', 'O'),\n",
       " ('té', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁partiel', 'O'),\n",
       " ('s', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁date', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('application', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁évolutions', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁salaire', 'O'),\n",
       " ('▁prévues', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁présent', 'O'),\n",
       " ('▁article', 'O'),\n",
       " ('▁seront', 'O'),\n",
       " ('▁applicables', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'B-DATE'),\n",
       " ('er', 'B-DATE'),\n",
       " ('▁mars', 'I-DATE'),\n",
       " ('▁20', 'I-DATE'),\n",
       " ('23', 'I-DATE'),\n",
       " ('.', 'I-DATE'),\n",
       " ('▁article', 'O'),\n",
       " ('▁2', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('▁exceptionnelle', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁dont', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁égale', 'O'),\n",
       " ('▁ou', 'O'),\n",
       " ('▁supérieure', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁1', 'O'),\n",
       " (',', 'O'),\n",
       " ('25', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁leur', 'O'),\n",
       " ('▁échelle', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁justifiant', 'O'),\n",
       " ('▁d', 'O'),\n",
       " ('’', 'O'),\n",
       " ('une', 'O'),\n",
       " ('▁présence', 'O'),\n",
       " ('▁effective', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁3', 'O'),\n",
       " ('▁mois', 'O'),\n",
       " ('▁minimum', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁2022', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁présents', 'O'),\n",
       " ('▁au', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('er', 'O'),\n",
       " ('▁avril', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁répondant', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁attentes', 'O'),\n",
       " ('▁du', 'O'),\n",
       " ('▁poste', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁dont', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁position', 'O'),\n",
       " ('▁dans', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('échelle', 'O'),\n",
       " ('▁est', 'O'),\n",
       " ('▁supérieure', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁1', 'O'),\n",
       " (',', 'O'),\n",
       " ('25', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁prime', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁1300', 'O'),\n",
       " ('▁€', 'O'),\n",
       " ('▁brut', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁versée', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁mars', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁ce', 'O'),\n",
       " ('▁montant', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁pro', 'O'),\n",
       " ('ra', 'O'),\n",
       " ('té', 'O'),\n",
       " ('▁pour', 'O'),\n",
       " ('▁les', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁à', 'O'),\n",
       " ('▁temps', 'O'),\n",
       " ('▁partiel', 'O'),\n",
       " ('s', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁article', 'O'),\n",
       " ('▁3', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁promotions', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁ajustement', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁en', 'O'),\n",
       " ('▁20', 'O'),\n",
       " ('23', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁l', 'O'),\n",
       " ('’', 'O'),\n",
       " ('évolution', 'O'),\n",
       " ('▁professionnelle', 'O'),\n",
       " ('▁des', 'O'),\n",
       " ('▁collaborateurs', 'O'),\n",
       " ('▁reste', 'O'),\n",
       " ('▁une', 'O'),\n",
       " ('▁priorité', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁ainsi', 'O'),\n",
       " ('▁', 'O'),\n",
       " (',', 'O'),\n",
       " ('▁0,3', 'O'),\n",
       " ('▁%', 'O'),\n",
       " ('▁de', 'O'),\n",
       " ('▁la', 'O'),\n",
       " ('▁masse', 'O'),\n",
       " ('▁salariale', 'O'),\n",
       " ('▁sera', 'O'),\n",
       " ('▁consacrée', 'O'),\n",
       " ('▁aux', 'O'),\n",
       " ('▁promotions', 'O'),\n",
       " ('▁et', 'O'),\n",
       " ('▁ajustement', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁', 'O'),\n",
       " ('.', 'O'),\n",
       " ('</s>', -100)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens, aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 67, 342, 5363, 7486, 11485, 26, 599, 14573...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5, 17, 12, 16886, 4141, 18, 12, 5314, 20, 601...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 2756, 36, 897, 8, 17, 12, 16286, 8, 117, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5, 63, 13807, 32, 28, 462, 8, 6016, 1344, 38,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5, 5996, 18, 12, 1311, 8776, 5998, 3329, 325,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [5, 67, 342, 5363, 7486, 11485, 26, 599, 14573...   \n",
       "1  [5, 17, 12, 16886, 4141, 18, 12, 5314, 20, 601...   \n",
       "2  [5, 2756, 36, 897, 8, 17, 12, 16286, 8, 117, 1...   \n",
       "3  [5, 63, 13807, 32, 28, 462, 8, 6016, 1344, 38,...   \n",
       "4  [5, 5996, 18, 12, 1311, 8776, 5998, 3329, 325,...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_iids_am(df):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for _,i in df.iterrows():\n",
    "        input = i[\"ids\"][\"input_ids\"]\n",
    "        attention = i[\"ids\"][\"attention_mask\"]\n",
    "\n",
    "        input_ids.append(input)\n",
    "        attention_mask.append(attention)\n",
    "\n",
    "    df[\"input_ids\"] = input_ids\n",
    "    df[\"attention_mask\"] = attention_mask\n",
    "    return df\n",
    "\n",
    "df = create_iids_am(df)\n",
    "df[[\"input_ids\", \"attention_mask\"]].head()\n",
    "# df_ids = df[[\"ids\"]].copy()\n",
    "# df_ids[\"ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de 'aligned_labels' faux: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>aligned_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-, evolution, des, salaires, de, bas...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, ...</td>\n",
       "      <td>[101, 1011, 9986, 14117, 2102, 1011, 6622, 407...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[101, 1048, 1521, 4372, 15985, 7361, 5051, 379...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 4, 4, 5, 6, 6, 7, 8, ...</td>\n",
       "      <td>[101, 22137, 2015, 8740, 7634, 2139, 1048, 152...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 1, 1, 1, 1, 2, 3, 4, 4, 4, 5, 6, ...</td>\n",
       "      <td>[101, 2053, 2271, 19817, 12462, 20343, 2015, 7...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...</td>\n",
       "      <td>[None, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, ...</td>\n",
       "      <td>[101, 8778, 2063, 1040, 1521, 15802, 11265, 39...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [-DOCSTART-, evolution, des, salaires, de, bas...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, ...   \n",
       "\n",
       "                                            word_ids  \\\n",
       "0  [None, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, ...   \n",
       "1  [None, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, ...   \n",
       "2  [None, 0, 0, 1, 2, 3, 4, 4, 4, 5, 6, 6, 7, 8, ...   \n",
       "3  [None, 0, 0, 1, 1, 1, 1, 2, 3, 4, 4, 4, 5, 6, ...   \n",
       "4  [None, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1011, 9986, 14117, 2102, 1011, 6622, 407...   \n",
       "1  [101, 1048, 1521, 4372, 15985, 7361, 5051, 379...   \n",
       "2  [101, 22137, 2015, 8740, 7634, 2139, 1048, 152...   \n",
       "3  [101, 2053, 2271, 19817, 12462, 20343, 2015, 7...   \n",
       "4  [101, 8778, 2063, 1040, 1521, 15802, 11265, 39...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      aligned_labels  \n",
       "0  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "\n",
    "def tokenize_and_align_data(path_conll:str, new_id:dict):\n",
    "    #charge and create the df\n",
    "    df = charge_conll(path_conll)\n",
    "    df = format_text(df)\n",
    "    df = format_labels(df)\n",
    "    df[\"new_labels\"]=df[\"label\"].apply(lambda x: change_ids(x.split(\" \"), new_id))\n",
    "\n",
    "    #tokenize and align the text\n",
    "    df[\"ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True))\n",
    "    df = create_iids_am(df)\n",
    "    df[\"word_ids\"] = df[\"formated_text\"].apply(lambda x: tokenizer(x, truncation=True, is_split_into_words=True).word_ids())\n",
    "    df[\"aligned_labels\"] = df.apply(lambda x: align_labels(x[\"word_ids\"], x[\"new_labels\"]), axis=1)\n",
    "    return df\n",
    "\n",
    "def apply_tokenization(conll_path:str, new_id:dict, columns:list, new_names:dict, save_path=False, output_save=None):\n",
    "    df = tokenize_and_align_data(conll_path, new_id=new_id)\n",
    "    df = select_columns(df, columns)\n",
    "    df = df.rename(columns=new_names)\n",
    "    #we check the length of the two columns so that they are of the same dimensions\n",
    "    print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "    if save_path:\n",
    "        df.to_csv(output_save, index=False)\n",
    "    return df\n",
    "\n",
    "df = apply_tokenization(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id, columns=columns, new_names=new_names, save_path=True, output_save=r\"../../data/intermediate/data499_token.csv\")\n",
    "df.head()\n",
    "# df = tokenize_and_align_data(r\"..\\..\\data\\raw\\data449.conll\", new_id=new_id)\n",
    "# df = select_columns(df, columns)\n",
    "# df = df.rename(columns=new_names)\n",
    "# #we check the length of the two columns so that they are of the same dimensions\n",
    "# print(f\"nombre de 'aligned_labels' faux: {count_error(df, 'aligned_labels')}\")\n",
    "# # df.to_csv(r\"../../data/intermediate/data499_token.csv\", index=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "list_text = df[\"text\"].tolist()\n",
    "\n",
    "for i in list_text:\n",
    "    if len(i) > 512:\n",
    "        print(len(i))\n",
    "    else:\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the objective is to collect all the inputs_ids, attention_mask and labels in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "def final_formating(df, start:int, end:int):\n",
    "    \"\"\"\n",
    "    Collect the data from a dataframe in a list for a given range of sentences and store them into a dictionnary\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the data\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "\n",
    "    Returns:\n",
    "        list: list of the data\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        input_ids.append(df[\"input_ids\"][i])\n",
    "        attention_mask.append(df[\"attention_mask\"][i])\n",
    "        labels.append(df[\"aligned_labels\"][i])\n",
    "\n",
    "    data = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "    return data\n",
    "    \n",
    "# data = final_formating(df, 0, 5)\n",
    "# print(data)\n",
    "# for _,i in df.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_treat_data(conll_path:str, new_id:dict, columns:list, new_names:dict, start:int, end:int):\n",
    "    \"\"\"\n",
    "    This function compile all the previous one and treat automatically all data provided as conll format.\\n\n",
    "    The objective is to provide a dictionnary containing the data for a given range of sentences with only one function. \\n\n",
    "    The same results can be obtained by parts with apply_tokenization and final_formating functions. This one is more a convenience function.\n",
    "\n",
    "    Args:\n",
    "        conll_path (str): path to the conll file\n",
    "        new_id (dict): dictionnary to change the labels\n",
    "        columns (list): columns to keep\n",
    "        new_names (dict): new names of the columns\n",
    "        start (int): index of the first sentence to collect\n",
    "        end (int): index of the last sentence to collect\n",
    "    \n",
    "    Returns:\n",
    "        dict: dictionnary containing the data\n",
    "    \"\"\"\n",
    "    df = apply_tokenization(conll_path, new_id=new_id, columns=columns, new_names=new_names)\n",
    "    data = final_formating(df, start, end)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de 'aligned_labels' faux: 0\n"
     ]
    }
   ],
   "source": [
    "new_id = {\"O\":0,\"B-SYND\":1, \"I-SYND\":1, \"SYND\":1, \"B-DIR\":2, \"I-DIR\":2, \"DIR\":2, \"B-DATE\":4, \"I-DATE\":4, \"DATE\":4, \"B-ENT\":3, \"I-ENT\":3, \"ENT\":3, \"B-CAD\":5, \"I-CAD\":5, \"CAD\":5, \"B-INT\":6, \"I-INT\":6, \"INT\":6,\"B-OUV\":7, \"I-OUV\":7, \"OUV\":7, \"B-NCAD\":8, \"I-NCAD\":8, \"NCAD\":8,\"B-NOUV\":9, \"I-NOUV\":9, \"NOUV\":9, \"B-TOUS\":10,\"I-TOUS\":10, \"TOUS\":10, \"B-AG CAD\":11,\"I-AG CAD\":11, \"AG CAD\":11, \"B-AG INT\":12, \"I-AG INT\":12, \"AG INT\":12, \"B-AG OUV\":13, \"I-AG OUV\":13, \"AG OUV\":13,\"B-AG NCAD\":14, \"I-AG NCAD\":14, \"AG NCAD\":14, \"B-AG NOUV\":15, \"I-AG NOUV\":15, \"AG NOUV\":15,\"B-AI CAD\":16, \"I-AI CAD\":16,\"AI CAD\":16,\"B-AI INT\":17, \"I-AI INT\":17, \"AI INT\":17,\"B-AI OUV\":18, \"I-AI OUV\":18,\"AI OUV\":18,\"B-AI NCAD\":19, \"I-AI NCAD\":19,\"AI NCAD\":19,\"B-AI NOUV\":20, \"I-AI NOUV\":20, \"AI NOUV\":20, \"B-AG\":21, \"I-AG\":21, \"AG\":21,\"B-AI\":22, \"I-AI\":22,\"AI\":22,\"B-ATOT\":23,\"I-ATOT\":23, \"ATOT\":23,\"B-ATOT CAD\":24, \"I-ATOT CAD\":24, \"ATOT CAD\":24,\"B-ATOT INT\":25, \"I-ATOT INT\":25,\"ATOT INT\":25,\"B-ATOT OUV\":26, \"I-ATOT OUV\":26, \"ATOT OUV\":26,\"B-ATOT NCAD\":27, \"I-ATOT NCAD\":27, \"ATOT NCAD\":27,\"B-ATOT NOUV\":28, \"I-ATOT NOUV\":28, \"ATOT NOUV\":28,\"B-PPV\":29, \"I-PPV\":29, \"PPV\":29,\"B-PPVm\":30, \"I-PPVm\":30, \"PPVm\":30}\n",
    "columns = [\"formated_text\", \"new_labels\",\"word_ids\", \"input_ids\", \"attention_mask\", \"aligned_labels\"]\n",
    "new_names = {\"formated_text\": \"text\", \"new_labels\": \"label\", \"word_ids\": \"word_ids\", \"aligned_labels\": \"aligned_labels\"}\n",
    "conll_path = r\"..\\..\\data\\raw\\data449.conll\"\n",
    "start = 0\n",
    "end = 449\n",
    "\n",
    "data = download_and_treat_data(conll_path=conll_path, new_id=new_id, columns=columns, new_names=new_names, start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we divide the data into train, test, and validation sets\n",
    "\n",
    "def split_data(data_dict, train_size=0.8, test_size=0.1, val_size=0.1, random_seed=None):\n",
    "    # Set a random seed for reproducibility\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Combine input_ids, attention_mask, and aligned_labels into a single list\n",
    "    combined_data = list(zip(data_dict['input_ids'], data_dict['attention_mask'], data_dict['labels']))\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(combined_data)\n",
    "\n",
    "    # Calculate the sizes of each set\n",
    "    total_size = len(combined_data)\n",
    "    train_size = int(train_size * total_size)\n",
    "    test_size = int(test_size * total_size)\n",
    "    val_size = int(val_size * total_size)\n",
    "\n",
    "    # Split the data into train, test, and val sets\n",
    "    train_data = combined_data[:train_size]\n",
    "    test_data = combined_data[train_size:train_size + test_size]\n",
    "    val_data = combined_data[train_size + test_size:train_size + test_size + val_size]\n",
    "\n",
    "    # Unzip the data to restore the original structure\n",
    "    train_input_ids, train_attention_mask, train_aligned_labels = zip(*train_data)\n",
    "    test_input_ids, test_attention_mask, test_aligned_labels = zip(*test_data)\n",
    "    val_input_ids, val_attention_mask, val_aligned_labels = zip(*val_data)\n",
    "\n",
    "    # Create dictionaries for the train, test, and val sets\n",
    "    train_set = {\n",
    "        'input_ids': list(train_input_ids),\n",
    "        'attention_mask': list(train_attention_mask),\n",
    "        'labels': list(train_aligned_labels)\n",
    "    }\n",
    "    test_set = {\n",
    "        'input_ids': list(test_input_ids),\n",
    "        'attention_mask': list(test_attention_mask),\n",
    "        'labels': list(test_aligned_labels)\n",
    "    }\n",
    "    val_set = {\n",
    "        'input_ids': list(val_input_ids),\n",
    "        'attention_mask': list(val_attention_mask),\n",
    "        'labels': list(val_aligned_labels)\n",
    "    }\n",
    "\n",
    "    return train_set, test_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, val_data = split_data(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get rid of the list with a dimension superior to 512 as the model doesn't hold more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn into tensors our data so that BERT can read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "val_dataset = Dataset.from_dict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11265,  3995,  ...,     0,     0,     0],\n",
       "        [  101,  2033, 28632,  ...,     0,     0,     0],\n",
       "        [  101, 11265,  3995,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 15802,  9530,  ...,     0,     0,     0],\n",
       "        [  101,  8292,  4674,  ...,     0,     0,     0],\n",
       "        [  101, 15802,  8145,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15802,  1040,  ...,     0,     0,     0],\n",
       "        [  101, 15802,  1040,  ...,     0,     0,     0],\n",
       "        [  101,  3720,  1015,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 11265,  3995,  ...,  3619,  1012,   102],\n",
       "        [  101, 11265,  3995,  ...,     0,     0,     0],\n",
       "        [  101, 15802,  4241,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100,    0,    0,  ...,    0,    0, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (864) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [8, 864].  Tensor sizes: [1, 514]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(\u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataloader))))\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\models\\camembert\\modeling_camembert.py:1268\u001b[0m, in \u001b[0;36mCamembertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[39m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1269\u001b[0m     input_ids,\n\u001b[0;32m   1270\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1271\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1272\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1273\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1274\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1275\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1276\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1277\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1278\u001b[0m )\n\u001b[0;32m   1280\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1282\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\garsonj\\Desktop\\bert_main\\venv\\Lib\\site-packages\\transformers\\models\\camembert\\modeling_camembert.py:863\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings, \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    862\u001b[0m     buffered_token_type_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[1;32m--> 863\u001b[0m     buffered_token_type_ids_expanded \u001b[39m=\u001b[39m buffered_token_type_ids\u001b[39m.\u001b[39;49mexpand(batch_size, seq_length)\n\u001b[0;32m    864\u001b[0m     token_type_ids \u001b[39m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[0;32m    865\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (864) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [8, 864].  Tensor sizes: [1, 514]"
     ]
    }
   ],
   "source": [
    "model(**(next(iter(train_dataloader))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drafty draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 67, 342, 5363, 7486, 11485, 26, 599, 14573, 20, 8058, 8, 494, 43, 10182, 11558, 43, 51, 30, 13646, 128, 19, 1503, 27, 17, 12, 16886, 11558, 5670, 15, 17, 12, 2010, 20, 8058, 8, 494, 20, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 14, 22745, 37, 1044, 19, 214, 643, 10121, 10, 18, 12, 28806, 15, 28, 3708, 21, 7, 5826, 55, 205, 7, 929, 453, 8, 13, 2269, 16054, 9, 8, 82, 21, 7, 19, 17218, 16850, 10, 22, 8001, 25, 4360, 8, 494, 3138, 263, 163, 22, 93, 125, 27, 16, 4360, 20, 4891, 5552, 9, 24, 3232, 21, 7, 4953, 15, 13, 462, 16054, 18, 12, 35, 1187, 17, 5337, 537, 642, 35, 21, 7, 16, 20692, 297, 759, 28, 1141, 1627, 15, 17, 12, 2010, 25, 4360, 8, 494, 8, 58, 18010, 29, 58, 6230, 21, 7, 22, 3184, 22, 8590, 43, 77, 2283, 32, 17, 12, 520, 77, 2283, 29, 13, 1083, 1363, 13, 2283, 20, 4412, 8733, 36, 93, 359, 8, 1337, 16, 459, 128, 77, 1266, 29, 17, 12, 4515, 8, 4360, 14, 77, 2283, 29, 13, 1083, 38, 951, 169, 350, 8, 325, 3009, 21, 7, 23, 18010, 45, 7826, 34, 68, 3306, 38, 165, 16918, 10, 936, 575, 3642, 2135, 18, 12, 70, 3708, 8, 4360, 2325, 632, 257, 12, 169, 712, 453, 25, 1622, 8, 13, 15570, 3591, 9731, 24, 58, 31, 113, 3368, 21, 7, 14, 12385, 19395, 13, 1993, 8, 58, 7333, 936, 586, 4110, 43, 19, 4891, 7826, 68, 3306, 25, 1337, 21, 7, 31, 27374, 19, 643, 18, 12, 28806, 15, 28, 3708, 25, 4360, 8, 494, 21, 7, 14, 31, 56, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 2135, 263, 18, 12, 70, 2196, 18, 12, 5314, 25, 4360, 8, 494, 8, 23945, 1163, 6201, 10, 34, 674, 9, 19, 4891, 16918, 10, 45, 2325, 2135, 25, 1520, 1458, 8, 78, 586, 21, 7, 173, 25936, 16, 3767, 8, 97, 6230, 21, 7, 48, 21221, 8625, 16, 5652, 161, 431, 8, 4870, 9, 16, 1520, 8, 17, 12, 5314, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 749, 18, 12, 1663, 43, 19, 8211, 8, 4360, 5931, 36, 689, 724, 519, 9819, 36, 124, 108, 697, 325, 3009, 9, 724, 118, 43, 4870, 4110, 24, 19, 4891, 174, 13, 1266, 29, 17, 12, 4515, 30, 7318, 47, 2731, 15, 124, 7, 2195, 8, 97, 6230, 43, 24, 19, 4891, 26962, 18, 12, 70, 922, 12765, 8, 135, 250, 1858, 22, 25922, 21, 7, 2559, 36, 124, 108, 871, 325, 3009, 21, 7, 7826, 68, 3306, 25, 1337, 14, 174, 13, 1266, 29, 17, 12, 4515, 30, 2731, 15, 124, 7, 2195, 21, 7, 28, 4870, 8, 23945, 984, 6201, 10, 210, 21336, 22, 697, 325, 3009, 9, 44, 1520, 210, 909, 305, 728, 24, 19, 4891, 15, 125, 9105, 10, 9, 724, 135, 43, 11522, 14, 13486, 10, 43, 22, 325, 3009, 21, 7, 17, 12, 2010, 1050, 20, 4891, 353, 28, 4712, 9, 163, 21, 7, 18971, 453, 8, 13, 2269, 16054, 210, 5670, 68, 11522, 14, 13486, 10, 21, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "df_token = df[['input_ids']].copy()\n",
    "print(df_token[\"input_ids\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOUS', 'I-TOUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ATOT', 'B-ATOT', 'B-ATOT', 'I-ATOT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENT', 'B-ENT', 'B-ENT', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', -100]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 23, 23, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"aligned_labels\"][0])\n",
    "df_test = df[\"aligned_labels\"].apply(lambda x :change_ids(x, new_id=new_id))\n",
    "print(df_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOUS', 'I-TOUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ATOT', 'I-ATOT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "527\n",
      "527\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "aligned_labels = [-100 if i is None else example[f\"label\"][i] for i in word_ids]\n",
    "print(example[\"label\"])\n",
    "print(len(word_ids))\n",
    "print(len(aligned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(tokens_mod, aligned_labels))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_mod' is not defined"
     ]
    }
   ],
   "source": [
    "list(zip(tokens_mod, aligned_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset for training from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you like it you can download it\n",
    "data = import_label_studio_data(\"../../data/raw/data449.json\")\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_sentence_to_list(sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a list of tokens\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: list of tokens\n",
    "    \"\"\"\n",
    "    return [token for token in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[evolution, des, salaires, de, base, :, envelo...</td>\n",
       "      <td>{'entities': [(322, 326, 'ATOT'), (161, 179, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l’enveloppe, globale, d’augmentation, des, ré...</td>\n",
       "      <td>{'entities': [(229, 237, 'OUV'), (239, 247, 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dispositions, au, regard, de, l’implication, ...</td>\n",
       "      <td>{'entities': [(101, 105, 'SYND'), (110, 122, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nous, travaillons, sur, une, politique, de, r...</td>\n",
       "      <td>{'entities': [(165, 172, 'SYND'), (364, 371, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[protocole, d’accord, négociation, annuelle, o...</td>\n",
       "      <td>{'entities': [(73, 82, 'DIR'), (108, 119, 'ENT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [evolution, des, salaires, de, base, :, envelo...   \n",
       "1  [l’enveloppe, globale, d’augmentation, des, ré...   \n",
       "2  [dispositions, au, regard, de, l’implication, ...   \n",
       "3  [nous, travaillons, sur, une, politique, de, r...   \n",
       "4  [protocole, d’accord, négociation, annuelle, o...   \n",
       "\n",
       "                                               label  \n",
       "0  {'entities': [(322, 326, 'ATOT'), (161, 179, '...  \n",
       "1  {'entities': [(229, 237, 'OUV'), (239, 247, 'O...  \n",
       "2  {'entities': [(101, 105, 'SYND'), (110, 122, '...  \n",
       "3  {'entities': [(165, 172, 'SYND'), (364, 371, '...  \n",
       "4  {'entities': [(73, 82, 'DIR'), (108, 119, 'ENT...  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df[\"text\"])):\n",
    "    df[\"text\"][i] = turn_sentence_to_list(df[\"text\"][i])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "\n",
    "for i in range(len(df[\"text\"])):\n",
    "    element = df[\"text\"][i]\n",
    "    length.append(len(element))\n",
    "    # print(len(df[\"text\"][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "plt.hist(length, bins=20, color = \"lightgreen\", edgecolor='black') \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Longeur (élément)')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Histogram de la longueurs des phrases en élément')\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text': [\"evolution des salaires de base : enveloppe\"],\n",
    "    'label': [{'entities': [(22, 31, 'ATOT')]}]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to tokenize text while preserving whitespace\n",
    "def tokenize_text(row):\n",
    "    text = row['text']\n",
    "    tokens = []\n",
    "    start = 0\n",
    "\n",
    "    for start, end, entity in row['label']['entities']:\n",
    "        # Add non-entity text\n",
    "        tokens.extend(text[start:end].split())\n",
    "        start = end\n",
    "\n",
    "    # Add any remaining text after the last entity\n",
    "    tokens.extend(text[start:].split())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df['tokenized_text'] = df.apply(tokenize_text, axis=1)\n",
    "\n",
    "# Display the DataFrame with tokenized text while preserving whitespace\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df[\"label\"])):\n",
    "    print(df[\"label\"][i][\"entities\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'][0]\n",
    "text = turn_sentence_to_list(text)\n",
    "# print(type(text))\n",
    "tokens = tokenizer(text, is_split_into_words=True)\n",
    "# print(tokens)\n",
    "\n",
    "tokens_mod = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "print(tokens_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokens.word_ids()\n",
    "# aligned_labels = [-100 if i is None else text[\"label\"][i] for i in word_ids]\n",
    "for i in word_ids:\n",
    "    # print(i)\n",
    "    if i is None:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(text[\"label\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
